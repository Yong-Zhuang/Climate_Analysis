{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and figure setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "# from tensorflow import set_random_seed, get_seed\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace import sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Conv1D,Flatten, MaxPooling1D, InputLayer\n",
    "from keras import initializers, regularizers\n",
    "from keras.callbacks import EarlyStopping, Callback, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_info_columns', 1000)\n",
    "pd.set_option('display.max_colwidth', 10000000000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 40, 10\n",
    "rcParams['figure.subplot.wspace'] = 0.05\n",
    "rcParams['figure.subplot.hspace'] = 0.2\n",
    "rcParams['axes.titlesize'] = 'x-large'\n",
    "rcParams['axes.titleweight'] = 'bold'\n",
    "rcParams['axes.labelsize'] = 'x-large'\n",
    "rcParams['axes.labelweight'] = 'bold'\n",
    "rcParams['axes.titleweight'] = 'bold'\n",
    "rcParams[\"legend.loc\"] = 'upper left'\n",
    "rcParams[\"legend.markerscale\"] = 2\n",
    "rcParams[\"xtick.labelsize\"] = 'x-large'\n",
    "rcParams[\"ytick.labelsize\"] = 'x-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=1\n",
    "LOOK_BACK = 15\n",
    "LEAD = [5,7,10]\n",
    "TRAIN_TEST = 3825\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 50\n",
    "YEAR_DAYS = 153\n",
    "PATH_RESULT = 'results/result'\n",
    "PATH_LOG = 'results/log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # NumPy\n",
    "random.seed(3) # Python\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 2 3 1 0 0]\n",
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "original = [0, 1, 0, 0, 1, 1, 0, 0]\n",
    "impulse_response = [2, 1]\n",
    "recorded = signal.convolve(impulse_response, original)\n",
    "print (recorded)\n",
    "recovered, remainder = signal.deconvolve([1,2], impulse_response)\n",
    "print (recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_samples():\n",
    "    ganges = pd.read_csv('../Data/Ganges.csv')\n",
    "    dates = (ganges.Year > 1984) & (ganges.Year<2017)\n",
    "    ganges = ganges[dates]\n",
    "    ganges = ganges.reset_index()\n",
    "    for lag in np.arange(-20,16):\n",
    "        new_index = ganges.index + lag\n",
    "        x = ganges.loc[new_index, 'Q (m3/s)' ]\n",
    "        x = pd.DataFrame(x)\n",
    "        x.columns = [''.join(['Q_',str(lag)])]   \n",
    "        x.index = ganges.index\n",
    "        ganges = pd.concat([ganges,x],axis=1)\n",
    "    ganges = ganges[ (ganges.Month > 4) & (ganges.Month<10)]\n",
    "\n",
    "    columns = ['Date',]\n",
    "    for lag in np.arange(1,16):\n",
    "        columns.append(''.join(['Q_',str(lag)]))\n",
    "\n",
    "    Y_Ganges = ganges[columns]\n",
    "\n",
    "    columns = ['Date']\n",
    "    for lag in np.arange(-20,1):\n",
    "        columns.append(''.join(['Q_',str(lag)]))    \n",
    "    X_Ganges = ganges[columns]\n",
    "\n",
    "    X_Ganges.to_csv(''.join([dirname,'X_Ganges.csv']))\n",
    "    Y_Ganges.to_csv(''.join([dirname,'Y_Ganges.csv']))\n",
    "def load_data(river='g'): #g:Ganges; b: Brahmaputra; m: Meghna;\n",
    "    #1934-4-01  2018-07-09\n",
    "    if river =='g':\n",
    "        river_name = 'Ganges'\n",
    "    elif river =='b':\n",
    "        river_name = 'Brahmaputra'\n",
    "    else:\n",
    "        river_name = 'Meghna'\n",
    "    Qx = pd.read_csv('../../data/streamflw_precipitation/X_'+river_name+'.csv', index_col=1,header=0,parse_dates=True)\n",
    "    X = Qx.iloc[:, -LOOK_BACK:]\n",
    "    Qy = pd.read_csv('../../data/streamflw_precipitation/Y_'+river_name+'.csv', index_col=1,header=0,parse_dates=True)\n",
    "    #print (Qy.head())\n",
    "    idy = []\n",
    "    for i in LEAD:\n",
    "        idy.append('Q_'+str(i))\n",
    "    y = Qy.loc[:,idy]\n",
    "    display(X.head())\n",
    "    display(y.head())\n",
    "    return X, y\n",
    "\n",
    "def initialization():\n",
    "    if os.path.isdir(PATH_RESULT) is False:\n",
    "        os.mkdir(PATH_RESULT)\n",
    "    if os.path.isdir(PATH_LOG) is False:\n",
    "        os.mkdir(PATH_LOG)\n",
    "\n",
    "def get_metrics(y, pred):\n",
    "    m_mae = metrics.mean_absolute_error(y, pred)\n",
    "    m_rmse = metrics.mean_squared_error(y, pred)** 0.5\n",
    "    m_r2 = metrics.r2_score(y, pred) \n",
    "    return m_mae,m_rmse,m_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_-14</th>\n",
       "      <th>Q_-13</th>\n",
       "      <th>Q_-12</th>\n",
       "      <th>Q_-11</th>\n",
       "      <th>Q_-10</th>\n",
       "      <th>Q_-9</th>\n",
       "      <th>Q_-8</th>\n",
       "      <th>Q_-7</th>\n",
       "      <th>Q_-6</th>\n",
       "      <th>Q_-5</th>\n",
       "      <th>Q_-4</th>\n",
       "      <th>Q_-3</th>\n",
       "      <th>Q_-2</th>\n",
       "      <th>Q_-1</th>\n",
       "      <th>Q_0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-05-01</th>\n",
       "      <td>824.00</td>\n",
       "      <td>814.00</td>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-02</th>\n",
       "      <td>814.00</td>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-03</th>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-04</th>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "      <td>934.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-05</th>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>952.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Q_-14  Q_-13  Q_-12  Q_-11  Q_-10   Q_-9   Q_-8   Q_-7   Q_-6  \\\n",
       "Date                                                                        \n",
       "1985-05-01 824.00 814.00 815.00 803.00 793.00 800.00 805.00 813.00 824.00   \n",
       "1985-05-02 814.00 815.00 803.00 793.00 800.00 805.00 813.00 824.00 829.00   \n",
       "1985-05-03 815.00 803.00 793.00 800.00 805.00 813.00 824.00 829.00 829.00   \n",
       "1985-05-04 803.00 793.00 800.00 805.00 813.00 824.00 829.00 829.00 852.00   \n",
       "1985-05-05 793.00 800.00 805.00 813.00 824.00 829.00 829.00 852.00 810.00   \n",
       "\n",
       "             Q_-5   Q_-4   Q_-3   Q_-2   Q_-1    Q_0  \n",
       "Date                                                  \n",
       "1985-05-01 829.00 829.00 852.00 810.00 756.00 751.00  \n",
       "1985-05-02 829.00 852.00 810.00 756.00 751.00 830.00  \n",
       "1985-05-03 852.00 810.00 756.00 751.00 830.00 903.00  \n",
       "1985-05-04 810.00 756.00 751.00 830.00 903.00 934.00  \n",
       "1985-05-05 756.00 751.00 830.00 903.00 934.00 952.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_5</th>\n",
       "      <th>Q_7</th>\n",
       "      <th>Q_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-05-01</th>\n",
       "      <td>958.00</td>\n",
       "      <td>918.00</td>\n",
       "      <td>922.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-02</th>\n",
       "      <td>968.00</td>\n",
       "      <td>783.00</td>\n",
       "      <td>893.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-03</th>\n",
       "      <td>918.00</td>\n",
       "      <td>838.00</td>\n",
       "      <td>874.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-04</th>\n",
       "      <td>783.00</td>\n",
       "      <td>922.00</td>\n",
       "      <td>879.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-05</th>\n",
       "      <td>838.00</td>\n",
       "      <td>893.00</td>\n",
       "      <td>861.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q_5    Q_7   Q_10\n",
       "Date                           \n",
       "1985-05-01 958.00 918.00 922.00\n",
       "1985-05-02 968.00 783.00 893.00\n",
       "1985-05-03 918.00 838.00 874.00\n",
       "1985-05-04 783.00 922.00 879.00\n",
       "1985-05-05 838.00 893.00 861.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_X, data_y = load_data('g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann():\n",
    "    ann = Sequential()\n",
    "    ann.add(InputLayer((LOOK_BACK, 1)))\n",
    "    ann.add(Flatten())\n",
    "    ann.add(Dense(100, activation='relu'))\n",
    "    ann.add(Dense(50, activation='relu'))\n",
    "    ann.add(Dense(1))\n",
    "    ann.compile(optimizer='adam', loss='mse')\n",
    "    ann.summary()\n",
    "    return ann\n",
    "\n",
    "def build_cnn():\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=2))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(100, activation='relu'))\n",
    "    cnn.add(Dense(1))\n",
    "    cnn.compile(optimizer='adam', loss='mse')\n",
    "    cnn.summary()\n",
    "    return cnn\n",
    "def build_rnn():\n",
    "    rnn = Sequential()\n",
    "    rnn.add(SimpleRNN(50, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    rnn.add(Dense(100, activation='relu'))\n",
    "    rnn.add(Dense(1))\n",
    "    rnn.compile(optimizer='adam', loss='mse')\n",
    "    rnn.summary()\n",
    "    return rnn\n",
    "def build_lstm():\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(50, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    lstm.add(Dense(100, activation='relu'))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(optimizer='adam', loss='mse')\n",
    "    lstm.summary()\n",
    "    return lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 5 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out_full[ind] += zi\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out = out_full[ind]\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  zf = out_full[ind]\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/kalmanf/kalmanfilter.py:649: RuntimeWarning: divide by zero encountered in divide\n",
      "  R_mat, T_mat)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:606: RuntimeWarning: overflow encountered in exp\n",
      "  newparams = ((1-np.exp(-params))/\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:607: RuntimeWarning: overflow encountered in exp\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:607: RuntimeWarning: invalid value encountered in divide\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:608: RuntimeWarning: overflow encountered in exp\n",
      "  tmp = ((1-np.exp(-params))/\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:609: RuntimeWarning: overflow encountered in exp\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:609: RuntimeWarning: invalid value encountered in divide\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2907.660567646018, 4892.254639918654, 0.8949061242718296)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 152us/step - loss: 93892786.5686 - val_loss: 49083719.3333\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 47680731.5098 - val_loss: 41553499.0196\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 43514948.4314 - val_loss: 40401708.4706\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 39807087.0588 - val_loss: 38885143.6078\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 38462405.8039 - val_loss: 35500718.8039\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 37684494.8627 - val_loss: 33738885.1569\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 35394002.4118 - val_loss: 36884714.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 34092730.0784 - val_loss: 37953228.3529\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 35494491.7353 - val_loss: 30744695.6863\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 31867926.5882 - val_loss: 30371349.6863\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 32991187.5294 - val_loss: 29610792.3725\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 30986127.1176 - val_loss: 29355224.3725\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 30544525.5490 - val_loss: 29371800.2353\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 29098685.4118 - val_loss: 28429402.2549\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 29274626.1569 - val_loss: 27746516.6078\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 28376591.4902 - val_loss: 32158698.3333\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 30769881.9216 - val_loss: 31337699.3922\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 32310047.2941 - val_loss: 31648359.6471\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 29195937.4118 - val_loss: 31658256.9020\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28208878.5098 - val_loss: 28466031.2941\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 27027072.2941 - val_loss: 26983944.2941\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 26637760.3725 - val_loss: 26929327.3725\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 26606213.6667 - val_loss: 27157053.0588\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26597062.3922 - val_loss: 26899191.2157\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 26501125.9020 - val_loss: 27233178.4902\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 26554328.0784 - val_loss: 26931229.5882\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 27us/step - loss: 26314383.3725 - val_loss: 27081707.3333\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26589494.4510 - val_loss: 26787771.9020\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26371275.5490 - val_loss: 26797295.9608\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26322974.0588 - val_loss: 26761860.3725\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26296928.5294 - val_loss: 27107920.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26718618.1569 - val_loss: 26802472.6667\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26230774.1176 - val_loss: 26642742.1373\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26665186.4118 - val_loss: 26857373.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 26293608.9804 - val_loss: 26847182.4118\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 26165151.0000 - val_loss: 26643495.7255\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26340792.5882 - val_loss: 27171678.5686\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26206081.1373 - val_loss: 26706090.0784\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25984745.7843 - val_loss: 26518956.7059\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25923911.2941 - val_loss: 26539909.3529\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25894429.8627 - val_loss: 26517164.6471\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25892622.4510 - val_loss: 26536433.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 25901273.1765 - val_loss: 26516339.9216\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25882777.7647 - val_loss: 26518779.2941\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25884427.8333 - val_loss: 26536823.9608\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25891208.8039 - val_loss: 26523700.1961\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25878771.7451 - val_loss: 26509996.5098\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25854853.6275 - val_loss: 26505809.7059\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25902706.9020 - val_loss: 26521245.2549\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25865475.8039 - val_loss: 26530064.0980\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25877409.6078 - val_loss: 26500765.5882\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25842797.0588 - val_loss: 26494871.8039\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25870502.4510 - val_loss: 26492756.4510\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25840951.5686 - val_loss: 26492532.2157\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25856152.1961 - val_loss: 26500683.1961\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25840211.2745 - val_loss: 26491935.3333\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25876204.7059 - val_loss: 26495394.7843\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25843834.1765 - val_loss: 26487394.8235\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25830703.3333 - val_loss: 26481321.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25845190.9608 - val_loss: 26489341.8235\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25810050.5294 - val_loss: 26479905.2353\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25806737.7647 - val_loss: 26474288.3529\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25810150.6471 - val_loss: 26483884.4902\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25822737.4118 - val_loss: 26496339.1961\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25802467.1569 - val_loss: 26470040.9216\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25809947.1373 - val_loss: 26466939.5294\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25801348.8431 - val_loss: 26479908.0784\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25797207.1569 - val_loss: 26465536.1569\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25798658.5490 - val_loss: 26465266.4510\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25813152.3137 - val_loss: 26456787.8039\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25792397.1569 - val_loss: 26462089.9216\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25771485.1569 - val_loss: 26451971.0000\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25780081.6078 - val_loss: 26454265.8431\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25779980.6078 - val_loss: 26454936.7647\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25808563.0392 - val_loss: 26464452.4706\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25779977.5294 - val_loss: 26445052.0980\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25786732.4706 - val_loss: 26445250.1176\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25806727.5490 - val_loss: 26442423.6667\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25785659.6667 - val_loss: 26436435.4118\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25803053.4510 - val_loss: 26433024.2157\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25811260.5490 - val_loss: 26431714.2549\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25791133.4902 - val_loss: 26429385.6275\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25784913.1961 - val_loss: 26438202.9608\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 25774315.7451 - val_loss: 26421062.1961\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 25725960.3529 - val_loss: 26421842.1765\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 25736264.3725 - val_loss: 26427478.7647\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25752254.0980 - val_loss: 26439717.8431\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25765213.2941 - val_loss: 26412208.6863\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25742535.2745 - val_loss: 26413162.9608\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25797232.9608 - val_loss: 26474068.2549\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25726975.3922 - val_loss: 26403549.5294\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25728117.6471 - val_loss: 26417133.2941\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25714150.9020 - val_loss: 26400461.4510\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25714411.8431 - val_loss: 26389701.2745\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25704808.2353 - val_loss: 26390446.4510\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25735060.2353 - val_loss: 26388605.8235\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25702030.3137 - val_loss: 26422187.6863\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25694997.2157 - val_loss: 26387962.7843\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 27us/step - loss: 25695685.7255 - val_loss: 26385308.7451\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25687813.8235 - val_loss: 26388196.5686\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25692825.5882 - val_loss: 26380028.7255\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25690613.6078 - val_loss: 26387935.6078\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25724778.7843 - val_loss: 26397523.5098\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25690715.8627 - val_loss: 26365480.0392\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25696784.6471 - val_loss: 26366546.1373\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25690033.6078 - val_loss: 26394412.0392\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25724834.7451 - val_loss: 26389441.9020\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25661560.9804 - val_loss: 26360510.2549\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25660258.6667 - val_loss: 26353560.3333\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25671408.0980 - val_loss: 26359812.8235\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25658880.4216 - val_loss: 26354793.9412\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25648020.4314 - val_loss: 26360784.6078\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25654357.9804 - val_loss: 26350709.8431\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25646822.6078 - val_loss: 26351317.8235\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25652326.0000 - val_loss: 26360808.4118\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25638363.6275 - val_loss: 26339549.7843\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25666498.2353 - val_loss: 26347835.9020\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25643912.4706 - val_loss: 26338330.1961\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25668355.3333 - val_loss: 26369759.1765\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25629628.9804 - val_loss: 26364703.4118\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25634915.3137 - val_loss: 26336387.2745\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25681237.5686 - val_loss: 26349917.5294\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25672662.3137 - val_loss: 26327109.0980\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25630503.6275 - val_loss: 26328092.3922\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25629006.3333 - val_loss: 26348630.5686\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25630546.7451 - val_loss: 26316139.7059\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25633215.1176 - val_loss: 26348138.4314\n",
      "Epoch 128/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 38us/step - loss: 25656724.8235 - val_loss: 26321653.1373\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25593307.1569 - val_loss: 26336891.2353\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25643034.7647 - val_loss: 26307951.7843\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25590267.5882 - val_loss: 26318507.5490\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25633674.1961 - val_loss: 26315495.7647\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25615743.4902 - val_loss: 26301898.7647\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25606563.6667 - val_loss: 26300349.6863\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25581068.1078 - val_loss: 26300653.8235\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25588011.7255 - val_loss: 26296084.2745\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25587831.4314 - val_loss: 26305654.7843\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25585180.4902 - val_loss: 26288175.9804\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25592703.5294 - val_loss: 26306660.0980\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25568389.0588 - val_loss: 26307566.8824\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25589480.2353 - val_loss: 26276217.0196\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 25631471.0784 - val_loss: 26295337.3529\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 22us/step - loss: 25560010.7647 - val_loss: 26277172.0784\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25616816.8431 - val_loss: 26295456.2549\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25540858.4706 - val_loss: 26266840.7647\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25593666.2157 - val_loss: 26277435.0980\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25585860.6471 - val_loss: 26266385.3922\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25584942.3922 - val_loss: 26321706.8824\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25596169.6471 - val_loss: 26264352.8627\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25542082.5490 - val_loss: 26313875.8235\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25590239.4510 - val_loss: 26258735.2157\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25569523.8725 - val_loss: 26256447.1765\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25544350.8824 - val_loss: 26281367.3529\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25575748.7059 - val_loss: 26292481.0980\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25569512.0588 - val_loss: 26248491.6667\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25552509.4902 - val_loss: 26261163.9020\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25551840.5882 - val_loss: 26242138.4314\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25551885.9216 - val_loss: 26306259.8039\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25545783.9020 - val_loss: 26239862.2745\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25525784.3529 - val_loss: 26228196.7059\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25523667.9608 - val_loss: 26235653.0000\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25517237.4706 - val_loss: 26232716.9216\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25504528.9020 - val_loss: 26227851.8235\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25494571.1176 - val_loss: 26229655.3922\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25503577.1961 - val_loss: 26224284.6471\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25509072.6275 - val_loss: 26224819.7843\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25514654.6275 - val_loss: 26230129.3333\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25487270.4706 - val_loss: 26229974.9020\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25520142.0784 - val_loss: 26217401.2549\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25486724.0392 - val_loss: 26217572.2941\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25474241.8235 - val_loss: 26210721.0392\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25503981.5294 - val_loss: 26215448.8235\n",
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25510853.3725 - val_loss: 26217471.0392\n",
      "Epoch 174/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25513216.2549 - val_loss: 26210248.9608\n",
      "Epoch 175/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25492484.5490 - val_loss: 26193913.1373\n",
      "Epoch 176/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25468315.7843 - val_loss: 26209428.1765\n",
      "Epoch 177/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25482057.0196 - val_loss: 26195454.5686\n",
      "Epoch 178/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25524726.6471 - val_loss: 26222345.8627\n",
      "Epoch 179/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25465555.2451 - val_loss: 26196587.3922\n",
      "Epoch 180/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25520042.5294 - val_loss: 26279965.5098\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 181/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25485771.2941 - val_loss: 26249912.8627\n",
      "Epoch 182/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25461050.2941 - val_loss: 26220955.8627\n",
      "Epoch 183/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25442036.4902 - val_loss: 26209290.2941\n",
      "Epoch 184/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25439818.8824 - val_loss: 26203038.1765\n",
      "Epoch 185/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25431086.3333 - val_loss: 26194925.9216\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 186/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25426238.0784 - val_loss: 26194371.7451\n",
      "Epoch 187/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25426039.2059 - val_loss: 26194193.6863\n",
      "Epoch 188/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25426044.1176 - val_loss: 26193915.4706\n",
      "Epoch 189/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25426458.4510 - val_loss: 26193987.2549\n",
      "Epoch 190/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425824.1176 - val_loss: 26193312.5686\n",
      "Epoch 191/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25425448.5490 - val_loss: 26193269.1176\n",
      "Epoch 192/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425483.3333 - val_loss: 26192784.7843\n",
      "Epoch 193/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425499.4314 - val_loss: 26192559.3922\n",
      "Epoch 194/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25426160.5490 - val_loss: 26192078.3333\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 35us/step - loss: 25426027.4902 - val_loss: 26192241.1176\n",
      "Epoch 196/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25424993.4902 - val_loss: 26192053.8235\n",
      "Epoch 197/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25425101.2353 - val_loss: 26191973.9608\n",
      "Epoch 198/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25424643.8431 - val_loss: 26191939.5490\n",
      "Epoch 199/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424803.9020 - val_loss: 26191481.8824\n",
      "Epoch 200/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424733.0000 - val_loss: 26191629.3725\n",
      "Epoch 201/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25424589.6667 - val_loss: 26191369.9608\n",
      "Epoch 202/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25424330.6667 - val_loss: 26191317.9412\n",
      "Epoch 203/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25424329.4706 - val_loss: 26191265.4118\n",
      "Epoch 204/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424457.5490 - val_loss: 26190998.5294\n",
      "Epoch 205/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25424093.4510 - val_loss: 26190872.8627\n",
      "Epoch 206/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424418.1961 - val_loss: 26190579.2941\n",
      "Epoch 207/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424001.0784 - val_loss: 26190684.6667\n",
      "Epoch 208/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25424065.5686 - val_loss: 26190495.8039\n",
      "Epoch 209/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424310.4706 - val_loss: 26190561.2353\n",
      "Epoch 210/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423740.3725 - val_loss: 26190408.5686\n",
      "Epoch 211/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423709.4314 - val_loss: 26190168.9020\n",
      "Epoch 212/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25423924.4510 - val_loss: 26190051.4902\n",
      "Epoch 213/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25423988.5490 - val_loss: 26189930.6078\n",
      "Epoch 214/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25423685.1176 - val_loss: 26189786.6471\n",
      "Epoch 215/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423512.9706 - val_loss: 26189756.1373\n",
      "Epoch 216/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25423607.1961 - val_loss: 26189669.0000\n",
      "Epoch 217/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423723.9020 - val_loss: 26189386.2157\n",
      "Epoch 218/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423362.3137 - val_loss: 26189493.8235\n",
      "Epoch 219/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25423448.6863 - val_loss: 26189390.5686\n",
      "Epoch 220/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423242.9412 - val_loss: 26189386.2745\n",
      "Epoch 221/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25423453.4118 - val_loss: 26189103.6078\n",
      "Epoch 222/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25424145.0196 - val_loss: 26188917.8824\n",
      "Epoch 223/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423167.5686 - val_loss: 26189227.4902\n",
      "Epoch 224/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25423533.8627 - val_loss: 26189071.4118\n",
      "Epoch 225/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25423319.2549 - val_loss: 26188903.7255\n",
      "Epoch 226/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25423189.9020 - val_loss: 26188961.0392\n",
      "Epoch 227/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25423245.1961 - val_loss: 26188718.9804\n",
      "Epoch 228/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422956.6667 - val_loss: 26188799.7255\n",
      "Epoch 229/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423010.3922 - val_loss: 26188567.3137\n",
      "Epoch 230/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25423153.8039 - val_loss: 26188761.3725\n",
      "Epoch 231/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422896.5196 - val_loss: 26188661.6471\n",
      "Epoch 232/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25422948.7647 - val_loss: 26188625.9216\n",
      "Epoch 233/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25422754.3137 - val_loss: 26188554.5882\n",
      "Epoch 234/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25422891.8627 - val_loss: 26188414.0980\n",
      "Epoch 235/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423249.8529 - val_loss: 26188563.9216\n",
      "Epoch 236/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25422586.3725 - val_loss: 26188403.1961\n",
      "Epoch 237/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423008.3333 - val_loss: 26188332.0000\n",
      "Epoch 238/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422592.7843 - val_loss: 26188287.5882\n",
      "Epoch 239/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423293.9216 - val_loss: 26188352.7255\n",
      "Epoch 240/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25422999.8824 - val_loss: 26187989.4510\n",
      "Epoch 241/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25422556.7843 - val_loss: 26188002.3137\n",
      "Epoch 242/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25422435.9510 - val_loss: 26188027.0392\n",
      "Epoch 243/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25422482.6667 - val_loss: 26187888.9608\n",
      "Epoch 244/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25423289.3725 - val_loss: 26187885.9608\n",
      "Epoch 245/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25422645.1765 - val_loss: 26187970.4510\n",
      "Epoch 246/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422371.0000 - val_loss: 26187901.3922\n",
      "Epoch 247/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25422353.6176 - val_loss: 26187686.8235\n",
      "Epoch 248/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422201.4020 - val_loss: 26187703.9216\n",
      "Epoch 249/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25422861.6863 - val_loss: 26187898.6275\n",
      "Epoch 250/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422344.8824 - val_loss: 26187733.7843\n",
      "Epoch 251/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422233.9020 - val_loss: 26187596.4706\n",
      "Epoch 252/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422387.7059 - val_loss: 26187470.9412\n",
      "Epoch 253/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422353.2353 - val_loss: 26187393.0000\n",
      "Epoch 254/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25422295.8824 - val_loss: 26187403.8431\n",
      "Epoch 255/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422277.1765 - val_loss: 26187281.1961\n",
      "Epoch 256/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422342.7647 - val_loss: 26187337.5098\n",
      "Epoch 257/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25421856.9216 - val_loss: 26187293.9804\n",
      "Epoch 258/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422543.9412 - val_loss: 26187362.4118\n",
      "Epoch 259/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422729.9020 - val_loss: 26187215.8431\n",
      "Epoch 260/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422245.4314 - val_loss: 26187318.7843\n",
      "Epoch 261/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421781.5686 - val_loss: 26187187.8627\n",
      "Epoch 262/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422900.9216 - val_loss: 26186996.5098\n",
      "Epoch 263/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25421951.7843 - val_loss: 26187087.8431\n",
      "Epoch 264/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421949.9804 - val_loss: 26187019.5294\n",
      "Epoch 265/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422121.8725 - val_loss: 26187022.0000\n",
      "Epoch 266/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25422058.4314 - val_loss: 26187111.9412\n",
      "Epoch 267/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421608.8235 - val_loss: 26186969.3529\n",
      "Epoch 268/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421764.7157 - val_loss: 26186942.2549\n",
      "Epoch 269/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421715.5882 - val_loss: 26186802.4706\n",
      "Epoch 270/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421724.7843 - val_loss: 26186855.3725\n",
      "Epoch 271/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25421714.4314 - val_loss: 26186785.4118\n",
      "Epoch 272/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421659.3725 - val_loss: 26186737.3333\n",
      "Epoch 273/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421608.6078 - val_loss: 26186698.1961\n",
      "Epoch 274/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421626.5000 - val_loss: 26186871.2549\n",
      "Epoch 275/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421766.7647 - val_loss: 26186797.7451\n",
      "Epoch 276/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421680.8235 - val_loss: 26186626.7451\n",
      "Epoch 277/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421381.1961 - val_loss: 26186568.0784\n",
      "Epoch 278/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421374.5490 - val_loss: 26186534.7451\n",
      "Epoch 279/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421530.5098 - val_loss: 26186596.2157\n",
      "Epoch 280/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421163.0000 - val_loss: 26186484.9804\n",
      "Epoch 281/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421341.7059 - val_loss: 26186503.2941\n",
      "Epoch 282/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421760.3137 - val_loss: 26186617.0392\n",
      "Epoch 283/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421030.2549 - val_loss: 26186450.1373\n",
      "Epoch 284/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421309.3922 - val_loss: 26186386.1176\n",
      "Epoch 285/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421939.6471 - val_loss: 26186496.4706\n",
      "Epoch 286/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421978.7451 - val_loss: 26186278.8039\n",
      "Epoch 287/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421062.3137 - val_loss: 26186285.8431\n",
      "Epoch 288/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421240.7843 - val_loss: 26186328.8235\n",
      "Epoch 289/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421251.8627 - val_loss: 26186176.2549\n",
      "Epoch 290/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25421727.7255 - val_loss: 26186139.1961\n",
      "Epoch 291/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421368.4902 - val_loss: 26186099.9020\n",
      "Epoch 292/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 25421487.5000 - val_loss: 26186155.2353\n",
      "Epoch 293/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420826.8824 - val_loss: 26186097.3725\n",
      "Epoch 294/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25420889.4902 - val_loss: 26186045.3725\n",
      "Epoch 295/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421057.1569 - val_loss: 26185963.4510\n",
      "Epoch 296/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25420923.3725 - val_loss: 26185928.1961\n",
      "Epoch 297/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420924.5686 - val_loss: 26185911.2353\n",
      "Epoch 298/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420845.9216 - val_loss: 26185901.4118\n",
      "Epoch 299/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421769.6863 - val_loss: 26186086.5686\n",
      "Epoch 300/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25420516.6471 - val_loss: 26185847.1569\n",
      "Epoch 301/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421263.9020 - val_loss: 26185970.1373\n",
      "Epoch 302/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420638.7647 - val_loss: 26185846.6471\n",
      "Epoch 303/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25420930.2745 - val_loss: 26185710.5882\n",
      "Epoch 304/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420836.7255 - val_loss: 26185761.2745\n",
      "Epoch 305/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420704.5882 - val_loss: 26185756.3725\n",
      "Epoch 306/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420642.6471 - val_loss: 26185793.8431\n",
      "Epoch 307/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420560.1373 - val_loss: 26185530.1961\n",
      "Epoch 308/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420499.4118 - val_loss: 26185583.2745\n",
      "Epoch 309/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25420340.0000 - val_loss: 26185583.4902\n",
      "Epoch 310/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420987.0980 - val_loss: 26185565.8824\n",
      "Epoch 311/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420524.9216 - val_loss: 26185613.9804\n",
      "Epoch 312/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25420687.5294 - val_loss: 26185409.9020\n",
      "Epoch 313/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420736.9804 - val_loss: 26185302.3333\n",
      "Epoch 314/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25421065.1373 - val_loss: 26185608.7647\n",
      "Epoch 315/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420454.1765 - val_loss: 26185427.2941\n",
      "Epoch 316/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25420336.2157 - val_loss: 26185332.6078\n",
      "Epoch 317/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25421142.9608 - val_loss: 26185389.8431\n",
      "Epoch 318/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420342.8235 - val_loss: 26185357.1373\n",
      "\n",
      "Epoch 00318: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 319/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25419875.9902 - val_loss: 26185356.6863\n",
      "Epoch 320/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25419902.4510 - val_loss: 26185351.2549\n",
      "Epoch 321/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25419878.0588 - val_loss: 26185357.2549\n",
      "Epoch 322/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25419897.7451 - val_loss: 26185350.5490\n",
      "Epoch 323/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25419878.4314 - val_loss: 26185349.3333\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 324/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25419870.7451 - val_loss: 26185350.4706\n",
      "Epoch 325/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.1176 - val_loss: 26185350.2353\n",
      "Epoch 326/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25419870.1176 - val_loss: 26185350.3725\n",
      "Epoch 327/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25419870.6667 - val_loss: 26185350.2549\n",
      "Epoch 328/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25419870.3137 - val_loss: 26185349.8039\n",
      "\n",
      "Epoch 00328: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.8039 - val_loss: 26185349.8039\n",
      "Epoch 330/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25419870.7451 - val_loss: 26185349.8039\n",
      "Epoch 331/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.9608 - val_loss: 26185349.6471\n",
      "Epoch 332/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25419870.4118 - val_loss: 26185349.7647\n",
      "Epoch 333/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25419870.2941 - val_loss: 26185349.7647\n",
      "\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 1.00000008274e-11.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 120544575.0196 - val_loss: 46537400.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 45517998.0784 - val_loss: 44576693.0980\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 41233442.1765 - val_loss: 37326838.8235\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 38820193.3529 - val_loss: 35634166.1569\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 37208767.4118 - val_loss: 38850934.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 35596801.0588 - val_loss: 33311926.0784\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 33663811.5294 - val_loss: 32328503.8824\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 32742940.2745 - val_loss: 32285271.5098\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 34567445.6471 - val_loss: 39390784.5882\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 33603220.0196 - val_loss: 30123633.8235\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 31309100.2941 - val_loss: 31207081.4118\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 32556031.5882 - val_loss: 29302807.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 32045132.1176 - val_loss: 32304040.1961\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 29590610.3922 - val_loss: 28690491.5882\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 29232220.9608 - val_loss: 30272915.0980\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 30135781.1765 - val_loss: 28783906.2353\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 30966215.6471 - val_loss: 28016813.7843\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28930208.2549 - val_loss: 28285920.2745\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 30254504.1373 - val_loss: 31089218.6667\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 31465016.3333 - val_loss: 30428420.2549\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 28601711.7059 - val_loss: 28855708.0000\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 31634053.6471 - val_loss: 26966624.6667\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 27983275.1569 - val_loss: 26541010.1176\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 27528148.6471 - val_loss: 34581467.0588\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 28877569.4902 - val_loss: 29056742.5882\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 28765944.7451 - val_loss: 26285034.1176\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 27742265.2549 - val_loss: 31963261.5294\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 28821930.3137 - val_loss: 26228787.5882\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26925705.8235 - val_loss: 26163466.4706\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27971340.9216 - val_loss: 26621642.3922\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27402208.6275 - val_loss: 30465266.6471\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 26460298.7451 - val_loss: 25932493.6078\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 29787598.7843 - val_loss: 25397549.8039\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27512321.0392 - val_loss: 30556507.8431\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28624302.3922 - val_loss: 30441015.5686\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26082519.9020 - val_loss: 25072037.5490\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 27574119.4118 - val_loss: 27666322.6863\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26713316.0784 - val_loss: 27111568.7059\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 28338924.0588 - val_loss: 25630703.3529\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 28497190.8627 - val_loss: 27134779.7059\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 29880654.7647 - val_loss: 35070799.9608\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25828696.3529 - val_loss: 25656031.9216\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24758450.1373 - val_loss: 25053691.0196\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 24725144.0392 - val_loss: 25071203.7843\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24548551.2941 - val_loss: 25099698.3137\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24620061.9608 - val_loss: 25116285.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24542732.4902 - val_loss: 25946609.1961\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24584189.4118 - val_loss: 25083228.2157\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24335361.7647 - val_loss: 25043519.1765\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24323402.8039 - val_loss: 25065827.7059\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 24311683.7843 - val_loss: 25029255.5882\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 24312188.3725 - val_loss: 25072113.0784\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24314488.5490 - val_loss: 25102830.1373\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 24305297.6667 - val_loss: 25067493.2941\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24309984.0980 - val_loss: 25050231.4118\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24314180.1569 - val_loss: 25031917.4902\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24285684.8824 - val_loss: 25037085.2353\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24287243.0980 - val_loss: 25025241.3725\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24284117.1176 - val_loss: 25039447.7451\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24282863.0000 - val_loss: 25044486.0000\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24284050.3529 - val_loss: 25041139.4902\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24283976.9412 - val_loss: 25046077.5098\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24284771.4314 - val_loss: 25041660.1765\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24279470.6667 - val_loss: 25042460.4706\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24279536.1176 - val_loss: 25042623.1569\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279455.7647 - val_loss: 25043259.7255\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 24279445.5098 - val_loss: 25042417.8627\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279439.7255 - val_loss: 25043284.6667\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279144.7451 - val_loss: 25043285.0980\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24279135.2745 - val_loss: 25043276.9020\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24279128.1569 - val_loss: 25043279.3725\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279131.8627 - val_loss: 25043297.9804\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279128.0196 - val_loss: 25043301.7255\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24279120.2549 - val_loss: 25043302.0392\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24279120.6275 - val_loss: 25043300.9608\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24279120.4706 - val_loss: 25043302.5294\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24279120.1667 - val_loss: 25043302.2745\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 24279120.1569 - val_loss: 25043302.1569\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 65826311.9216 - val_loss: 47074178.1176\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 39830344.2353 - val_loss: 43613818.2745\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 38016291.5686 - val_loss: 37088118.7451\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 36815741.1569 - val_loss: 34504696.6863\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 35344467.0196 - val_loss: 39566928.4706\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 34466731.9608 - val_loss: 32690925.6863\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 31770075.2941 - val_loss: 30976415.4902\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 31316624.8039 - val_loss: 33849190.8627\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 32614712.3529 - val_loss: 33021039.3333\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 29377246.3137 - val_loss: 32825637.6078\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 32069478.8431 - val_loss: 29151170.5490\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 30058976.1961 - val_loss: 35337605.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 28171072.5686 - val_loss: 28861497.1961\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27609409.3725 - val_loss: 28108267.7451\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27474984.0980 - val_loss: 29141887.6863\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 26842688.1569 - val_loss: 27453919.5882\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 27997702.4706 - val_loss: 28285493.1569\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 27973147.0784 - val_loss: 28935434.3725\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 30720476.0588 - val_loss: 40944016.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27084622.5882 - val_loss: 26947928.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 28000554.8431 - val_loss: 30103337.0980\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 29572562.8431 - val_loss: 27377386.5098\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 29258425.6863 - val_loss: 28522151.0784\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26804227.1176 - val_loss: 27295753.0196\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26009668.2353 - val_loss: 32755472.3922\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25846565.7451 - val_loss: 26783962.6471\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25457765.9608 - val_loss: 27573895.3333\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25167025.8235 - val_loss: 26463732.1569\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24970371.1176 - val_loss: 26396109.2941\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24868424.6863 - val_loss: 26304594.9804\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25111585.9412 - val_loss: 26515050.6863\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24973016.2549 - val_loss: 26532267.2549\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 24996420.6569 - val_loss: 26990568.3137\n",
      "Epoch 34/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 40us/step - loss: 25032524.7059 - val_loss: 26100072.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25548655.3333 - val_loss: 26119802.1765\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25075140.4706 - val_loss: 26657515.0392\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25190916.6667 - val_loss: 26517141.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24872277.5686 - val_loss: 26138300.5294\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24792706.3235 - val_loss: 26151555.1373\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 24666553.4510 - val_loss: 26337137.6667\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 24689450.1961 - val_loss: 26271354.2353\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24637654.9608 - val_loss: 26170484.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24642096.0980 - val_loss: 26243117.6275\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 24637666.2157 - val_loss: 26181644.0000\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24620708.8824 - val_loss: 26181838.9216\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24621101.5490 - val_loss: 26178688.8039\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 24622445.4706 - val_loss: 26187756.1765\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24621964.7059 - val_loss: 26180701.4314\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24620925.2157 - val_loss: 26189804.0980\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24618395.7451 - val_loss: 26191223.8235\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24618454.1373 - val_loss: 26191503.6078\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24617762.2157 - val_loss: 26190873.1765\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24617895.7843 - val_loss: 26189948.6863\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24617698.2745 - val_loss: 26190494.7843\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 243us/step - loss: 93162582.1961 - val_loss: 52219979.6471\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 52366867.5686 - val_loss: 43531344.0784\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 43364544.9020 - val_loss: 35891263.2941\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 37137540.0000 - val_loss: 38599103.8431\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 35710214.6275 - val_loss: 33873025.1765\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 36704742.8824 - val_loss: 32032433.4314\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33987347.3922 - val_loss: 29654903.1569\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 30903245.2549 - val_loss: 29911226.7451\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 30339585.3725 - val_loss: 28505497.1569\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 30469382.7255 - val_loss: 28369478.7451\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 28918814.9020 - val_loss: 28077086.2157\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 29998911.2941 - val_loss: 28573383.9608\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 29583466.2549 - val_loss: 31179843.4314\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 28723108.7647 - val_loss: 30994227.0196\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 32349858.9020 - val_loss: 40296663.6078\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 29888783.9804 - val_loss: 33922150.7451\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 28362534.4902 - val_loss: 27031481.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 27344286.8824 - val_loss: 26900842.3333\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26990724.4314 - val_loss: 26838942.0196\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 27041716.3725 - val_loss: 26644403.7843\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 27129312.6863 - val_loss: 26696462.7843\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 26919980.9804 - val_loss: 26522661.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26858876.0588 - val_loss: 26813890.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26875542.7843 - val_loss: 27091620.2549\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 27001768.3235 - val_loss: 26435145.8039\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26928417.7941 - val_loss: 26407837.2549\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26791972.5686 - val_loss: 26400676.0392\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 27103226.1961 - val_loss: 26588791.4510\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26713049.5490 - val_loss: 26556805.4510\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26737770.0784 - val_loss: 26455830.1961\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27005964.6471 - val_loss: 26509732.1765\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26720136.1373 - val_loss: 26304904.8431\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26698572.7647 - val_loss: 26356809.1961\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26751212.3137 - val_loss: 26265809.8627\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 26734156.5098 - val_loss: 27393481.4314\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 26555007.7647 - val_loss: 26198288.4510\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 75us/step - loss: 26650113.2157 - val_loss: 26212170.5686\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26747355.1569 - val_loss: 26262710.0784\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26742194.9020 - val_loss: 26188332.6667\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 26532055.0588 - val_loss: 26308077.4902\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26351771.0882 - val_loss: 26522606.2157\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 26528997.6275 - val_loss: 26112812.8039\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 26609002.6667 - val_loss: 26086024.3922\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26645498.9216 - val_loss: 26245784.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 26400337.6471 - val_loss: 26202825.2353\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 26473532.0980 - val_loss: 26007431.8627\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 26527851.2549 - val_loss: 26052001.4902\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26424215.6275 - val_loss: 25987807.0784\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26361284.4510 - val_loss: 25959310.1765\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26499003.6863 - val_loss: 26153111.3922\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 26712495.0196 - val_loss: 26266373.8627\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26351843.4510 - val_loss: 26900832.7647\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 26828695.0588 - val_loss: 25945881.4510\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26568616.1961 - val_loss: 26086135.7255\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26243200.9020 - val_loss: 25919311.0392\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26289221.7255 - val_loss: 25886160.7647\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26224088.9608 - val_loss: 25857599.7059\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 26162313.16 - 0s 78us/step - loss: 26107873.1961 - val_loss: 25838205.8235\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26369776.5882 - val_loss: 25983228.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 26337257.9314 - val_loss: 25818209.8039\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 26283633.9216 - val_loss: 26714433.2157\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 26312934.3922 - val_loss: 25753595.7451\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26248755.7451 - val_loss: 26016117.8235\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26429732.9216 - val_loss: 25753953.0980\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26118695.7451 - val_loss: 25848513.8039\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25973414.2941 - val_loss: 27340839.4314\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26411796.8039 - val_loss: 25714783.4314\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26595901.6471 - val_loss: 25674012.5294\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25888265.0784 - val_loss: 26125341.5294\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26175949.3333 - val_loss: 25896648.3922\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26011915.8235 - val_loss: 25783858.3137\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25913769.3725 - val_loss: 25934698.9020\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26255403.6471 - val_loss: 26535175.0980\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 26125441.9608 - val_loss: 25605844.1373\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25809634.7255 - val_loss: 25613259.9804\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25809655.7059 - val_loss: 25604381.0000\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25820071.1176 - val_loss: 25612690.8824\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25843904.4706 - val_loss: 25649532.4118\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25873809.9412 - val_loss: 25594960.0392\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25830629.7647 - val_loss: 25604698.3529\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25810221.8235 - val_loss: 25593035.9216\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25816231.0980 - val_loss: 25590408.0392\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25790979.2941 - val_loss: 25591031.0196\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25797385.2255 - val_loss: 25589364.1176\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25790781.8235 - val_loss: 25589141.8627\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25803878.7059 - val_loss: 25587724.3725\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25831724.4314 - val_loss: 25610472.0980\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25867188.3529 - val_loss: 25644601.7647\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25827592.2941 - val_loss: 25587738.5686\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25777225.7059 - val_loss: 25581447.2157\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25788780.9608 - val_loss: 25585866.5098\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25804256.3725 - val_loss: 25577827.0784\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25790989.9608 - val_loss: 25585986.7451\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25817176.0588 - val_loss: 25608827.8627\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25822276.6471 - val_loss: 25571564.1373\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25801463.0588 - val_loss: 25575310.5294\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25784677.7255 - val_loss: 25572677.0588\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25782337.9020 - val_loss: 25576574.2353\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25854073.7647 - val_loss: 25570677.5490\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25777168.6471 - val_loss: 25570719.9804\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25792362.8039 - val_loss: 25567405.8431\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25784531.9412 - val_loss: 25564182.1569\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25774500.6078 - val_loss: 25561087.3529\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25759770.6275 - val_loss: 25564828.4902\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 85us/step - loss: 25791133.7451 - val_loss: 25568448.3333\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25766249.5882 - val_loss: 25565172.0196\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25758004.5000 - val_loss: 25561499.4706\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25770748.1176 - val_loss: 25560332.4314\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25762761.1078 - val_loss: 25560439.5098\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25751125.8235 - val_loss: 25557815.9608\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25752598.8235 - val_loss: 25555953.7059\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25749758.0196 - val_loss: 25557232.0196\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25742867.0098 - val_loss: 25556890.1569\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25741275.3137 - val_loss: 25550931.2549\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25770953.8431 - val_loss: 25551477.6275\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25756742.4510 - val_loss: 25554161.8039\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 25775884.3039 - val_loss: 25572859.5490\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25796504.6667 - val_loss: 25560648.3137\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25739219.6667 - val_loss: 25553238.6275\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25728960.9020 - val_loss: 25548851.0784\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25724626.1569 - val_loss: 25549018.5098\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25721137.0196 - val_loss: 25544809.2745\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25720331.8333 - val_loss: 25544533.3725\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25721331.5098 - val_loss: 25543485.8824\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25719039.2745 - val_loss: 25543471.0784\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25718354.3529 - val_loss: 25543391.4510\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25722508.1373 - val_loss: 25542656.6275\n",
      "Epoch 128/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25719284.4608 - val_loss: 25543145.2157\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25721536.5686 - val_loss: 25543191.2353\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25719170.8039 - val_loss: 25542367.8824\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25716919.4510 - val_loss: 25542494.7059\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25723158.6471 - val_loss: 25542561.8431\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25719389.9608 - val_loss: 25541968.2157\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25719379.0980 - val_loss: 25542006.1373\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25718088.3529 - val_loss: 25541582.4706\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25719379.2745 - val_loss: 25541490.0784\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25719122.7647 - val_loss: 25541850.5098\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25720343.6275 - val_loss: 25541839.8627\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25717644.0784 - val_loss: 25541043.8039\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25717600.0196 - val_loss: 25541092.4118\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25720038.9216 - val_loss: 25541283.9804\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25720635.2843 - val_loss: 25540775.3333\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25715716.9804 - val_loss: 25540860.0980\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25720553.0784 - val_loss: 25540755.0588\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25714896.8824 - val_loss: 25540192.4118\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25718027.7451 - val_loss: 25540598.9412\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25716006.3725 - val_loss: 25540171.7843\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25715212.2353 - val_loss: 25539963.4902\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25722184.0588 - val_loss: 25540082.0588\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25718933.3529 - val_loss: 25540288.1765\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25714452.0392 - val_loss: 25539821.6078\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25715568.0588 - val_loss: 25539843.4510\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25716528.7059 - val_loss: 25539898.2745\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25715027.3725 - val_loss: 25539571.7059\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25716078.5490 - val_loss: 25539320.6863\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25714088.3039 - val_loss: 25539573.4510\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25717070.8627 - val_loss: 25539195.9804\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25713170.4608 - val_loss: 25538959.1961\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25716385.7647 - val_loss: 25538898.0000\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25725885.6863 - val_loss: 25539525.7647\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25712867.2745 - val_loss: 25538533.0588\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25718018.2353 - val_loss: 25538635.4118\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25713694.4510 - val_loss: 25538373.1176\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25716471.7255 - val_loss: 25538653.0588\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25713137.7255 - val_loss: 25538257.9804\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25715961.6471 - val_loss: 25538237.9608\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25712807.7647 - val_loss: 25537880.1176\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25716229.9804 - val_loss: 25537650.0980\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25716731.7549 - val_loss: 25538063.1373\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25712756.1961 - val_loss: 25537651.9020\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25712683.8725 - val_loss: 25537428.4314\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25717142.1961 - val_loss: 25537589.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25712133.1765 - val_loss: 25537264.2745\n",
      "Epoch 174/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25710632.1176 - val_loss: 25537400.3725\n",
      "Epoch 175/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25711110.4216 - val_loss: 25537230.1569\n",
      "Epoch 176/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25713193.4902 - val_loss: 25536996.4314\n",
      "Epoch 177/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25714105.0784 - val_loss: 25537083.6078\n",
      "Epoch 178/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25714172.9804 - val_loss: 25536993.0392\n",
      "Epoch 179/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25714011.4706 - val_loss: 25536845.6471\n",
      "Epoch 180/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25710955.0000 - val_loss: 25536484.1961\n",
      "Epoch 181/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25711637.7941 - val_loss: 25536651.7451\n",
      "Epoch 182/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25710556.4118 - val_loss: 25536555.2549\n",
      "Epoch 183/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25715554.4216 - val_loss: 25536502.3529\n",
      "Epoch 184/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25708060.2157 - val_loss: 25536551.5490\n",
      "Epoch 185/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25712405.7451 - val_loss: 25536217.2353\n",
      "Epoch 186/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25710913.9804 - val_loss: 25536385.9216\n",
      "Epoch 187/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25712438.2549 - val_loss: 25536162.0196\n",
      "Epoch 188/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25708870.4118 - val_loss: 25536102.0000\n",
      "Epoch 189/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25711268.7059 - val_loss: 25535645.0784\n",
      "Epoch 190/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25712769.6275 - val_loss: 25536250.5882\n",
      "Epoch 191/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25708031.9020 - val_loss: 25535926.7647\n",
      "Epoch 192/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25714519.1961 - val_loss: 25535656.5098\n",
      "Epoch 193/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25718434.7059 - val_loss: 25535542.1176\n",
      "Epoch 194/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25707909.2157 - val_loss: 25534833.2941\n",
      "Epoch 195/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25710341.3333 - val_loss: 25535067.7647\n",
      "Epoch 196/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25708654.9216 - val_loss: 25534962.2353\n",
      "Epoch 197/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25709720.2745 - val_loss: 25535034.6667\n",
      "Epoch 198/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25709162.9412 - val_loss: 25534857.2157\n",
      "Epoch 199/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25707848.5098 - val_loss: 25534624.0588\n",
      "Epoch 200/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25708286.4902 - val_loss: 25534840.2941\n",
      "Epoch 201/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25709233.1373 - val_loss: 25534685.5098\n",
      "Epoch 202/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25707007.8235 - val_loss: 25534259.0980\n",
      "Epoch 203/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25708780.2549 - val_loss: 25534573.4510\n",
      "Epoch 204/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25711730.7255 - val_loss: 25534463.7255\n",
      "Epoch 205/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25711381.7843 - val_loss: 25534350.8824\n",
      "Epoch 206/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25710846.9020 - val_loss: 25533802.8431\n",
      "Epoch 207/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25707467.7059 - val_loss: 25533605.9216\n",
      "Epoch 208/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25714627.1961 - val_loss: 25533563.3725\n",
      "Epoch 209/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25710259.3922 - val_loss: 25533898.3922\n",
      "Epoch 210/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25707487.0784 - val_loss: 25533853.3137\n",
      "Epoch 211/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25708194.8333 - val_loss: 25533810.0980\n",
      "Epoch 212/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25705624.7549 - val_loss: 25533078.4510\n",
      "Epoch 213/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25708607.0588 - val_loss: 25533177.0588\n",
      "Epoch 214/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25705202.9608 - val_loss: 25533247.1765\n",
      "Epoch 215/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25705188.2157 - val_loss: 25533160.3333\n",
      "Epoch 216/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25708568.0392 - val_loss: 25533010.1569\n",
      "Epoch 217/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25713508.3333 - val_loss: 25532708.5098\n",
      "Epoch 218/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25714792.5490 - val_loss: 25533654.9020\n",
      "Epoch 219/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25703558.5490 - val_loss: 25532382.7647\n",
      "Epoch 220/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25706116.8431 - val_loss: 25532456.9804\n",
      "Epoch 221/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25703699.5686 - val_loss: 25532547.0784\n",
      "Epoch 222/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25704222.0588 - val_loss: 25532503.2745\n",
      "Epoch 223/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25705598.5882 - val_loss: 25532153.7647\n",
      "Epoch 224/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25704280.8824 - val_loss: 25532259.1765\n",
      "Epoch 225/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25705213.0000 - val_loss: 25531805.9412\n",
      "Epoch 226/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25708898.9804 - val_loss: 25532824.6863\n",
      "Epoch 227/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25706451.7647 - val_loss: 25531492.0980\n",
      "Epoch 228/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25706671.3137 - val_loss: 25531631.9216\n",
      "Epoch 229/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25707624.6667 - val_loss: 25531771.2353\n",
      "Epoch 230/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25704321.5294 - val_loss: 25531433.1961\n",
      "Epoch 231/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25705389.6275 - val_loss: 25531456.7843\n",
      "Epoch 232/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25704171.6863 - val_loss: 25531522.6078\n",
      "Epoch 233/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25708127.9608 - val_loss: 25531101.5098\n",
      "Epoch 234/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25706351.2549 - val_loss: 25531510.0392\n",
      "Epoch 235/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25702439.5294 - val_loss: 25530885.1176\n",
      "Epoch 236/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25704628.3235 - val_loss: 25531042.2941\n",
      "Epoch 237/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25705257.3529 - val_loss: 25530429.3725\n",
      "Epoch 238/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25703237.8039 - val_loss: 25530631.0588\n",
      "Epoch 239/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 25430295.28 - 0s 80us/step - loss: 25706297.2745 - val_loss: 25530879.1373\n",
      "Epoch 240/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25706954.9216 - val_loss: 25530723.7647\n",
      "Epoch 241/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 89us/step - loss: 25703109.3529 - val_loss: 25530159.0392\n",
      "Epoch 242/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25705636.1373 - val_loss: 25530103.7843\n",
      "Epoch 243/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25701943.7255 - val_loss: 25530007.9608\n",
      "Epoch 244/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25707330.8333 - val_loss: 25530410.7059\n",
      "Epoch 245/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25701273.2745 - val_loss: 25529955.9804\n",
      "Epoch 246/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25704221.2941 - val_loss: 25529451.9804\n",
      "Epoch 247/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25700456.8235 - val_loss: 25529608.0588\n",
      "Epoch 248/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25700544.7059 - val_loss: 25529397.2941\n",
      "Epoch 249/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25709417.6078 - val_loss: 25529786.1765\n",
      "Epoch 250/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25707875.8431 - val_loss: 25530080.4902\n",
      "Epoch 251/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25702173.8824 - val_loss: 25529204.7255\n",
      "Epoch 252/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25700582.4706 - val_loss: 25529056.6275\n",
      "Epoch 253/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25703804.8824 - val_loss: 25529748.8627\n",
      "Epoch 254/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25709528.3725 - val_loss: 25528965.1569\n",
      "Epoch 255/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25700192.0000 - val_loss: 25528664.2353\n",
      "Epoch 256/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25701264.4314 - val_loss: 25528528.1961\n",
      "Epoch 257/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25700579.0000 - val_loss: 25528576.2941\n",
      "Epoch 258/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25701295.0392 - val_loss: 25528207.2549\n",
      "Epoch 259/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25703932.5294 - val_loss: 25527939.5294\n",
      "Epoch 260/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25699522.5490 - val_loss: 25528386.6863\n",
      "Epoch 261/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25702512.6275 - val_loss: 25528130.6275\n",
      "Epoch 262/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25699170.3137 - val_loss: 25528084.0784\n",
      "Epoch 263/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25698533.9608 - val_loss: 25527677.8431\n",
      "Epoch 264/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25701496.2843 - val_loss: 25527719.1961\n",
      "Epoch 265/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25700688.9412 - val_loss: 25528052.3137\n",
      "Epoch 266/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25702236.4510 - val_loss: 25527222.2549\n",
      "Epoch 267/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25698212.0392 - val_loss: 25527448.7843\n",
      "Epoch 268/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25700640.2353 - val_loss: 25527323.0000\n",
      "Epoch 269/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25698329.0000 - val_loss: 25527347.6863\n",
      "Epoch 270/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25697624.9412 - val_loss: 25527154.5490\n",
      "Epoch 271/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697610.8235 - val_loss: 25527216.8824\n",
      "Epoch 272/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25699259.3333 - val_loss: 25526714.2549\n",
      "Epoch 273/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25701864.6667 - val_loss: 25526602.2941\n",
      "Epoch 274/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697226.3137 - val_loss: 25526433.9608\n",
      "Epoch 275/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25697754.0196 - val_loss: 25526381.3529\n",
      "Epoch 276/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697879.4706 - val_loss: 25526202.9020\n",
      "Epoch 277/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25702347.4706 - val_loss: 25526111.8824\n",
      "Epoch 278/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25700316.1569 - val_loss: 25526323.5294\n",
      "Epoch 279/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25696782.2157 - val_loss: 25525806.1176\n",
      "Epoch 280/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25697330.7255 - val_loss: 25526171.3725\n",
      "Epoch 281/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25696079.2353 - val_loss: 25525636.4314\n",
      "Epoch 282/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25701253.7647 - val_loss: 25525859.1961\n",
      "Epoch 283/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25715551.0000 - val_loss: 25525689.1569\n",
      "Epoch 284/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25698404.8235 - val_loss: 25525486.8235\n",
      "Epoch 285/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25695751.2549 - val_loss: 25525237.0588\n",
      "Epoch 286/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25696671.8431 - val_loss: 25525471.1765\n",
      "Epoch 287/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25698789.4118 - val_loss: 25525655.5686\n",
      "Epoch 288/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25696897.5490 - val_loss: 25524811.6078\n",
      "Epoch 289/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25695906.6471 - val_loss: 25524810.7059\n",
      "Epoch 290/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25697081.0392 - val_loss: 25524783.0784\n",
      "Epoch 291/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25697229.0000 - val_loss: 25524710.1765\n",
      "Epoch 292/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25698439.9608 - val_loss: 25524404.9216\n",
      "Epoch 293/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25697465.7647 - val_loss: 25524432.6863\n",
      "Epoch 294/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25696282.9804 - val_loss: 25524383.7843\n",
      "Epoch 295/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25699613.3333 - val_loss: 25525112.3922\n",
      "Epoch 296/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25696731.1961 - val_loss: 25524358.7843\n",
      "Epoch 297/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25693915.6863 - val_loss: 25524128.6667\n",
      "Epoch 298/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25697219.0392 - val_loss: 25523887.6275\n",
      "Epoch 299/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25695314.5490 - val_loss: 25523746.0000\n",
      "Epoch 300/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25695570.7843 - val_loss: 25523790.1373\n",
      "Epoch 301/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25693056.4118 - val_loss: 25523697.4902\n",
      "Epoch 302/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25694515.0196 - val_loss: 25523596.7255\n",
      "Epoch 303/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25693292.8039 - val_loss: 25523316.4902\n",
      "Epoch 304/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25694612.7843 - val_loss: 25523236.0588\n",
      "Epoch 305/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25699334.6471 - val_loss: 25523431.8627\n",
      "Epoch 306/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25693893.9608 - val_loss: 25522981.0784\n",
      "Epoch 307/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25695285.9020 - val_loss: 25523083.7255\n",
      "Epoch 308/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25696035.7255 - val_loss: 25522569.4902\n",
      "Epoch 309/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25693543.8627 - val_loss: 25522599.4706\n",
      "Epoch 310/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25698510.9608 - val_loss: 25522701.6667\n",
      "Epoch 311/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25696855.0000 - val_loss: 25523485.4510\n",
      "Epoch 312/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25691933.1765 - val_loss: 25522223.6078\n",
      "Epoch 313/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25695191.7843 - val_loss: 25522222.7451\n",
      "Epoch 314/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25692482.3725 - val_loss: 25522139.2549\n",
      "Epoch 315/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25693100.7843 - val_loss: 25522108.0000\n",
      "Epoch 316/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25691869.8235 - val_loss: 25522138.5098\n",
      "Epoch 317/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25694602.9216 - val_loss: 25521968.8627\n",
      "Epoch 318/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25700458.8039 - val_loss: 25521587.6471\n",
      "Epoch 319/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25694575.7843 - val_loss: 25521551.1961\n",
      "Epoch 320/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25694192.3137 - val_loss: 25522304.0392\n",
      "Epoch 321/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25693173.6471 - val_loss: 25521428.6275\n",
      "Epoch 322/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25692416.0980 - val_loss: 25521107.7843\n",
      "Epoch 323/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25693487.9608 - val_loss: 25521366.0392\n",
      "Epoch 324/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25689755.9216 - val_loss: 25521065.2549\n",
      "Epoch 325/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25694699.5098 - val_loss: 25521261.4706\n",
      "Epoch 326/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25692108.9118 - val_loss: 25520752.4902\n",
      "Epoch 327/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25697476.4314 - val_loss: 25520876.4902\n",
      "Epoch 328/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25691839.9510 - val_loss: 25520375.0000\n",
      "Epoch 329/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25689447.4118 - val_loss: 25520659.2745\n",
      "Epoch 330/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25690923.1373 - val_loss: 25520143.6667\n",
      "Epoch 331/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25688323.0588 - val_loss: 25520024.3333\n",
      "Epoch 332/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25689945.6471 - val_loss: 25520060.2745\n",
      "Epoch 333/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25691238.9608 - val_loss: 25519766.2157\n",
      "Epoch 334/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25690691.3725 - val_loss: 25520268.8627\n",
      "Epoch 335/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25689981.9216 - val_loss: 25519947.5098\n",
      "Epoch 336/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25688574.7647 - val_loss: 25519458.5882\n",
      "Epoch 337/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25689705.0392 - val_loss: 25519626.1176\n",
      "Epoch 338/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25691444.0000 - val_loss: 25519192.6078\n",
      "Epoch 339/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25688543.3333 - val_loss: 25519288.4510\n",
      "Epoch 340/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25689195.3922 - val_loss: 25519290.0588\n",
      "Epoch 341/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25688793.8431 - val_loss: 25518788.5882\n",
      "Epoch 342/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25689324.1667 - val_loss: 25519099.3922\n",
      "Epoch 343/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25694144.8824 - val_loss: 25520371.5294\n",
      "Epoch 344/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25689439.8235 - val_loss: 25518676.8235\n",
      "Epoch 345/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25688674.7059 - val_loss: 25518192.0196\n",
      "Epoch 346/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25686359.4510 - val_loss: 25518310.6667\n",
      "Epoch 347/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25688371.0980 - val_loss: 25518360.2353\n",
      "Epoch 348/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25690201.5686 - val_loss: 25518028.1961\n",
      "Epoch 349/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25687786.8039 - val_loss: 25518086.6863\n",
      "Epoch 350/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25693034.0588 - val_loss: 25518047.3529\n",
      "Epoch 351/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25686597.3333 - val_loss: 25517786.6667\n",
      "Epoch 352/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25687311.3333 - val_loss: 25517928.8627\n",
      "Epoch 353/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25691637.7059 - val_loss: 25518347.7451\n",
      "Epoch 354/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25683948.6863 - val_loss: 25517427.4314\n",
      "Epoch 355/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25687522.1373 - val_loss: 25517236.1765\n",
      "Epoch 356/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25684292.0784 - val_loss: 25517292.6863\n",
      "Epoch 357/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 25691297.5098 - val_loss: 25517327.0980\n",
      "Epoch 358/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25686092.3333 - val_loss: 25517159.6667\n",
      "Epoch 359/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25688422.3137 - val_loss: 25517150.9020\n",
      "Epoch 360/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25685668.9706 - val_loss: 25517104.8824\n",
      "Epoch 361/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25688290.6078 - val_loss: 25516473.6078\n",
      "Epoch 362/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25686394.0196 - val_loss: 25516349.5294\n",
      "Epoch 363/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25686407.6275 - val_loss: 25516735.1373\n",
      "Epoch 364/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25683692.1765 - val_loss: 25516134.1765\n",
      "Epoch 365/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25688178.1569 - val_loss: 25516179.8824\n",
      "Epoch 366/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25684961.1176 - val_loss: 25516529.9804\n",
      "Epoch 367/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25687732.4216 - val_loss: 25516031.3333\n",
      "Epoch 368/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25693072.8039 - val_loss: 25516164.6078\n",
      "Epoch 369/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25685447.7059 - val_loss: 25516005.9412\n",
      "Epoch 370/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25686738.3137 - val_loss: 25515791.6471\n",
      "Epoch 371/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25685348.4510 - val_loss: 25515734.2745\n",
      "Epoch 372/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25683599.9608 - val_loss: 25515538.9216\n",
      "Epoch 373/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25692658.8431 - val_loss: 25516223.7451\n",
      "Epoch 374/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25684378.3725 - val_loss: 25515453.3922\n",
      "Epoch 375/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25683532.5392 - val_loss: 25515378.0588\n",
      "Epoch 376/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25684417.6863 - val_loss: 25515298.9804\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 70us/step - loss: 25682670.3137 - val_loss: 25514992.1373\n",
      "Epoch 378/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25685851.3922 - val_loss: 25514655.1765\n",
      "Epoch 379/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25682877.6961 - val_loss: 25514951.7843\n",
      "Epoch 380/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25686253.2157 - val_loss: 25515457.2157\n",
      "Epoch 381/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25682016.9804 - val_loss: 25514770.3922\n",
      "Epoch 382/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25684835.8235 - val_loss: 25514531.4510\n",
      "Epoch 383/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25691286.2353 - val_loss: 25514659.4510\n",
      "Epoch 384/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25682945.4314 - val_loss: 25514072.2353\n",
      "Epoch 385/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25685699.4118 - val_loss: 25514185.9608\n",
      "Epoch 386/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25685273.2941 - val_loss: 25514618.7255\n",
      "Epoch 387/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25680091.5294 - val_loss: 25513822.6471\n",
      "Epoch 388/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25681678.4902 - val_loss: 25514253.2549\n",
      "Epoch 389/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25683473.4902 - val_loss: 25513674.6471\n",
      "Epoch 390/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25682226.0000 - val_loss: 25513448.8431\n",
      "Epoch 391/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25682506.8235 - val_loss: 25513912.6863\n",
      "Epoch 392/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25683402.7745 - val_loss: 25513231.4314\n",
      "Epoch 393/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25684637.1373 - val_loss: 25513867.5882\n",
      "Epoch 394/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25679684.2353 - val_loss: 25513055.0000\n",
      "Epoch 395/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25680387.6078 - val_loss: 25513260.2353\n",
      "Epoch 396/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25685581.3922 - val_loss: 25512979.0980\n",
      "Epoch 397/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25678876.4118 - val_loss: 25512820.3333\n",
      "Epoch 398/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25680507.0980 - val_loss: 25512843.1176\n",
      "Epoch 399/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25680839.3529 - val_loss: 25512717.4510\n",
      "Epoch 400/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25678949.5686 - val_loss: 25512680.8824\n",
      "Epoch 401/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25682670.6078 - val_loss: 25512332.1373\n",
      "Epoch 402/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25678049.8824 - val_loss: 25512354.6863\n",
      "Epoch 403/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25683519.2157 - val_loss: 25512133.7451\n",
      "Epoch 404/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677830.8039 - val_loss: 25512062.3333\n",
      "Epoch 405/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25679584.9608 - val_loss: 25511711.0392\n",
      "Epoch 406/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25679785.8824 - val_loss: 25511855.0392\n",
      "Epoch 407/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25678953.9216 - val_loss: 25511862.0784\n",
      "Epoch 408/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25679995.4902 - val_loss: 25511697.2549\n",
      "Epoch 409/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25678950.8627 - val_loss: 25511466.5490\n",
      "Epoch 410/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25681967.0000 - val_loss: 25511554.9020\n",
      "Epoch 411/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25680683.9412 - val_loss: 25511596.0392\n",
      "Epoch 412/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25678283.1373 - val_loss: 25510955.5098\n",
      "Epoch 413/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25681441.2549 - val_loss: 25511097.0588\n",
      "Epoch 414/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25681484.4902 - val_loss: 25511211.5490\n",
      "Epoch 415/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25679560.1569 - val_loss: 25510854.1765\n",
      "Epoch 416/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25679530.8824 - val_loss: 25510673.3529\n",
      "Epoch 417/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25678139.0882 - val_loss: 25510616.1373\n",
      "Epoch 418/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25679115.9608 - val_loss: 25510326.8627\n",
      "Epoch 419/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25679783.4510 - val_loss: 25510278.3333\n",
      "Epoch 420/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25676741.3725 - val_loss: 25510310.3137\n",
      "Epoch 421/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677748.5686 - val_loss: 25510420.3922\n",
      "Epoch 422/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25675992.7451 - val_loss: 25510362.7451\n",
      "Epoch 423/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677417.5098 - val_loss: 25509946.4510\n",
      "Epoch 424/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25679605.1961 - val_loss: 25509723.6275\n",
      "Epoch 425/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25676173.3137 - val_loss: 25509780.1176\n",
      "Epoch 426/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25680237.1961 - val_loss: 25509746.3333\n",
      "Epoch 427/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675996.0196 - val_loss: 25509789.0588\n",
      "Epoch 428/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25675291.1569 - val_loss: 25509855.5490\n",
      "Epoch 429/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25678651.3725 - val_loss: 25509322.9216\n",
      "Epoch 430/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25675510.4020 - val_loss: 25509200.3725\n",
      "Epoch 431/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 25675748.7647 - val_loss: 25509226.5490\n",
      "Epoch 432/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25676359.8824 - val_loss: 25509431.3137\n",
      "Epoch 433/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25675170.0784 - val_loss: 25508991.8431\n",
      "Epoch 434/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25674314.3529 - val_loss: 25508844.5882\n",
      "Epoch 435/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25676871.9412 - val_loss: 25508522.8431\n",
      "Epoch 436/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675479.1373 - val_loss: 25508399.9216\n",
      "Epoch 437/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25680021.8627 - val_loss: 25508528.7843\n",
      "Epoch 438/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25674162.6275 - val_loss: 25508393.9412\n",
      "Epoch 439/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25674480.0294 - val_loss: 25507946.6863\n",
      "Epoch 440/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25673987.6471 - val_loss: 25507827.7255\n",
      "Epoch 441/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25676403.6471 - val_loss: 25508089.0980\n",
      "Epoch 442/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25677171.3922 - val_loss: 25507923.4314\n",
      "Epoch 443/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25684117.9216 - val_loss: 25507834.5882\n",
      "Epoch 444/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25679537.4706 - val_loss: 25508099.0000\n",
      "Epoch 445/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 25402005.97 - 0s 78us/step - loss: 25681364.8431 - val_loss: 25507580.5882\n",
      "Epoch 446/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25677450.1176 - val_loss: 25507749.9020\n",
      "Epoch 447/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25673081.2843 - val_loss: 25507354.6078\n",
      "Epoch 448/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25675854.9608 - val_loss: 25507119.7059\n",
      "Epoch 449/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25673080.5294 - val_loss: 25507483.2353\n",
      "Epoch 450/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25672300.3725 - val_loss: 25506882.8235\n",
      "Epoch 451/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25672824.3333 - val_loss: 25506744.5098\n",
      "Epoch 452/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25674981.8824 - val_loss: 25506854.8824\n",
      "Epoch 453/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25676017.3725 - val_loss: 25506989.3333\n",
      "Epoch 454/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25674206.8824 - val_loss: 25507030.5882\n",
      "Epoch 455/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25676664.4902 - val_loss: 25506319.8824\n",
      "Epoch 456/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25673122.4902 - val_loss: 25506324.4118\n",
      "Epoch 457/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25671619.6078 - val_loss: 25506459.0588\n",
      "Epoch 458/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25673103.5686 - val_loss: 25506061.9608\n",
      "Epoch 459/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675077.2941 - val_loss: 25505992.4902\n",
      "Epoch 460/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25671439.6667 - val_loss: 25506228.0980\n",
      "Epoch 461/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25672290.4510 - val_loss: 25505785.6863\n",
      "Epoch 462/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25675722.6471 - val_loss: 25505950.1176\n",
      "Epoch 463/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25682794.5882 - val_loss: 25505522.6078\n",
      "Epoch 464/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25678123.7059 - val_loss: 25505698.0392\n",
      "Epoch 465/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25672556.7647 - val_loss: 25505065.2745\n",
      "Epoch 466/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25679363.0588 - val_loss: 25505489.8039\n",
      "Epoch 467/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25672813.3137 - val_loss: 25504881.7843\n",
      "Epoch 468/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25674576.7647 - val_loss: 25505027.8431\n",
      "Epoch 469/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25671734.9804 - val_loss: 25505218.3137\n",
      "Epoch 470/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25669920.4902 - val_loss: 25504704.2549\n",
      "Epoch 471/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25669637.6275 - val_loss: 25504685.5882\n",
      "Epoch 472/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25674714.7843 - val_loss: 25504308.6471\n",
      "Epoch 473/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25672252.3725 - val_loss: 25504322.2745\n",
      "Epoch 474/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25673218.9804 - val_loss: 25504888.5098\n",
      "Epoch 475/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25672873.0784 - val_loss: 25504255.3137\n",
      "Epoch 476/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25671242.4314 - val_loss: 25503756.1765\n",
      "Epoch 477/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25670504.3922 - val_loss: 25504112.0392\n",
      "Epoch 478/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25673864.4118 - val_loss: 25504575.3137\n",
      "Epoch 479/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25669941.7059 - val_loss: 25503563.0392\n",
      "Epoch 480/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25674898.3431 - val_loss: 25503743.7059\n",
      "Epoch 481/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25667963.3137 - val_loss: 25503611.2745\n",
      "Epoch 482/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25672233.8137 - val_loss: 25503552.1373\n",
      "Epoch 483/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25669225.9902 - val_loss: 25503113.2353\n",
      "Epoch 484/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25674987.8431 - val_loss: 25503033.9608\n",
      "Epoch 485/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25669587.4314 - val_loss: 25503273.2353\n",
      "Epoch 486/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25674161.6765 - val_loss: 25503001.1373\n",
      "Epoch 487/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25669687.8725 - val_loss: 25503039.5686\n",
      "Epoch 488/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25667535.2353 - val_loss: 25502544.9216\n",
      "Epoch 489/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25669746.7255 - val_loss: 25502917.8627\n",
      "Epoch 490/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25673101.1765 - val_loss: 25502541.5098\n",
      "Epoch 491/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25669089.0980 - val_loss: 25502630.0196\n",
      "Epoch 492/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25668483.8627 - val_loss: 25502257.5490\n",
      "Epoch 493/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25670566.7647 - val_loss: 25503157.8235\n",
      "Epoch 494/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25672528.3137 - val_loss: 25502126.9216\n",
      "Epoch 495/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25667468.6863 - val_loss: 25501967.1961\n",
      "Epoch 496/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25666715.2745 - val_loss: 25502356.5098\n",
      "Epoch 497/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25667745.2745 - val_loss: 25502002.6667\n",
      "Epoch 498/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25666157.5294 - val_loss: 25501892.9608\n",
      "Epoch 499/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25667632.3725 - val_loss: 25501862.7451\n",
      "Epoch 500/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25667546.4118 - val_loss: 25501808.0196\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 82946778.9804 - val_loss: 59721666.9804\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 51390808.5098 - val_loss: 46493818.5882\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 44553900.6667 - val_loss: 39583364.0392\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 40352690.5490 - val_loss: 37321797.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 37948536.9412 - val_loss: 37709264.6667\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 34997566.9804 - val_loss: 34277705.2549\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 34819948.3725 - val_loss: 42126162.0000\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 34708454.0000 - val_loss: 30128186.5294\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 32233651.3725 - val_loss: 31018821.6471\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33018381.1569 - val_loss: 34972100.9020\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 34030645.7843 - val_loss: 28890710.9412\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 30980059.9020 - val_loss: 29335250.8627\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 29583558.0392 - val_loss: 27912522.5294\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 30192600.5882 - val_loss: 27551970.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 28919453.4804 - val_loss: 28178441.6471\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 30327976.9608 - val_loss: 29095154.7647\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 28926893.3137 - val_loss: 27594425.4510\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 28362243.1373 - val_loss: 26708582.8627\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 28341436.8235 - val_loss: 26726154.9804\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 27471091.7059 - val_loss: 26919381.6471\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 28800522.6471 - val_loss: 26636041.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 30273411.5686 - val_loss: 27022335.1765\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 29576127.2549 - val_loss: 45416530.8235\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 29123969.4510 - val_loss: 26573367.5882\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 28276563.4706 - val_loss: 27028726.1569\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 29152622.9216 - val_loss: 26237803.7451\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 27165934.5686 - val_loss: 26269912.2745\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 28971347.6471 - val_loss: 29388564.0784\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 27356388.8824 - val_loss: 27788380.3529\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26958624.1569 - val_loss: 26725952.4314\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27199138.0588 - val_loss: 26528076.0000\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 26025675.5294 - val_loss: 26009434.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25598945.7647 - val_loss: 26196297.9608\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25618391.9216 - val_loss: 26120719.3137\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25610775.3529 - val_loss: 26166324.6471\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25482755.0980 - val_loss: 26261289.6078\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25510269.1961 - val_loss: 26589966.4902\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25503763.1765 - val_loss: 26172221.6667\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25373699.9216 - val_loss: 26138137.8039\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25381695.4510 - val_loss: 26087105.5098\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25363109.7353 - val_loss: 26140071.7059\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25389411.7255 - val_loss: 26112121.8431\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25354612.5294 - val_loss: 26113062.5686\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25351844.2157 - val_loss: 26116639.0392\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25351273.4902 - val_loss: 26117161.9412\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25350898.6078 - val_loss: 26118361.5294\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25352085.7745 - val_loss: 26127261.2941\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25349723.8824 - val_loss: 26126466.0588\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25349732.3137 - val_loss: 26127188.6275\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25350005.2745 - val_loss: 26126837.3529\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25349664.9608 - val_loss: 26126632.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25349620.0784 - val_loss: 26126960.1961\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 271us/step - loss: 91782332.3137 - val_loss: 59672409.1373\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 47173485.7647 - val_loss: 47204361.4118\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 40496850.6667 - val_loss: 40404836.2745\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 35929500.4314 - val_loss: 36417344.1176\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 35199159.9412 - val_loss: 34505865.7255\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 34088930.1961 - val_loss: 34205457.5294\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 31450674.7451 - val_loss: 31540641.4510\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 31199857.1373 - val_loss: 33053099.3137\n",
      "Epoch 9/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 73us/step - loss: 33523884.9412 - val_loss: 31555560.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 31176775.5098 - val_loss: 34681539.1961\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 30122155.1569 - val_loss: 35287318.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 29912483.4510 - val_loss: 30468553.3725\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 28712861.6667 - val_loss: 35443550.5098\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 28589489.7451 - val_loss: 33150980.7843\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 28147537.9020 - val_loss: 28027794.2941\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 27811565.4902 - val_loss: 30160031.8824\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 27449481.0196 - val_loss: 29074900.0392\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 27724137.3137 - val_loss: 28100571.8627\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 29390495.4804 - val_loss: 31797294.6667\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 27623131.4902 - val_loss: 27994412.6471\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 27161977.8627 - val_loss: 27629192.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26617729.2353 - val_loss: 26927668.8039\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 27693152.8235 - val_loss: 28653036.4118\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 27871095.1765 - val_loss: 29372523.2549\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 27490789.5098 - val_loss: 27873033.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27434450.6667 - val_loss: 26859492.5294\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 28502153.5098 - val_loss: 35336093.0196\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 27744901.4314 - val_loss: 32550237.8824\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 28182435.5882 - val_loss: 30289542.7255\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 27535056.4510 - val_loss: 26707000.9804\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 27834761.9216 - val_loss: 27201667.4902\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 27352160.6863 - val_loss: 26882251.3333\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 26427065.0392 - val_loss: 27942884.0588\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 26877436.1961 - val_loss: 27772624.0196\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 26964514.7647 - val_loss: 29191848.3137\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26110991.8431 - val_loss: 26414715.2745\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25546812.4510 - val_loss: 26394750.8039\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 25528601.2745 - val_loss: 27130954.7843\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25647612.0784 - val_loss: 26522132.3333\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25555928.8039 - val_loss: 26373997.2353\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25529719.1961 - val_loss: 26319625.8235\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25708189.3922 - val_loss: 26372280.4902\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25717426.9608 - val_loss: 27129180.8431\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25628774.1569 - val_loss: 26456925.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25492344.9020 - val_loss: 26515135.1176\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25652070.4510 - val_loss: 26479785.9020\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25372392.3922 - val_loss: 26338789.9804\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25348140.6078 - val_loss: 26384502.4706\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25358454.8235 - val_loss: 26328286.2745\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25346250.3922 - val_loss: 26347990.1176\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25374283.6863 - val_loss: 26349128.7059\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 25334662.2941 - val_loss: 26340621.8235\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25336179.8627 - val_loss: 26336090.7843\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25337545.8039 - val_loss: 26346933.4118\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25335149.6471 - val_loss: 26338951.1961\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25334905.2549 - val_loss: 26338207.3137\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25333418.2451 - val_loss: 26338471.3333\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25333463.9216 - val_loss: 26338751.8824\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25333442.3922 - val_loss: 26338982.0196\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25333222.7647 - val_loss: 26338213.7255\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25333310.0196 - val_loss: 26338867.8039\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 329us/step - loss: 118263792.0784 - val_loss: 42715792.2745\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 41898732.5882 - val_loss: 38583136.9020\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 39087490.9412 - val_loss: 35245440.0000\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 35199048.3529 - val_loss: 33101889.0392\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33288501.7255 - val_loss: 36056351.0784\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 32631250.5098 - val_loss: 30015323.3333\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 30608631.7647 - val_loss: 30558668.2549\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 29693668.3137 - val_loss: 29297594.8235\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 28322629.6471 - val_loss: 27791851.9804\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 27944775.2549 - val_loss: 27962289.0784\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 27165172.9608 - val_loss: 26253950.2353\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 27114924.0000 - val_loss: 25675070.1765\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25771009.2157 - val_loss: 25401173.3725\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 26049826.4314 - val_loss: 31582816.0000\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 26334134.8824 - val_loss: 24906350.0980\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 24682684.9216 - val_loss: 25150368.0000\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25038873.7451 - val_loss: 25835424.7255\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25052825.0490 - val_loss: 24920511.3529\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24476969.9608 - val_loss: 26578019.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24711898.3725 - val_loss: 26123545.0588\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23792402.1373 - val_loss: 24322083.2745\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23458198.6471 - val_loss: 24241275.4510\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 23424090.5882 - val_loss: 24217778.8431\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23408108.1569 - val_loss: 24205910.6863\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23373400.8431 - val_loss: 24424043.1765\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23355975.3529 - val_loss: 24172935.1765\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 23306036.0196 - val_loss: 24211568.5490\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23280886.1961 - val_loss: 24157576.7059\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 23282916.2353 - val_loss: 24117001.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23203136.9216 - val_loss: 24512203.5294\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 23191117.3333 - val_loss: 24311569.9216\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 23174068.1961 - val_loss: 24378043.1961\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23185416.4902 - val_loss: 24112872.6078\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23154778.4314 - val_loss: 24099745.2157\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23119197.6471 - val_loss: 24095905.0588\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 23334749.3529 - val_loss: 24094599.0588\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23071229.9412 - val_loss: 24108057.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23091561.3922 - val_loss: 24349731.1765\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23149947.4314 - val_loss: 24522125.0980\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23253210.6373 - val_loss: 24537581.7647\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23120670.4706 - val_loss: 24138338.3137\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22981257.9804 - val_loss: 24010689.5882\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22934140.3137 - val_loss: 23997574.6863\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22930883.9902 - val_loss: 23997319.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22927508.7647 - val_loss: 24003744.4314\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22920614.2353 - val_loss: 24004229.5882\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22918080.3529 - val_loss: 24000236.8627\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22915901.7843 - val_loss: 24004114.5882\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 22918106.0196 - val_loss: 24002910.2549\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22900801.9902 - val_loss: 24002763.7059\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 22900325.5588 - val_loss: 24003455.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22899042.6373 - val_loss: 24003717.0980\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 22899154.9902 - val_loss: 24003959.7451\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 22898137.8627 - val_loss: 24004440.3725\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22897089.3333 - val_loss: 24004399.6863\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22897032.2157 - val_loss: 24004529.1765\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22897441.2353 - val_loss: 24004545.0588\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 22896887.5000 - val_loss: 24004537.1569\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 22896974.1176 - val_loss: 24004516.2549\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22896812.5882 - val_loss: 24004519.6078\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 22896811.4118 - val_loss: 24004513.8235\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22896808.8627 - val_loss: 24004519.7451\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22896821.4314 - val_loss: 24004523.7451\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22896806.6176 - val_loss: 24004520.8039\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 1s 250us/step - loss: 126534560.7451 - val_loss: 38835256.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 39599536.3529 - val_loss: 35459830.1569\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 38019695.6078 - val_loss: 33322305.5098\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 33558984.0980 - val_loss: 30644346.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 30549968.6863 - val_loss: 28448031.9020\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 30443045.9608 - val_loss: 28174112.7843\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 28671546.2353 - val_loss: 27324752.6078\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 28606383.6275 - val_loss: 27447076.9216\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 27481411.8039 - val_loss: 29500628.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 28214865.0392 - val_loss: 26950521.5490\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 27461660.9608 - val_loss: 31558550.3922\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26597962.1765 - val_loss: 25900246.1765\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 26987821.5294 - val_loss: 25803659.9804\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 26030728.6078 - val_loss: 25911930.3922\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25368182.9216 - val_loss: 26221916.4706\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 26770714.9412 - val_loss: 25537807.7255\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25810973.7843 - val_loss: 25506607.5294\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 26876238.4118 - val_loss: 26870477.9020\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25793857.7843 - val_loss: 28723725.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 24754711.6471 - val_loss: 24707536.0196\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26196981.1765 - val_loss: 25629985.1373\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25291450.6863 - val_loss: 25782056.2157\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25068476.6667 - val_loss: 25596600.9412\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 24585660.7255 - val_loss: 24790632.0392\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 24215369.5098 - val_loss: 25380909.5294\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 24015109.4314 - val_loss: 24506127.2353\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 23754095.6275 - val_loss: 24436059.9216\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 23701556.2549 - val_loss: 24384433.0392\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23781459.2549 - val_loss: 24276731.2157\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23629858.1765 - val_loss: 24391373.9412\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23563769.1176 - val_loss: 24582113.6471\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 23579401.8431 - val_loss: 24288692.2941\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 23579220.4314 - val_loss: 24405809.7255\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23491073.0980 - val_loss: 24181633.2353\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 23534819.7255 - val_loss: 24484757.4510\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 23536729.9020 - val_loss: 24296272.2353\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23451778.1765 - val_loss: 24180451.4314\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 23574892.8824 - val_loss: 24194979.5294\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 23471472.1961 - val_loss: 24325536.0196\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23505552.9412 - val_loss: 24700875.7451\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23584976.0882 - val_loss: 24157957.9020\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23425303.1961 - val_loss: 24195449.8039\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 23715365.0980 - val_loss: 24113476.5686\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 23463119.0000 - val_loss: 24080454.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23394959.7059 - val_loss: 24079891.0196\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 23366078.6471 - val_loss: 24103818.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23343197.8824 - val_loss: 24219675.7647\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23324282.3431 - val_loss: 24071808.8235\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23431956.3137 - val_loss: 24166314.9216\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 23106118.0980 - val_loss: 24561505.9608\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 23158733.8627 - val_loss: 24125956.7059\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23146631.1961 - val_loss: 24101370.7451\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23294820.8235 - val_loss: 23994159.7255\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 23023179.9804 - val_loss: 24439037.5098\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 23282023.3529 - val_loss: 24031546.2353\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 23075108.4902 - val_loss: 24002735.6471\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23062361.2157 - val_loss: 23988398.4314\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22991334.4216 - val_loss: 24052498.4510\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 23018030.1373 - val_loss: 24422462.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23090225.7451 - val_loss: 24300058.7647\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23059677.6275 - val_loss: 24006467.7059\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 22937177.3529 - val_loss: 24285111.7647\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22937568.1373 - val_loss: 24093279.8235\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 22863575.1961 - val_loss: 24021994.3137\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 22852598.0980 - val_loss: 23989614.3333\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 22857490.5686 - val_loss: 23978399.7451\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22850432.6275 - val_loss: 23993968.7059\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22870454.1176 - val_loss: 23996797.0588\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 22854384.0196 - val_loss: 23977162.4902\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22860236.5686 - val_loss: 23993742.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22855134.4902 - val_loss: 23982494.4510\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22851631.3137 - val_loss: 23984645.0000\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22843299.9804 - val_loss: 23999546.7059\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 22843506.1961 - val_loss: 23988705.9216\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22830215.3529 - val_loss: 23988497.5490\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22829077.4118 - val_loss: 23985898.4706\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22829050.2941 - val_loss: 23982882.9020\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 22828099.1176 - val_loss: 23983026.1765\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 22827412.3725 - val_loss: 23982231.7647\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22826166.6667 - val_loss: 23982263.4118\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22826195.0588 - val_loss: 23982362.4314\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 22826276.8824 - val_loss: 23982197.3529\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 22826214.3039 - val_loss: 23982166.8039\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22826181.3333 - val_loss: 23982389.3922\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 22825981.8235 - val_loss: 23982387.5490\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22825989.3137 - val_loss: 23982372.6863\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22825985.3333 - val_loss: 23982360.7843\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22825976.6078 - val_loss: 23982371.3725\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22825972.4314 - val_loss: 23982364.6275\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 365us/step - loss: 188411387.6078 - val_loss: 46044743.4510\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 38068344.2549 - val_loss: 37013242.8235\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 33784286.4706 - val_loss: 33215039.3725\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 34617741.1961 - val_loss: 32351768.5686\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 30464655.9608 - val_loss: 33053275.2157\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 29039302.3922 - val_loss: 31231439.1961\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 28453388.9608 - val_loss: 31046997.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 28773784.7255 - val_loss: 28136898.2157\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 27970094.5098 - val_loss: 28102582.2353\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26949277.3137 - val_loss: 27891071.3725\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 27629367.4314 - val_loss: 27497914.9412\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 27018101.2157 - val_loss: 31827598.2941\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 27228265.5490 - val_loss: 27458633.4902\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26504292.6471 - val_loss: 29688548.8627\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25656777.8235 - val_loss: 25804448.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26860562.5882 - val_loss: 30061555.2157\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26787371.1569 - val_loss: 26008004.0784\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25540745.9804 - val_loss: 26351110.6667\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25827711.3922 - val_loss: 30475428.9608\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25968552.3725 - val_loss: 25324330.7843\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25442411.1961 - val_loss: 25576539.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25903129.4510 - val_loss: 27307794.4902\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 24876747.5294 - val_loss: 24547734.8824\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 24739550.7843 - val_loss: 24541399.3333\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 24961004.4706 - val_loss: 28987265.4706\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25552252.6863 - val_loss: 25932402.5686\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25703343.2549 - val_loss: 30677695.7647\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 24994236.5686 - val_loss: 24651693.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 24241675.9804 - val_loss: 24069432.2941\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26083473.3333 - val_loss: 25292427.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 24690236.7059 - val_loss: 23748899.9608\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24645339.3333 - val_loss: 24119995.2549\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24465573.2549 - val_loss: 25060943.1373\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24571040.9216 - val_loss: 23828889.8235\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 24333717.8235 - val_loss: 23789646.6078\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 24006949.2941 - val_loss: 23715695.3922\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24064238.9608 - val_loss: 24843601.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 24403238.4118 - val_loss: 24257585.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 24086675.9804 - val_loss: 23445022.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23962993.0784 - val_loss: 23765076.1373\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 24228290.1373 - val_loss: 23528088.1176\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23961780.7549 - val_loss: 23665412.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 24088762.9902 - val_loss: 25789395.6667\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 24006538.9902 - val_loss: 23387551.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23535500.0392 - val_loss: 23247013.4118\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23917638.0392 - val_loss: 25458118.9608\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 23797788.9412 - val_loss: 23311499.0196\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 24291929.6863 - val_loss: 23060936.6078\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23391507.0784 - val_loss: 25873744.9020\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 24647350.7647 - val_loss: 23500446.5686\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23700977.1373 - val_loss: 23396441.9216\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 23273258.2941 - val_loss: 24256198.1373\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23451122.0196 - val_loss: 23604952.0392\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 23026823.5882 - val_loss: 22858827.3529\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22702006.0980 - val_loss: 22911793.8039\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22643471.3529 - val_loss: 22825833.1765\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22536191.9804 - val_loss: 23040526.2353\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22651604.5588 - val_loss: 22886361.3529\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 22574145.8333 - val_loss: 22955208.1765\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22507235.2647 - val_loss: 22862621.9020\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22495783.3824 - val_loss: 22799820.5294\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22482532.8627 - val_loss: 22737156.7843\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22495691.6275 - val_loss: 22771335.1569\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22493940.6275 - val_loss: 22773654.3922\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22433698.7941 - val_loss: 22799281.2157\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 22432990.6863 - val_loss: 22816959.3529\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22419460.2549 - val_loss: 22978573.0196\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 22300392.0000 - val_loss: 22843087.6471\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22272207.3137 - val_loss: 22820832.2745\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22261868.5294 - val_loss: 22814571.1961\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22248478.6275 - val_loss: 22794638.2941\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 22249047.0784 - val_loss: 22799666.7647\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 22235510.0196 - val_loss: 22802114.5098\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 22234896.6961 - val_loss: 22804191.8627\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22233607.4902 - val_loss: 22803465.6078\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22234314.9216 - val_loss: 22803448.2941\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22235481.6275 - val_loss: 22805477.9020\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22231922.6667 - val_loss: 22805422.1961\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22231800.6863 - val_loss: 22805425.8235\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22231783.2745 - val_loss: 22805605.1373\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 22231889.4118 - val_loss: 22805711.6078\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 22231845.6863 - val_loss: 22805499.7843\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 572us/step - loss: 236093481.5686 - val_loss: 86871950.5098\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 74215839.4510 - val_loss: 127136287.6863\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 74196395.6078 - val_loss: 57332914.7843\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 58661151.0588 - val_loss: 49560105.4118\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 54151350.9020 - val_loss: 46580192.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 48224615.0196 - val_loss: 51496940.8627\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 58245508.3137 - val_loss: 68641306.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 72812609.8039 - val_loss: 58124100.6275\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 65863459.4510 - val_loss: 61539441.7255\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 76019799.8039 - val_loss: 83749897.0196\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 72725872.7451 - val_loss: 59979632.7843\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 66185629.0196 - val_loss: 64554599.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 65329413.7255 - val_loss: 65080089.8039\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 60554247.5294 - val_loss: 56663354.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 61853634.0000 - val_loss: 56505628.0000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 62000869.9216 - val_loss: 54365905.9608\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 61022247.9608 - val_loss: 53812063.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 60416192.2353 - val_loss: 52401017.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 59917956.0784 - val_loss: 53118877.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 59421984.0000 - val_loss: 53094746.2745\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 59424207.4902 - val_loss: 52981206.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 59359133.8824 - val_loss: 52938559.8431\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 59461953.3333 - val_loss: 52957167.9216\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 59385824.9020 - val_loss: 52983766.6667\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 59324336.6275 - val_loss: 52996207.2157\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 588us/step - loss: 265568064.9412 - val_loss: 70625555.6863\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 69264747.8431 - val_loss: 53689811.8039\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 58062044.0784 - val_loss: 52002202.3529\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 50630834.9608 - val_loss: 51811420.9020\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 1s 215us/step - loss: 59543342.1176 - val_loss: 50853689.2157\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 227us/step - loss: 50147073.8431 - val_loss: 55272581.0588\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 85172164.3137 - val_loss: 63389489.0196\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 62902856.3922 - val_loss: 74484233.2941\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 69531525.2549 - val_loss: 49023075.4118\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 58219540.3137 - val_loss: 60381949.9608\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 54676276.0000 - val_loss: 57245270.5098\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54898758.7843 - val_loss: 54446991.5294\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 61046362.4314 - val_loss: 65688503.6863\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 62832880.3529 - val_loss: 56546635.5294\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 59468802.4314 - val_loss: 52789398.4314\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 58588170.2745 - val_loss: 53804553.5686\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 55527395.6863 - val_loss: 52498683.2941\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 53132464.8627 - val_loss: 53906304.0000\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 53373115.0980 - val_loss: 52010106.8627\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 51719941.9608 - val_loss: 51813492.5490\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 51710450.4314 - val_loss: 51344305.8431\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 51857829.8039 - val_loss: 52717042.5098\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 51588350.8235 - val_loss: 52450965.6471\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 51453741.8824 - val_loss: 52655783.9216\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 51360537.7255 - val_loss: 51455105.0980\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 51373229.7255 - val_loss: 52533384.2745\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 51358981.7255 - val_loss: 52504332.0392\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 167us/step - loss: 51351710.3529 - val_loss: 52511826.6667\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 51346249.3333 - val_loss: 52577559.8824\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 643us/step - loss: 104227161.7255 - val_loss: 62090245.7647\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 65490972.6667 - val_loss: 70477488.9412\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 73217837.4902 - val_loss: 78013570.9804\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 58231116.9412 - val_loss: 62762515.6078\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 54733682.7059 - val_loss: 52539658.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 50729703.4902 - val_loss: 55050194.6667\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 49737692.9412 - val_loss: 48825280.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 234us/step - loss: 51550888.6275 - val_loss: 50973036.8235\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 254us/step - loss: 49666638.3922 - val_loss: 70031927.4510\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 233us/step - loss: 68785659.9608 - val_loss: 70732202.6667\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 51271616.1569 - val_loss: 54608181.0196\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 188us/step - loss: 47239799.8431 - val_loss: 52169689.2157\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 45431005.2941 - val_loss: 52737803.2941\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 164us/step - loss: 45221952.3922 - val_loss: 52290478.0784\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 45141756.2745 - val_loss: 51954780.1176\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 45089189.2157 - val_loss: 51208064.7451\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 44750483.7647 - val_loss: 51379186.4314\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 44398362.9412 - val_loss: 51338924.9804\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 227us/step - loss: 44390968.0392 - val_loss: 51232526.0392\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 44389373.9216 - val_loss: 51097322.8235\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 234us/step - loss: 44337279.8824 - val_loss: 51196838.1961\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 44348374.6275 - val_loss: 51315601.9216\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 44282746.1765 - val_loss: 51298688.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 212us/step - loss: 44275974.8431 - val_loss: 51266432.4706\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 44272917.6471 - val_loss: 51249671.9216\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 44273980.0000 - val_loss: 51232288.8627\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 44268078.2353 - val_loss: 51227975.6863\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 7 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4298.941025214379, 7148.259284985343, 0.7738868599113026)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 344us/step - loss: 84808858.5882 - val_loss: 63445905.3333\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 68111017.1373 - val_loss: 61921716.4706\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 59857207.0588 - val_loss: 62318097.3333\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 56904943.1765 - val_loss: 58483435.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 54973909.7647 - val_loss: 57055785.7647\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 55677800.0392 - val_loss: 57409088.5882\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54287498.0784 - val_loss: 54696690.9412\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 53077111.2157 - val_loss: 54218346.6275\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 53617748.8627 - val_loss: 71874680.1569\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 52318959.0588 - val_loss: 53575464.0000\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 52846291.8824 - val_loss: 62851206.0392\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 53740366.4706 - val_loss: 53643618.1176\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 50582359.8431 - val_loss: 57252867.2157\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 52346878.8627 - val_loss: 52570931.1373\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 53080635.5294 - val_loss: 63859796.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 51839640.1176 - val_loss: 61521410.0392\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 55251955.6471 - val_loss: 63674761.6863\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 49330070.3529 - val_loss: 52390665.0980\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50654373.2549 - val_loss: 59358234.1961\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 50089277.8824 - val_loss: 54027628.4706\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 49324577.2157 - val_loss: 60879526.1961\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 50473687.7255 - val_loss: 51935568.0000\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 52195066.4314 - val_loss: 50836637.2941\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51198372.7059 - val_loss: 52784553.9608\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50840281.7647 - val_loss: 55135353.1373\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 50522118.0784 - val_loss: 53159367.1373\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 51658444.8627 - val_loss: 51436437.1765\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 51560110.7843 - val_loss: 68540537.9608\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 49282016.7059 - val_loss: 50686132.2353\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46355783.3725 - val_loss: 50592739.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46269562.3922 - val_loss: 50437314.6275\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 46164456.6667 - val_loss: 50451795.0980\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46228020.5882 - val_loss: 50910948.2745\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45944910.1569 - val_loss: 50533209.0196\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 46293348.00 - 0s 40us/step - loss: 45883570.4706 - val_loss: 50370025.2549\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46223628.5490 - val_loss: 51107020.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45872397.4510 - val_loss: 50287677.3333\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45956052.2745 - val_loss: 50255819.0980\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45934514.0392 - val_loss: 50160725.4510\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 45977840.0392 - val_loss: 50065723.4510\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45775439.3725 - val_loss: 50127523.9216\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45587727.5294 - val_loss: 50398336.3922\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45852695.3333 - val_loss: 50045264.7843\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 45603039.8431 - val_loss: 50008201.3725\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45949907.8039 - val_loss: 51084777.4902\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45557439.6078 - val_loss: 49937181.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45594556.9412 - val_loss: 50265413.2941\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 45472882.4314 - val_loss: 50454997.6863\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45598200.9412 - val_loss: 50033218.6667\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45592408.7843 - val_loss: 50012450.0000\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45652208.8235 - val_loss: 49783103.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45392372.3137 - val_loss: 50928750.4314\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45573331.8039 - val_loss: 49930922.9020\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45403176.0000 - val_loss: 50035615.7647\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45685569.1373 - val_loss: 49747940.3137\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45651518.0392 - val_loss: 50422479.9216\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45676761.7647 - val_loss: 49684618.6667\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45674778.4314 - val_loss: 50471529.9216\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45475239.6471 - val_loss: 50475982.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45397185.4902 - val_loss: 49763205.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45856468.9804 - val_loss: 51711886.7059\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45338990.7059 - val_loss: 52428323.8824\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 46276799.4706 - val_loss: 49516067.2157\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45042438.6863 - val_loss: 49538663.0196\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 44873053.2157 - val_loss: 49444151.0980\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44808064.7451 - val_loss: 49443810.8627\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44919461.8039 - val_loss: 49441534.0784\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44815908.0784 - val_loss: 49420819.0196\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44808957.9608 - val_loss: 49428927.6471\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44824096.7843 - val_loss: 49422240.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44815705.6078 - val_loss: 49425937.4510\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44811366.1176 - val_loss: 49417545.9216\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44773745.5686 - val_loss: 49423421.6863\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44804307.2941 - val_loss: 49410523.2157\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 44816856.4706 - val_loss: 49423432.0000\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44792561.0588 - val_loss: 49407739.1765\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 44783188.2745 - val_loss: 49392141.0196\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44807281.1961 - val_loss: 49413742.7059\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44802917.1373 - val_loss: 49412365.9608\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44759997.9216 - val_loss: 49407584.2745\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44825014.9804 - val_loss: 49390517.6471\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 44794358.2353 - val_loss: 49384728.8235\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44750139.8431 - val_loss: 49391168.2745\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44838214.4314 - val_loss: 49419619.1765\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44762859.8824 - val_loss: 49378989.1765\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44782231.0980 - val_loss: 49392308.5490\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44763039.8039 - val_loss: 49387355.6863\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44792748.0784 - val_loss: 49372392.2745\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44756831.0588 - val_loss: 49379876.0784\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44768256.3922 - val_loss: 49409531.1765\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44804352.4706 - val_loss: 49394721.0196\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44736903.8824 - val_loss: 49362776.5490\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44727734.4314 - val_loss: 49358278.8235\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44727987.1765 - val_loss: 49364285.9608\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44715873.2157 - val_loss: 49360049.7255\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44761590.4706 - val_loss: 49371253.1765\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44748318.2353 - val_loss: 49350499.9216\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44722402.9020 - val_loss: 49369355.3725\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44682754.4706 - val_loss: 49356603.2549\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 44702338.5882 - val_loss: 49363319.7255\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44709641.0588 - val_loss: 49345669.0588\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44732324.2353 - val_loss: 49371057.4118\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44697591.6078 - val_loss: 49365475.6863\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44697173.1569 - val_loss: 49337346.0000\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44693247.8039 - val_loss: 49334443.0980\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44700859.1765 - val_loss: 49327455.6471\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44676248.8627 - val_loss: 49330397.5294\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 44684188.8431 - val_loss: 49345329.6863\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 44685299.9216 - val_loss: 49320283.7647\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44656601.2549 - val_loss: 49325138.6275\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44657721.4118 - val_loss: 49315912.8627\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44726202.1569 - val_loss: 49318348.0784\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44679583.6863 - val_loss: 49336399.5686\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44643353.5686 - val_loss: 49323646.9804\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 44644840.3529 - val_loss: 49304820.5490\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44635890.7059 - val_loss: 49311119.2549\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44639284.7059 - val_loss: 49301318.5882\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44633241.0588 - val_loss: 49327097.0196\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44722148.6275 - val_loss: 49338577.0196\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44693598.2745 - val_loss: 49304760.9020\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44667054.1176 - val_loss: 49313570.1961\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44601593.5294 - val_loss: 49303844.8627\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44588847.7451 - val_loss: 49302050.5882\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44591971.2549 - val_loss: 49302679.5294\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44585589.0196 - val_loss: 49300186.0000\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44585146.1569 - val_loss: 49298003.9216\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 44582575.1373 - val_loss: 49298327.4118\n",
      "Epoch 128/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44583960.8235 - val_loss: 49295655.2549\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44584304.5882 - val_loss: 49295628.3922\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 44581364.0784 - val_loss: 49296743.6471\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44579899.0196 - val_loss: 49294886.8627\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44578890.5882 - val_loss: 49295592.9412\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44579069.7647 - val_loss: 49292958.7843\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44578254.4314 - val_loss: 49294027.9216\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 44590820.2353 - val_loss: 49293506.2353\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44575274.5098 - val_loss: 49293297.2941\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44582826.3922 - val_loss: 49291786.3137\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 44584593.9216 - val_loss: 49292402.7843\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 44575719.3725 - val_loss: 49291535.0980\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 44582223.3725 - val_loss: 49292381.6078\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 44584618.4314 - val_loss: 49291872.1961\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 44580581.6471 - val_loss: 49290471.8431\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44580073.3333 - val_loss: 49291006.7843\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44576644.4706 - val_loss: 49291602.6275\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44580163.7647 - val_loss: 49289534.3137\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44575523.2353 - val_loss: 49289070.2353\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44576504.6275 - val_loss: 49288815.3725\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44578341.2549 - val_loss: 49288914.6667\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44577035.8431 - val_loss: 49288441.0196\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44572522.3137 - val_loss: 49288856.4314\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44576677.2549 - val_loss: 49288960.4314\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44574660.2745 - val_loss: 49288043.8039\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44571443.1373 - val_loss: 49286419.8824\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44573742.4314 - val_loss: 49288026.1961\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44578666.3529 - val_loss: 49286791.6078\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44584134.1569 - val_loss: 49288630.1176\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44573724.4706 - val_loss: 49287331.0588\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44569716.3137 - val_loss: 49286760.3529\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44566492.6275 - val_loss: 49286731.1765\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566269.0588 - val_loss: 49286668.2353\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44566029.7647 - val_loss: 49286618.0000\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566037.1765 - val_loss: 49286697.9608\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566448.3922 - val_loss: 49286534.9804\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565574.5490 - val_loss: 49286533.0588\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565577.8431 - val_loss: 49286532.8627\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565582.4706 - val_loss: 49286532.7059\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565569.8039 - val_loss: 49286532.4706\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44565568.5882 - val_loss: 49286530.1961\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44565553.9608 - val_loss: 49286530.8235\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44565556.1176 - val_loss: 49286530.2745\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565555.9608 - val_loss: 49286529.6863\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565555.4902 - val_loss: 49286531.6078\n",
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44565555.3725 - val_loss: 49286529.6078\n",
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 382us/step - loss: 151757093.5686 - val_loss: 70957379.7647\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 67871070.8627 - val_loss: 61862141.4902\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 63894815.1765 - val_loss: 61375243.7647\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 61641311.6078 - val_loss: 57935817.1373\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 62012130.8235 - val_loss: 57103756.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 59800734.7059 - val_loss: 56564912.3137\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 60006956.5490 - val_loss: 55948337.2157\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 57755411.8824 - val_loss: 60359357.0980\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 57716062.4314 - val_loss: 54043472.9020\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 57476168.1961 - val_loss: 54761479.8824\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56157854.4706 - val_loss: 53319657.5294\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 57338013.6471 - val_loss: 55404265.5686\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 44us/step - loss: 56992183.0588 - val_loss: 63685062.0392\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 54539470.0784 - val_loss: 52979291.9216\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 53820956.0392 - val_loss: 52128031.3725\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 57217298.7059 - val_loss: 51499227.7647\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 55048909.1373 - val_loss: 56651560.5882\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 52751626.8627 - val_loss: 51272351.4118\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 54270295.3333 - val_loss: 51216875.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 53139499.0196 - val_loss: 51139117.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 53823830.9020 - val_loss: 56198084.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 52642337.1373 - val_loss: 62889883.1765\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 55128180.3529 - val_loss: 50137356.9804\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51342403.3725 - val_loss: 53046757.8039\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 51797853.1765 - val_loss: 51158496.9804\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 53197550.6667 - val_loss: 52900605.9608\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 54534866.3529 - val_loss: 49297509.6863\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 51295099.7255 - val_loss: 55421956.7843\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 51136053.2941 - val_loss: 52752438.9020\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53384643.0588 - val_loss: 51365643.0588\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 51183461.0980 - val_loss: 50161552.9804\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 49336292.0784 - val_loss: 48759556.3922\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 50176672.2353 - val_loss: 50918348.9804\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 51741539.3725 - val_loss: 51707609.9608\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 49576216.7059 - val_loss: 48032312.9412\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 49705560.1961 - val_loss: 50478614.3137\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 49251858.2745 - val_loss: 49776391.7647\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 51160394.9804 - val_loss: 63557485.0196\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 50248781.0588 - val_loss: 49277590.4706\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 50462796.0392 - val_loss: 47184744.1569\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 54162722.5098 - val_loss: 48964246.3922\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51591607.2549 - val_loss: 52736171.4510\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 48526353.2941 - val_loss: 47898125.3333\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 48052337.0196 - val_loss: 48471644.7451\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 48213416.7059 - val_loss: 47729563.1373\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 46835706.5098 - val_loss: 47177890.5882\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46518397.1569 - val_loss: 48503151.3725\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 46769006.6275 - val_loss: 48154142.7451\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 46528068.0784 - val_loss: 47068418.4314\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 46231941.0588 - val_loss: 47566160.9412\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46302962.7059 - val_loss: 47462827.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46261804.3137 - val_loss: 47630501.9216\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 46191358.5098 - val_loss: 47050818.6275\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46187244.6471 - val_loss: 47011085.4510\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 46396304.1569 - val_loss: 47043753.9608\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46094771.8431 - val_loss: 46971362.0392\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 46494449.1765 - val_loss: 48091973.6471\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 46431469.4118 - val_loss: 49104558.5098\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 46637814.8235 - val_loss: 46955717.9216\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 46034756.9020 - val_loss: 47624038.0784\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46450880.1961 - val_loss: 47731773.2549\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 46031953.6863 - val_loss: 47600178.0392\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46170376.2353 - val_loss: 46855817.0196\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 45812045.1961 - val_loss: 47896598.4706\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 46339227.8824 - val_loss: 47776822.1569\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 45658733.6078 - val_loss: 48088061.6471\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 45789732.6667 - val_loss: 46777885.7647\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 45762277.4902 - val_loss: 47484349.3333\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45976010.0000 - val_loss: 47209092.5882\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45789620.9412 - val_loss: 47042756.5882\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45759657.9608 - val_loss: 47252992.1961\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45882837.0980 - val_loss: 46776981.0980\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 46104877.4510 - val_loss: 46659085.0588\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45764537.2549 - val_loss: 47689247.4902\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45928546.0196 - val_loss: 47780897.8039\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45790127.9608 - val_loss: 47356927.8824\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 46116339.0196 - val_loss: 48312437.1765\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45465042.3529 - val_loss: 47669046.4706\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45499819.0784 - val_loss: 46968777.8431\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 45316989.6863 - val_loss: 46844300.1569\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45294610.3529 - val_loss: 46752345.1373\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45280380.9804 - val_loss: 46827228.5490\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45282040.1961 - val_loss: 46800239.4902\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45253433.5686 - val_loss: 46773332.3922\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 45252868.5098 - val_loss: 46803821.8824\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45251227.0980 - val_loss: 46785897.3333\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45245094.3137 - val_loss: 46783865.0980\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45251200.0588 - val_loss: 46799581.2549\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45239631.1373 - val_loss: 46798862.3137\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45239845.5294 - val_loss: 46798685.5686\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45239560.7843 - val_loss: 46799545.1373\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45239449.0588 - val_loss: 46798444.8235\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45239868.9020 - val_loss: 46797098.4706\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 361us/step - loss: 276728817.4902 - val_loss: 81769053.8824\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 70548733.1765 - val_loss: 74022201.4902\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 64083016.8627 - val_loss: 73795471.8431\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 65638427.9608 - val_loss: 75441180.3137\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 62009262.9020 - val_loss: 64714881.0980\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 62013097.3333 - val_loss: 66318156.3922\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 59893180.6667 - val_loss: 63554323.5294\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 61534086.6667 - val_loss: 59092849.1765\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 60179238.5882 - val_loss: 57949727.8824\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 58988382.9412 - val_loss: 60252445.5686\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 59416167.9216 - val_loss: 56481975.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 59024240.2353 - val_loss: 56084555.0588\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 56868783.3725 - val_loss: 55916652.7059\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 56171920.0000 - val_loss: 55222371.6078\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56759407.8824 - val_loss: 53918906.1176\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 55845213.4902 - val_loss: 54570867.8431\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56872694.2353 - val_loss: 54398005.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 56009678.0000 - val_loss: 52874918.6667\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 55576945.6471 - val_loss: 53482350.5882\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 55251222.8627 - val_loss: 53558987.5294\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54833988.6275 - val_loss: 55941929.8039\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54200301.6078 - val_loss: 51774944.5490\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 55083182.9804 - val_loss: 51289205.2157\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 53681563.7647 - val_loss: 54960497.7255\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 57819446.5490 - val_loss: 62782906.3529\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53554073.2549 - val_loss: 50843846.3922\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 55774873.4510 - val_loss: 50782002.1569\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 55072171.4706 - val_loss: 50568431.8039\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 55644650.9412 - val_loss: 57337693.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 53451262.4314 - val_loss: 51620042.5490\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 52621000.7059 - val_loss: 51924797.7647\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 53038456.0000 - val_loss: 53854837.8824\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 53051022.5882 - val_loss: 50313083.2549\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 54692450.7843 - val_loss: 51928049.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53736774.1176 - val_loss: 50930255.4902\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51749991.0980 - val_loss: 49274942.7451\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51706702.1961 - val_loss: 52387148.0000\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51608374.6667 - val_loss: 53754945.7255\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 52661317.1373 - val_loss: 48622475.6078\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 51287937.4118 - val_loss: 49946533.8431\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 53577621.7647 - val_loss: 57127315.0588\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 51285951.8431 - val_loss: 49829124.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 50444889.6471 - val_loss: 49480473.2549\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 50370036.4706 - val_loss: 48036381.0196\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 52049246.1176 - val_loss: 59586267.1373\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 33us/step - loss: 50670565.0588 - val_loss: 50710077.4902\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 51479450.1569 - val_loss: 51346206.3529\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 50537306.5098 - val_loss: 47783653.0196\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 50044558.8627 - val_loss: 48696262.3137\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50698600.4314 - val_loss: 55415683.3725\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 49672549.7647 - val_loss: 52088335.9216\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50121094.0392 - val_loss: 46928444.0392\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 49246268.1961 - val_loss: 48250927.8431\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 52334587.6863 - val_loss: 47120263.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 49610682.0196 - val_loss: 47250043.8824\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 50060924.5098 - val_loss: 47702532.2745\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 54407495.8824 - val_loss: 47028563.9608\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 48182087.7647 - val_loss: 47693942.0784\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 48021469.6471 - val_loss: 47846618.9804\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 47871769.2157 - val_loss: 48116281.6078\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47862642.7451 - val_loss: 47080237.3333\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 47772614.1176 - val_loss: 47926232.1961\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 47761766.4706 - val_loss: 47214812.0784\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 47638536.7451 - val_loss: 46938378.4314\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 47595872.3922 - val_loss: 46991451.3333\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 47614387.5490 - val_loss: 46935162.7843\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47593375.4902 - val_loss: 46990938.0000\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47574777.2157 - val_loss: 46977351.7255\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 47572563.4510 - val_loss: 46974014.0784\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 47572014.7843 - val_loss: 46962582.7843\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 47572047.4118 - val_loss: 46956230.3529\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47572665.8824 - val_loss: 46948058.1569\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 442us/step - loss: 165217343.8431 - val_loss: 84400593.9608\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 83623962.0784 - val_loss: 71464753.1765\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 73664486.5882 - val_loss: 68653560.9412\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 68159168.6275 - val_loss: 63151302.5098\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 63510081.4118 - val_loss: 62967203.8431\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 60642448.8627 - val_loss: 59694128.3922\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 59796708.0392 - val_loss: 59267080.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 58586828.1176 - val_loss: 58665242.2353\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 57485611.0196 - val_loss: 56668099.1765\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 55222602.7843 - val_loss: 55916239.6471\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 53891379.4902 - val_loss: 55451842.0000\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 53701932.6275 - val_loss: 55174225.0588\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 54697000.8235 - val_loss: 55930363.5294\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 54264085.5686 - val_loss: 56317873.5294\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 53381672.3922 - val_loss: 54840650.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 51793427.1569 - val_loss: 53575447.2549\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 51673586.2745 - val_loss: 57193372.0000\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 55050727.8824 - val_loss: 55991339.7647\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 50885840.0196 - val_loss: 60631127.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 51048498.3922 - val_loss: 52268165.6078\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49881437.6471 - val_loss: 53462322.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 52433323.6863 - val_loss: 51791701.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 50121147.2941 - val_loss: 55555787.2157\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 49371560.3725 - val_loss: 53671084.8627\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 49283667.8039 - val_loss: 52624595.8824\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 49661682.3137 - val_loss: 56104900.0000\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 53687689.1765 - val_loss: 58113904.0784\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48955089.7647 - val_loss: 50987030.7451\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48021310.1961 - val_loss: 51150142.9804\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 47843012.2745 - val_loss: 51342509.3725\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 79us/step - loss: 47800882.6667 - val_loss: 51207166.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47780209.6863 - val_loss: 51479031.6078\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 47788380.3529 - val_loss: 51222081.5294\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 47684122.0392 - val_loss: 51174582.9412\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47591860.2941 - val_loss: 51173431.1373\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47593294.8235 - val_loss: 51171959.9608\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 47578763.2549 - val_loss: 51172708.1569\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 47598698.7843 - val_loss: 51166660.7843\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47550766.2353 - val_loss: 51165941.7647\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47549047.2549 - val_loss: 51165629.0588\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47549864.5098 - val_loss: 51166016.1569\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47548702.6667 - val_loss: 51165273.0980\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 47551992.5098 - val_loss: 51165357.1373\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 47545812.0980 - val_loss: 51165258.7059\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 47545466.0000 - val_loss: 51165222.4706\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47545621.6078 - val_loss: 51165142.5098\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47545344.9412 - val_loss: 51165094.5098\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47545288.3922 - val_loss: 51165101.2549\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 430us/step - loss: 98546263.6078 - val_loss: 72421729.8039\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 73018704.0392 - val_loss: 66627666.1961\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 67693964.0000 - val_loss: 72662208.3137\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 65448650.1176 - val_loss: 58256421.9216\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 64814476.4706 - val_loss: 57636051.6471\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 60697368.0000 - val_loss: 56169095.7647\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 59820010.4314 - val_loss: 64399023.3333\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 57264596.2353 - val_loss: 53936605.6078\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 56004599.2157 - val_loss: 53025199.6471\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 56043249.6863 - val_loss: 59037037.8824\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 53896680.2745 - val_loss: 52345772.9804\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 54254152.9412 - val_loss: 62373661.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 56288446.6667 - val_loss: 57669794.0392\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 53737886.0000 - val_loss: 57804530.9804\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 53884126.7059 - val_loss: 73104876.1569\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 54616575.8824 - val_loss: 53425998.6667\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51122934.1569 - val_loss: 51167718.8627\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 50548426.0784 - val_loss: 50213898.5490\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 50443654.5882 - val_loss: 50181491.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 50430755.8039 - val_loss: 51219832.1569\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 50307540.1569 - val_loss: 50379632.5490\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50448305.2549 - val_loss: 50946157.6863\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 50099545.9608 - val_loss: 50032117.6863\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 50175831.2941 - val_loss: 50950686.9804\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50294675.6078 - val_loss: 50411934.3137\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50109648.2745 - val_loss: 50973627.6078\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 50421211.2941 - val_loss: 49968213.6471\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 50160562.1569 - val_loss: 50141293.1765\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49963061.9216 - val_loss: 49808244.6275\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 50106969.7647 - val_loss: 49962493.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49979502.2353 - val_loss: 50680243.5686\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49870324.5882 - val_loss: 49722495.3725\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49899614.5098 - val_loss: 50651212.5882\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 49770260.3922 - val_loss: 50268992.5098\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 50107912.4314 - val_loss: 50444058.3922\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 49772808.0392 - val_loss: 49815472.3529\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 49670399.9216 - val_loss: 49734013.5686\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 49443832.9412 - val_loss: 49886631.3333\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 49449644.1569 - val_loss: 49929191.0980\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49456874.8235 - val_loss: 49919778.0392\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 49428615.6078 - val_loss: 49921685.8431\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49448579.6078 - val_loss: 49982863.4510\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49390062.5098 - val_loss: 49951792.2745\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49382902.7843 - val_loss: 49954297.1373\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 49382879.6078 - val_loss: 49940294.6275\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 49382293.5294 - val_loss: 49934378.1569\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 49381787.2941 - val_loss: 49919954.0784\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377522.1961 - val_loss: 49920364.1961\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377283.6863 - val_loss: 49919114.3137\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377290.4706 - val_loss: 49918797.2157\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49377301.8431 - val_loss: 49916739.0196\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 49377242.3529 - val_loss: 49917461.9216\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 466us/step - loss: 112177520.4706 - val_loss: 83372344.3137\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 71842047.6471 - val_loss: 78766532.0784\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 69479413.2941 - val_loss: 69243020.3922\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 70751533.3333 - val_loss: 67551103.8431\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 65150425.7255 - val_loss: 67633144.2353\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 66820310.9020 - val_loss: 73827714.7451\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 64548796.3922 - val_loss: 66327726.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 59350638.3922 - val_loss: 71554019.0588\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 58771430.6275 - val_loss: 57737968.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 57441551.2549 - val_loss: 56972480.6275\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 57469548.5098 - val_loss: 56730714.4314\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 58285306.7451 - val_loss: 55466163.4118\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 56906604.6667 - val_loss: 55172829.2549\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 55205127.4118 - val_loss: 55006370.5882\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 54781046.9020 - val_loss: 52801754.7451\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 58339633.3529 - val_loss: 55172138.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 57206888.0392 - val_loss: 60688200.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 61663206.4706 - val_loss: 53140303.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 54258785.5686 - val_loss: 52769429.0196\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 55069513.4902 - val_loss: 56194934.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 55548910.8627 - val_loss: 50396813.4510\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 52054097.4118 - val_loss: 50778420.8235\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 52410450.6667 - val_loss: 59361402.1961\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 52608247.4118 - val_loss: 49495281.5686\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 52483176.2745 - val_loss: 49255766.0392\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51305113.4510 - val_loss: 48699331.9608\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 52401943.2157 - val_loss: 48217532.5098\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 52595206.3922 - val_loss: 53922205.6471\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 53010363.9216 - val_loss: 47974611.5294\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 50833701.7647 - val_loss: 53847590.8235\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 50469925.7255 - val_loss: 47692848.8235\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 50699573.8431 - val_loss: 47260446.5098\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 55663395.6863 - val_loss: 59998776.0000\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 52969119.9608 - val_loss: 49580857.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50977271.4510 - val_loss: 50283216.3137\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 50517434.9020 - val_loss: 48918739.8824\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 52118240.1961 - val_loss: 54060439.9216\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 49918309.6078 - val_loss: 47639306.7451\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 49062218.3922 - val_loss: 47231805.3725\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49134087.6863 - val_loss: 47914447.8039\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 49189010.6667 - val_loss: 47892338.3922\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49096643.3333 - val_loss: 47126200.1961\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 49082305.2549 - val_loss: 47084206.0784\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 49093563.1373 - val_loss: 47640517.4118\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 48883693.3725 - val_loss: 46951184.3922\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 48906863.1765 - val_loss: 46844313.1765\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 48924788.4706 - val_loss: 48093998.1176\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49306968.3529 - val_loss: 47431369.0980\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 48977516.0784 - val_loss: 46736079.6471\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 49080132.6667 - val_loss: 47171688.1176\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49027625.2157 - val_loss: 47929931.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 48971410.8235 - val_loss: 46722924.0784\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 48939153.8039 - val_loss: 46879219.2941\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48824874.5490 - val_loss: 46764624.0000\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49103964.3333 - val_loss: 47446510.2745\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 48684386.0784 - val_loss: 46671456.0000\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 49012218.0000 - val_loss: 46523194.1176\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 49261053.2549 - val_loss: 47088597.1373\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48859058.9804 - val_loss: 46956840.0000\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48951141.2941 - val_loss: 46577363.8431\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48606977.3725 - val_loss: 46401222.5098\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 48991735.4902 - val_loss: 47068292.7059\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48707459.7647 - val_loss: 47475101.4510\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49046158.9412 - val_loss: 46507849.0196\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48616365.0980 - val_loss: 46370300.4314\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48670542.3529 - val_loss: 46326393.2941\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48960379.6078 - val_loss: 49245032.5882\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48973579.4510 - val_loss: 46950624.0000\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 48741067.8824 - val_loss: 46549504.6275\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 48614576.8235 - val_loss: 46581603.8824\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48707234.9412 - val_loss: 46234767.2941\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48874011.3333 - val_loss: 46808888.4314\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 48655914.2745 - val_loss: 46196898.4706\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48616577.6863 - val_loss: 46270821.2549\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48754743.2157 - val_loss: 46156820.0000\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48697146.7059 - val_loss: 46142193.4118\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48802484.7843 - val_loss: 46250157.6471\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49038029.8431 - val_loss: 46080763.4510\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48461611.8824 - val_loss: 46086478.8235\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 48628061.3333 - val_loss: 46367287.4118\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48533904.3529 - val_loss: 46054790.3137\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48695353.7255 - val_loss: 46083341.4510\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48769452.5882 - val_loss: 46396371.4902\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 48466496.7451 - val_loss: 46145324.5882\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 48574766.9412 - val_loss: 46103161.7255\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48627729.4118 - val_loss: 45984489.7647\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 48602595.6078 - val_loss: 46795491.9216\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 48686074.4314 - val_loss: 47294454.5882\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48462661.2941 - val_loss: 45998203.4510\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48332021.6863 - val_loss: 46597420.5882\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48830690.7843 - val_loss: 46513611.1765\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48267379.4118 - val_loss: 46223622.5882\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 48240227.4902 - val_loss: 46190913.0196\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48239573.2353 - val_loss: 46203365.0196\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48231709.4118 - val_loss: 46283467.3333\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 48234059.0588 - val_loss: 46167002.5882\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48206365.1765 - val_loss: 46179169.3333\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 48207276.0000 - val_loss: 46190679.2941\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48206133.2941 - val_loss: 46177492.6667\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 48209548.6667 - val_loss: 46169522.5490\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48206746.3922 - val_loss: 46176804.9412\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48203126.1569 - val_loss: 46178531.0588\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48203168.1961 - val_loss: 46179219.3333\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48203687.2549 - val_loss: 46179582.3137\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 48204087.4510 - val_loss: 46177809.0196\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 48203231.2549 - val_loss: 46180651.2157\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 498us/step - loss: 135107947.3333 - val_loss: 61587299.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 58366253.8431 - val_loss: 60369020.0000\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 56600051.1765 - val_loss: 57204077.8431\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 54431024.7059 - val_loss: 57920596.9412\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 53066315.1373 - val_loss: 56251586.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 52317052.6667 - val_loss: 54204907.5686\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51527718.8235 - val_loss: 55924467.3333\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 51375007.2941 - val_loss: 57133960.3137\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50547762.5882 - val_loss: 52438415.8431\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 48760802.7059 - val_loss: 52885350.3529\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 48931229.8039 - val_loss: 51706756.8627\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48761950.4314 - val_loss: 51338311.4118\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 47779428.9804 - val_loss: 53826041.6471\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 48677362.1176 - val_loss: 51640131.2157\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 48453549.0980 - val_loss: 51073059.0588\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 46965916.6275 - val_loss: 49596915.1373\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48326496.4314 - val_loss: 49429024.4706\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 46913511.6863 - val_loss: 49236237.0196\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 47600780.3137 - val_loss: 49366979.3333\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 46802539.7451 - val_loss: 49494398.3922\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45949038.9020 - val_loss: 49271534.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 46790378.3922 - val_loss: 48847574.7843\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 46219966.4706 - val_loss: 48826151.8824\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45637438.8235 - val_loss: 49534403.4902\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 45780293.3725 - val_loss: 48395212.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 46969066.4706 - val_loss: 50097193.5294\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 46005415.4510 - val_loss: 54251890.4314\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 46614745.2157 - val_loss: 50298959.2941\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 45589526.4706 - val_loss: 51203094.7843\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 45474250.4706 - val_loss: 50750008.7843\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 44523411.4510 - val_loss: 48153978.1961\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43938631.2157 - val_loss: 47865620.5098\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 43995004.2157 - val_loss: 47820891.5294\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43827933.9216 - val_loss: 48092749.0588\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43982659.4510 - val_loss: 48074759.3725\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 43787381.6471 - val_loss: 47901938.2353\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43894770.0784 - val_loss: 48166186.6275\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 43903402.2353 - val_loss: 47836599.2941\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43629373.8824 - val_loss: 47822145.3725\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 43611262.2353 - val_loss: 47815912.4314\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 43589669.9216 - val_loss: 47823914.9804\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 43576566.0784 - val_loss: 47826686.0000\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43584302.5098 - val_loss: 47816938.5490\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 43571306.5098 - val_loss: 47822410.7843\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43571777.5294 - val_loss: 47811437.6863\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43574222.8627 - val_loss: 47801278.8627\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 43581656.9804 - val_loss: 47826523.1373\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 43566348.8627 - val_loss: 47816326.6667\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 43560869.0196 - val_loss: 47807132.9412\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 43558766.8235 - val_loss: 47806459.3333\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43556774.0784 - val_loss: 47808474.2745\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 43538596.9216 - val_loss: 47806346.2353\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43538870.6667 - val_loss: 47806668.5490\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43539003.6275 - val_loss: 47806951.3725\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43536263.2157 - val_loss: 47805741.3333\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 43536858.2353 - val_loss: 47804171.3725\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 43535128.0392 - val_loss: 47804181.6078\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 43534881.6078 - val_loss: 47804264.7451\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 43535333.4510 - val_loss: 47804413.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43534810.6275 - val_loss: 47804213.6863\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43534903.8039 - val_loss: 47804344.9804\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43534665.6471 - val_loss: 47804335.3725\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43534661.1765 - val_loss: 47804337.8039\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 43534667.5686 - val_loss: 47804338.3922\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43534694.8235 - val_loss: 47804327.3725\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43534688.9412 - val_loss: 47804330.9412\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_5 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 570us/step - loss: 125641743.9216 - val_loss: 64204435.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 63912883.4902 - val_loss: 57060183.6471\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 60340580.7843 - val_loss: 54601624.8235\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 58024927.0980 - val_loss: 53414166.9412\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 55444794.1176 - val_loss: 53875682.9020\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 54387859.9216 - val_loss: 51854104.5098\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 52574276.0784 - val_loss: 53970638.7843\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 53431876.9412 - val_loss: 50655439.7647\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 52611817.7647 - val_loss: 49962203.1765\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 51299453.7843 - val_loss: 50572598.0784\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 51789603.5294 - val_loss: 64218363.9216\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 52220955.4118 - val_loss: 52747463.4510\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50757355.2157 - val_loss: 48894541.4118\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 50249486.3529 - val_loss: 50057807.6078\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49834011.7255 - val_loss: 48357276.9020\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 48528842.6667 - val_loss: 49402103.9608\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 48394154.8235 - val_loss: 47578165.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 48887472.7059 - val_loss: 50737506.2353\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 47776602.1176 - val_loss: 47976153.9608\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49169859.6078 - val_loss: 48885811.2157\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49335080.7843 - val_loss: 49227391.2549\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48640694.5098 - val_loss: 47307896.3137\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 47531245.8431 - val_loss: 48250952.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 48043083.1961 - val_loss: 48423147.7647\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 49636391.8039 - val_loss: 51749091.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48742329.0196 - val_loss: 47436688.6667\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 48500362.2745 - val_loss: 48420755.9608\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 46446771.8824 - val_loss: 47766405.7255\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 46073627.3725 - val_loss: 48192164.1569\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45902918.9020 - val_loss: 47368870.3529\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45968657.6078 - val_loss: 47978443.8824\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 46092370.5490 - val_loss: 47110898.7059\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 45788049.3922 - val_loss: 47280965.8824\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45803838.0000 - val_loss: 47836380.6275\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 45657178.8235 - val_loss: 47061856.2353\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 45445105.6471 - val_loss: 46771180.7843\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 45580466.5882 - val_loss: 47458668.2353\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 45246024.3922 - val_loss: 47448523.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 45325658.0980 - val_loss: 46785121.8039\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 45163383.1373 - val_loss: 47785749.0196\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 45177750.7451 - val_loss: 46697551.1765\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45392294.8431 - val_loss: 47014961.1765\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 45182767.2157 - val_loss: 47089567.9216\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45173457.6863 - val_loss: 47207206.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 45176216.4314 - val_loss: 46991779.8039\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45299936.2353 - val_loss: 46754582.8627\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 44799785.2941 - val_loss: 46850552.0784\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44765610.0392 - val_loss: 46872336.2745\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 44745228.3922 - val_loss: 46923762.1569\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 44733547.4118 - val_loss: 46876903.6863\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44735923.8824 - val_loss: 46916216.5098\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 44694307.8431 - val_loss: 46913535.0588\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 44696476.3137 - val_loss: 46909375.6471\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44692743.6078 - val_loss: 46916496.5882\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 44691871.6471 - val_loss: 46916425.8039\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44691506.9412 - val_loss: 46912952.7843\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44688221.1765 - val_loss: 46912703.2549\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44687709.3725 - val_loss: 46912748.6275\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 74us/step - loss: 44687588.2353 - val_loss: 46913844.1569\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 44687593.4118 - val_loss: 46913207.2941\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 44687534.3922 - val_loss: 46913751.6471\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_6 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 572us/step - loss: 135934471.2157 - val_loss: 66575241.5686\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 61455651.8039 - val_loss: 61285399.5294\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 59179938.1961 - val_loss: 58539831.4510\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 57825424.4706 - val_loss: 56256853.6471\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 57041693.2941 - val_loss: 54654387.4118\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 56510542.3529 - val_loss: 60888682.1176\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 53987605.5686 - val_loss: 51393261.3725\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 52894770.3922 - val_loss: 51074179.5686\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 52717993.8824 - val_loss: 50120949.9216\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 53722502.9412 - val_loss: 50941562.1961\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50705880.3137 - val_loss: 48453647.7647\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 51416021.6078 - val_loss: 53195233.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 51477060.4314 - val_loss: 49976348.3529\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 50766060.3529 - val_loss: 47798348.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 51728691.0196 - val_loss: 48626210.5490\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 49648431.2157 - val_loss: 47034602.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 49816523.4118 - val_loss: 48091201.4510\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 49046421.9608 - val_loss: 46441505.6863\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 50337202.0392 - val_loss: 47165662.4314\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 49831516.1176 - val_loss: 46989671.2549\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 50161551.2941 - val_loss: 46520168.0000\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48769195.4902 - val_loss: 47084317.4902\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 48328651.4510 - val_loss: 45715494.0392\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 48160046.3137 - val_loss: 47773586.9412\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48545141.1765 - val_loss: 45397799.6078\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48486227.7647 - val_loss: 45231941.6471\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48475065.3333 - val_loss: 46204745.9216\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 49009174.1569 - val_loss: 46313583.7255\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 49808304.1176 - val_loss: 45114554.5490\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47970048.6667 - val_loss: 46475199.4118\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48247038.3529 - val_loss: 48360086.0392\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 47671535.2941 - val_loss: 44860403.0196\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49019351.0588 - val_loss: 44702093.9216\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48409679.0588 - val_loss: 49784610.1961\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48216292.7059 - val_loss: 44476775.4902\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 47667667.2941 - val_loss: 44295538.5098\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 47940237.2941 - val_loss: 44102151.2157\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 47965438.5098 - val_loss: 45793064.7843\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47041545.3333 - val_loss: 44186354.1569\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 46843725.7255 - val_loss: 43871218.3922\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 47761441.1373 - val_loss: 43752835.7647\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 47430273.0980 - val_loss: 49605499.9216\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47456168.2353 - val_loss: 44452847.8039\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48863348.2353 - val_loss: 46112301.8431\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 47583905.8039 - val_loss: 43868986.5098\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 47602143.8431 - val_loss: 44190301.6471\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 45967912.6863 - val_loss: 43466560.5490\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45942086.9804 - val_loss: 43686691.1765\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45735999.0784 - val_loss: 43486369.4902\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 45876465.7647 - val_loss: 43582003.8431\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 45713601.4902 - val_loss: 43551454.6667\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 45661354.1961 - val_loss: 43723714.7843\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45465351.6863 - val_loss: 43681683.6471\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45457434.8627 - val_loss: 43661130.4706\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45448497.7255 - val_loss: 43592494.3529\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45442147.1765 - val_loss: 43600354.2353\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 45439914.1569 - val_loss: 43605647.4118\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 71us/step - loss: 45421595.5686 - val_loss: 43606217.3725\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45421051.4314 - val_loss: 43606852.3137\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 45421118.4314 - val_loss: 43608980.7059\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 45420063.5686 - val_loss: 43607487.4510\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 45420242.0784 - val_loss: 43608722.7059\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45418028.0392 - val_loss: 43608522.5098\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 45417923.8039 - val_loss: 43608717.6863\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 45417971.5686 - val_loss: 43608569.2157\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 45417885.4118 - val_loss: 43608751.6078\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45417795.4902 - val_loss: 43608504.3529\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 803us/step - loss: 408406877.1765 - val_loss: 330579964.8627\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 176625718.9804 - val_loss: 105918374.9020\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 87703652.7843 - val_loss: 80171990.6667\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 74441336.7843 - val_loss: 67866599.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 68895494.1961 - val_loss: 66686903.6078\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 69493453.4902 - val_loss: 65580878.4314\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 67612451.2941 - val_loss: 64551671.0980\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 66215928.8627 - val_loss: 64067747.9608\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 65863897.0980 - val_loss: 62847888.7843\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 64941014.8235 - val_loss: 62901889.0196\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 62259569.5686 - val_loss: 62362962.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 61644691.4118 - val_loss: 60412629.3333\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 63018241.6863 - val_loss: 61937565.3333\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 62836202.5882 - val_loss: 60910957.2549\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 61655469.2157 - val_loss: 60147343.0588\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 60550575.4118 - val_loss: 59509267.9216\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 59775681.3333 - val_loss: 59039732.7059\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 59422375.7647 - val_loss: 59177702.1961\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 220us/step - loss: 58315011.2157 - val_loss: 58325984.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 57920038.6667 - val_loss: 59410970.9412\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 57375573.7255 - val_loss: 57918774.5098\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 57017707.4118 - val_loss: 57676520.1961\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 56603039.0980 - val_loss: 57554893.6471\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 56276392.1176 - val_loss: 57497150.1569\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 56236604.2353 - val_loss: 58086852.3529\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 56029626.0784 - val_loss: 57715478.1569\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 55814287.1373 - val_loss: 57904713.6078\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 55598159.3725 - val_loss: 57493231.4118\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 55559351.5686 - val_loss: 57479570.7059\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 55448974.5490 - val_loss: 57396423.6078\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 55281905.6471 - val_loss: 57525621.2157\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 55343694.1176 - val_loss: 57451067.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 55153882.1176 - val_loss: 57448736.4706\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 55137096.9412 - val_loss: 57892781.8039\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 55201001.7255 - val_loss: 57426546.9020\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 225us/step - loss: 54912784.9412 - val_loss: 57379670.1569\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 54900455.6078 - val_loss: 57417727.7647\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 54833186.6667 - val_loss: 57401098.5882\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 54855909.2941 - val_loss: 57386017.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 54813975.0980 - val_loss: 57400250.8627\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 54806532.4706 - val_loss: 57402077.6078\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 54783870.7059 - val_loss: 57406543.1373\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 54783288.7059 - val_loss: 57402976.5882\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 54782661.4510 - val_loss: 57403206.8627\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 54784368.8235 - val_loss: 57401684.0000\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 54780646.3922 - val_loss: 57403912.4706\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 54778183.3333 - val_loss: 57403573.0588\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54778081.5294 - val_loss: 57403470.0000\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 54778231.4902 - val_loss: 57403675.0196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 54777995.0196 - val_loss: 57403462.5490\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 54778074.7843 - val_loss: 57403705.8039\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 54777707.8039 - val_loss: 57403644.9020\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 54777705.0980 - val_loss: 57403666.5098\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54777672.3137 - val_loss: 57403636.7059\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 1s 236us/step - loss: 54777659.2941 - val_loss: 57403640.9020\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 1s 237us/step - loss: 54777728.7843 - val_loss: 57403655.0588\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 861us/step - loss: 139083540.7059 - val_loss: 101066495.6078\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 90960170.0392 - val_loss: 84202164.7059\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 94417585.6471 - val_loss: 97304843.6078\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 94362411.8431 - val_loss: 84264233.2549\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 87711525.0980 - val_loss: 87513833.4118\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 101887308.3922 - val_loss: 100340000.0784\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 93326950.9804 - val_loss: 156646870.7451\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 109186101.8039 - val_loss: 81537220.3137\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 83788696.9412 - val_loss: 83120527.9216\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 83609336.0000 - val_loss: 81975041.5686\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 82880653.3333 - val_loss: 82160670.1176\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 83500915.0588 - val_loss: 82762237.8039\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 83345812.0784 - val_loss: 81593199.0588\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 82652438.5882 - val_loss: 81698905.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 82565064.6275 - val_loss: 81204793.3333\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 82409156.4706 - val_loss: 80537052.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 82071935.2157 - val_loss: 80486923.5294\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 81896907.8431 - val_loss: 80437850.7451\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 81783184.3137 - val_loss: 80364113.3333\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 81700099.6078 - val_loss: 80345113.4118\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 81623148.0000 - val_loss: 80126009.1765\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 81588619.1373 - val_loss: 80278232.6275\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 81554580.6275 - val_loss: 80064336.6275\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 81413678.1176 - val_loss: 80082529.0980\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 81436060.1569 - val_loss: 79846524.5490\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 81257651.3725 - val_loss: 79833409.4118\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 81201571.2941 - val_loss: 79782882.3529\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 81077331.2941 - val_loss: 79726791.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 81061062.0392 - val_loss: 79548710.1176\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 80982312.0784 - val_loss: 79549138.5098\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 80879493.9608 - val_loss: 79387090.8235\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 81223565.2549 - val_loss: 79819085.7255\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 81217032.6275 - val_loss: 79497402.3529\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 81215977.6471 - val_loss: 79407343.2941\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 81114026.5490 - val_loss: 79314685.6471\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 81072677.1765 - val_loss: 79246695.8431\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 80879628.6275 - val_loss: 79127285.6471\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 80749249.8824 - val_loss: 78874171.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 80501394.1176 - val_loss: 78103921.2549\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 79650495.7647 - val_loss: 78322909.7255\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 79368311.6078 - val_loss: 78054956.7843\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 79404527.2941 - val_loss: 77696916.0000\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 79418160.7059 - val_loss: 77629908.3137\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 79071427.8431 - val_loss: 77411155.6863\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 79108691.0588 - val_loss: 77312531.9216\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 78990389.1373 - val_loss: 77161626.9804\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 78937880.3137 - val_loss: 77132547.6078\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 78516160.0784 - val_loss: 77044186.1961\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 78373688.5490 - val_loss: 76873655.2941\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 78175521.2549 - val_loss: 76612458.5882\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 78021499.3725 - val_loss: 76174802.4314\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 78920252.0784 - val_loss: 76257873.1765\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 78616337.8824 - val_loss: 75892014.7451\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 78449772.6275 - val_loss: 75834417.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 78415107.6078 - val_loss: 75851621.4902\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 78670341.0980 - val_loss: 75585469.8039\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 78658154.8235 - val_loss: 75348335.4510\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 78455056.6275 - val_loss: 75185437.6471\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 78219979.6863 - val_loss: 74704822.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 77721228.2353 - val_loss: 74599596.3137\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 77355327.7255 - val_loss: 74196411.9216\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 77047023.5294 - val_loss: 74293419.0588\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76827133.0196 - val_loss: 73585150.9020\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 76629590.2745 - val_loss: 73501242.6667\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 171us/step - loss: 76322841.4902 - val_loss: 73725030.9020\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76218806.6667 - val_loss: 73422946.5882\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76253538.7451 - val_loss: 73256221.5686\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 76021944.6275 - val_loss: 72590311.8431\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 75144247.1373 - val_loss: 72472967.7647\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 76108455.5294 - val_loss: 72934218.9804\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 76531383.5294 - val_loss: 73211042.9804\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 75715773.4118 - val_loss: 73497461.7255\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 76059376.9412 - val_loss: 72864324.2353\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 76154595.7647 - val_loss: 74325104.3137\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 77092613.6471 - val_loss: 74186009.8824\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 77157161.9608 - val_loss: 74162556.1569\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 77193709.2941 - val_loss: 74150208.4706\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 76612367.6863 - val_loss: 73928845.9608\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 76760293.0980 - val_loss: 74125651.9216\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 76640489.7255 - val_loss: 74073074.4314\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76586955.0980 - val_loss: 73934083.3725\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76495984.8627 - val_loss: 73703491.8431\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 76474943.0980 - val_loss: 73714956.7843\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 76471710.8627 - val_loss: 73728434.9804\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76468051.1765 - val_loss: 73728492.6275\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 76467867.4510 - val_loss: 73728570.5098\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76467692.7843 - val_loss: 73728675.2157\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 76467500.7843 - val_loss: 73728705.9608\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 76467226.5882 - val_loss: 73727949.4118\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 888us/step - loss: 407818732.1569 - val_loss: 84287360.5490\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 86359596.4706 - val_loss: 86022396.5490\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 81100528.9804 - val_loss: 89264327.2157\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 79311016.4706 - val_loss: 81830569.8039\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 72932488.2353 - val_loss: 109929820.1569\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 85539305.4118 - val_loss: 72436177.0196\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 73990634.5882 - val_loss: 78021975.7647\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 85590584.9412 - val_loss: 96539016.8627\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 82962549.4118 - val_loss: 76263789.7255\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 75217991.9216 - val_loss: 92012661.4118\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 83230637.7255 - val_loss: 84491628.3137\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 74339625.9608 - val_loss: 77104870.9804\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 70236457.3333 - val_loss: 73461957.9608\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 69543429.1765 - val_loss: 71768301.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 69589895.0588 - val_loss: 72369781.2549\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 66925986.5882 - val_loss: 72702940.3922\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 67391236.7843 - val_loss: 69574926.0392\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 69448319.4902 - val_loss: 74048475.3725\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 233us/step - loss: 68703133.5294 - val_loss: 72856931.7647\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 69761713.9608 - val_loss: 71565868.0000\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 67662095.6078 - val_loss: 70565393.0980\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 65849523.2157 - val_loss: 67847813.4118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 214us/step - loss: 66199910.1961 - val_loss: 68475224.0000\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 66019234.3529 - val_loss: 76649917.0980\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 229us/step - loss: 68203649.6078 - val_loss: 73847783.9216\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67620943.7647 - val_loss: 73553070.0392\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 66418981.2157 - val_loss: 73672645.2549\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 66461178.5490 - val_loss: 73571439.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 66473608.7451 - val_loss: 73604530.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 66288687.2549 - val_loss: 73560907.2157\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 66360777.9608 - val_loss: 73568689.0980\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 67132641.8039 - val_loss: 75339938.5882\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 67383825.0588 - val_loss: 75308078.8235\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 67344773.1765 - val_loss: 75291917.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 67296491.7647 - val_loss: 75285267.7647\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 67297557.0980 - val_loss: 75281898.5882\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 67279775.3333 - val_loss: 75406158.5882\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67310883.7255 - val_loss: 75409461.4118\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67300471.2157 - val_loss: 75417112.0784\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 67264503.3725 - val_loss: 75412434.0392\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 67307875.0980 - val_loss: 75394145.8039\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 67312908.5098 - val_loss: 75393130.9020\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 10 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6192.628560298074, 10312.562536357069, 0.5224473594907217)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 645us/step - loss: 129550009.4118 - val_loss: 99495640.6275\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 98134998.3529 - val_loss: 92968206.3529\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 90256816.4706 - val_loss: 91541754.7451\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 88982446.8235 - val_loss: 90995416.2353\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 88114691.7647 - val_loss: 91354018.2745\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 91738552.5490 - val_loss: 88420681.7255\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 87561482.1176 - val_loss: 88855851.1373\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 84303811.6863 - val_loss: 89767539.9216\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 83410492.7843 - val_loss: 90779699.7647\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 89833889.2549 - val_loss: 90143073.4902\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 86347796.6275 - val_loss: 90395379.3725\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 81959799.2941 - val_loss: 87299119.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 81179050.9020 - val_loss: 87005365.0980\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 81455814.0392 - val_loss: 87220224.7059\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 81448616.1569 - val_loss: 87253125.9608\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 81398455.5294 - val_loss: 88250805.0196\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 81175592.7843 - val_loss: 87574135.7647\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80760185.4902 - val_loss: 87954282.5098\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80495236.7059 - val_loss: 87530472.4706\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80450902.6667 - val_loss: 87347827.7647\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80515136.9412 - val_loss: 87427861.8824\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80468603.7647 - val_loss: 87293897.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80445585.3333 - val_loss: 87366490.1176\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 80374535.0196 - val_loss: 87373447.3725\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80370263.7647 - val_loss: 87363750.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 80373227.0196 - val_loss: 87361199.1373\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 80368588.4706 - val_loss: 87353626.1961\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 80369237.4902 - val_loss: 87357696.0784\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 80364662.5098 - val_loss: 87357854.5882\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 80364345.1765 - val_loss: 87357424.8627\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 80364502.3137 - val_loss: 87357686.1176\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 80364893.9608 - val_loss: 87357798.5882\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80363974.4314 - val_loss: 87356959.1373\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_14 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 607us/step - loss: 138067072.0784 - val_loss: 93219646.7451\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 94102016.9412 - val_loss: 88702702.4314\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 90449829.8824 - val_loss: 85796794.9020\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 89571938.0000 - val_loss: 92714074.1176\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88725180.0000 - val_loss: 83604334.1961\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 86602280.0000 - val_loss: 84363908.0000\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 86946841.6471 - val_loss: 82788836.4706\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88931565.1765 - val_loss: 83116589.4902\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 86226602.2745 - val_loss: 80699460.5490\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 86448478.5098 - val_loss: 84814490.9804\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 85236301.9608 - val_loss: 88702562.8235\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 88294816.2353 - val_loss: 80184527.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88243253.0196 - val_loss: 80077387.7647\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 91310277.7255 - val_loss: 80443004.3922\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 87483000.7843 - val_loss: 81141403.8431\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 84265031.6078 - val_loss: 80700778.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 85096788.0000 - val_loss: 80202441.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 82929891.1765 - val_loss: 79283896.0784\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 88177396.8627 - val_loss: 86456261.8824\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 83735149.0980 - val_loss: 80654156.1569\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 84341760.4314 - val_loss: 85687634.7451\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 85556914.9020 - val_loss: 79100888.0784\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 85977282.1961 - val_loss: 78771377.4902\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 83122513.0196 - val_loss: 79812066.3529\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 82884661.3333 - val_loss: 79764611.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 81758795.0588 - val_loss: 79976357.4510\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 81750564.0784 - val_loss: 81274667.7647\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 83614731.6078 - val_loss: 78385813.1765\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 86102224.8627 - val_loss: 82884894.8235\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 83560610.9804 - val_loss: 80457412.0784\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 82437798.5098 - val_loss: 87879963.5294\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 83004511.0588 - val_loss: 84534624.2353\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 90313474.5882 - val_loss: 97483716.6275\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 83584611.6863 - val_loss: 78747939.2157\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80147246.1961 - val_loss: 78279414.2745\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80156686.7451 - val_loss: 78537445.7255\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 79897701.4118 - val_loss: 78216195.0588\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80370300.5490 - val_loss: 78259778.2745\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80009145.2549 - val_loss: 80427195.1373\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 80116912.8627 - val_loss: 79600952.4706\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79805487.7647 - val_loss: 78157663.2941\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80280861.8431 - val_loss: 78853164.3137\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79790864.0000 - val_loss: 78492382.2745\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79504939.1373 - val_loss: 77941066.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80103775.0588 - val_loss: 79048778.9020\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79768708.7843 - val_loss: 77909061.9608\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 79703422.9020 - val_loss: 77817250.6667\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79533502.4314 - val_loss: 77929229.1765\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79951410.3529 - val_loss: 77776489.6471\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79972625.0980 - val_loss: 77818996.7843\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80470221.0980 - val_loss: 78866188.3137\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 79772701.0196 - val_loss: 77751528.7843\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79574327.8431 - val_loss: 78074043.3725\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79427181.6471 - val_loss: 78075289.2549\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79807029.8824 - val_loss: 78058161.3333\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 79447639.4510 - val_loss: 81016375.1373\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79792455.1373 - val_loss: 78185400.3137\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79116074.8235 - val_loss: 78122110.0392\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79100494.0392 - val_loss: 78033385.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79083099.2941 - val_loss: 78100056.7059\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 79076192.3137 - val_loss: 78165730.2745\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 79079130.3529 - val_loss: 78290868.7059\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79046107.6863 - val_loss: 78266299.5294\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79042179.0588 - val_loss: 78249405.3333\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79046965.8039 - val_loss: 78235384.3922\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79043170.1961 - val_loss: 78212835.4510\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79039597.8039 - val_loss: 78216179.9216\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 79034680.7843 - val_loss: 78214424.7843\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79034330.0392 - val_loss: 78212561.0196\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79034354.3137 - val_loss: 78212347.8431\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79034057.2549 - val_loss: 78212134.5882\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79033937.0980 - val_loss: 78211475.6078\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_15 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 676us/step - loss: 177828179.1373 - val_loss: 108278679.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 96620132.1569 - val_loss: 98444283.8431\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 93657493.1765 - val_loss: 94437622.3529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 91575059.0588 - val_loss: 90086436.1569\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 88035441.8824 - val_loss: 87349420.3922\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 90900295.5294 - val_loss: 88765748.3137\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 88301652.0000 - val_loss: 91687782.9804\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 87814998.0392 - val_loss: 84875037.4118\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 85626908.2353 - val_loss: 86908075.1373\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 85481545.6471 - val_loss: 83729763.1373\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 85738219.0588 - val_loss: 84232057.0196\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 84545669.2549 - val_loss: 82471869.4902\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 87467663.6863 - val_loss: 88991914.2745\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 84955505.0196 - val_loss: 82688775.7647\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 85225960.7059 - val_loss: 82012998.5882\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 85851717.4118 - val_loss: 84017847.9216\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 83185571.4510 - val_loss: 81579376.1569\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 85274928.2353 - val_loss: 83694367.2157\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 84757241.0980 - val_loss: 81623691.8431\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 85231193.4902 - val_loss: 86030260.6275\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 83415727.5686 - val_loss: 87197192.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 83692735.6863 - val_loss: 83846058.8235\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 82670835.7647 - val_loss: 80950463.9216\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80841676.0784 - val_loss: 81678050.3529\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 81031191.2549 - val_loss: 80613401.4902\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 81250499.1373 - val_loss: 80769238.1176\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 81175370.1176 - val_loss: 80462780.8627\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80710519.0588 - val_loss: 80599252.9412\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 80806574.9804 - val_loss: 80827862.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80465333.8039 - val_loss: 80283076.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80777164.4706 - val_loss: 80191216.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80747791.0588 - val_loss: 80853895.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80532755.0980 - val_loss: 80290084.5490\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80624005.6471 - val_loss: 80006581.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 80144530.7451 - val_loss: 81015853.8824\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 80328092.3922 - val_loss: 80064687.1373\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80408314.1961 - val_loss: 79890460.5490\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 80522155.4510 - val_loss: 82750018.8235\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 80764850.4706 - val_loss: 81657626.1961\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80316342.5882 - val_loss: 79803168.0784\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 81326376.6275 - val_loss: 79766894.1176\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80295816.8627 - val_loss: 79734844.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79686095.1373 - val_loss: 82948073.2549\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80814731.8431 - val_loss: 80143386.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 79999822.3529 - val_loss: 80231850.2745\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80322903.4510 - val_loss: 79655573.0980\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80586450.9020 - val_loss: 79599043.7647\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79973390.6667 - val_loss: 79735950.0392\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80026362.8235 - val_loss: 79659672.7843\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 80128099.2941 - val_loss: 80433641.2549\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80034679.1373 - val_loss: 79640093.5686\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80148740.8627 - val_loss: 79537526.0392\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 79938273.8431 - val_loss: 79657356.4706\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79698484.5882 - val_loss: 80141143.0588\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79998218.4314 - val_loss: 79958338.4314\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79685359.9216 - val_loss: 79856695.2941\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79841710.1961 - val_loss: 79435526.4314\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 79721904.8627 - val_loss: 79610653.1765\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 79395358.5098 - val_loss: 79489905.9608\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79907797.1765 - val_loss: 80220077.3333\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79545641.0196 - val_loss: 79737851.9216\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79979598.2353 - val_loss: 79694516.1569\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79223600.2353 - val_loss: 79551381.4118\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79219850.7059 - val_loss: 79415923.2941\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79225521.7255 - val_loss: 79492626.9804\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79204986.1961 - val_loss: 79496670.4314\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 79167170.2745 - val_loss: 79439071.2941\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79200623.8431 - val_loss: 79487139.6863\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79166931.5294 - val_loss: 79512208.3137\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79122979.4118 - val_loss: 79502056.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79126159.2157 - val_loss: 79503902.5882\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79122773.0980 - val_loss: 79481922.5098\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79122201.0980 - val_loss: 79478582.7451\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79121976.5490 - val_loss: 79478598.8235\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 79116648.0784 - val_loss: 79479264.5490\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79116033.0980 - val_loss: 79478138.9020\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79116763.9216 - val_loss: 79475677.0196\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79116260.0784 - val_loss: 79477647.5294\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79116081.0196 - val_loss: 79476814.1961\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79115453.3333 - val_loss: 79476804.0000\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79115475.9216 - val_loss: 79476734.0392\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79115464.0000 - val_loss: 79476730.9804\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79115456.0000 - val_loss: 79476670.1176\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79115454.8235 - val_loss: 79476650.5882\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 770us/step - loss: 147814872.9412 - val_loss: 104910030.0392\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 101801252.3137 - val_loss: 95499301.1765\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 98084067.6078 - val_loss: 101566383.7647\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 91951195.2157 - val_loss: 91306630.5098\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 93254251.9216 - val_loss: 98008458.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 91454585.4118 - val_loss: 92867548.1569\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 91041861.8824 - val_loss: 89055946.5098\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 86029199.8431 - val_loss: 89154607.8431\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 84510642.7451 - val_loss: 87908942.6667\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 85076673.2549 - val_loss: 91911061.3333\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87322138.8627 - val_loss: 89199871.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 85779492.0000 - val_loss: 87332140.0000\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 82583806.5882 - val_loss: 99397982.9804\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 83597587.2157 - val_loss: 91053220.5490\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 81447796.3137 - val_loss: 88992593.6471\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 82076597.6471 - val_loss: 85470812.9412\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 81056806.6667 - val_loss: 84720660.0784\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 81831108.6275 - val_loss: 87456941.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 80749167.9216 - val_loss: 85335524.7059\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 80207553.7255 - val_loss: 90414273.4902\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 80967308.2353 - val_loss: 83770296.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 79748518.6667 - val_loss: 84464642.5882\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 80179803.2941 - val_loss: 84768857.7255\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79405210.0392 - val_loss: 83784521.9608\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81089462.4706 - val_loss: 90873095.4510\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79203590.9020 - val_loss: 95962429.7255\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 81615975.6863 - val_loss: 84184809.8824\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 77913689.3333 - val_loss: 84480177.3333\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77649108.2353 - val_loss: 83596183.5294\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 78209102.3922 - val_loss: 83471180.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77873463.6078 - val_loss: 83305678.9804\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77534016.7059 - val_loss: 83253344.0000\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77450999.0980 - val_loss: 84445648.6275\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 77819812.8627 - val_loss: 84256879.8431\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77550548.0784 - val_loss: 83512638.7451\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 77372438.0784 - val_loss: 83311713.6471\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 77321942.9020 - val_loss: 83956902.5882\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 77213559.0588 - val_loss: 83554334.5098\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 77117887.6078 - val_loss: 83459966.1961\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 77131872.3922 - val_loss: 83409116.0784\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77130818.8235 - val_loss: 83403614.7451\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 77102888.0000 - val_loss: 83420005.4118\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77088213.1765 - val_loss: 83412329.8824\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 77087506.4314 - val_loss: 83420091.2157\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 77092682.5882 - val_loss: 83414766.5882\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 77085386.5882 - val_loss: 83412335.6863\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 77085087.2157 - val_loss: 83410927.9216\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77082843.2941 - val_loss: 83410113.3333\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 77082937.5686 - val_loss: 83410424.3922\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77083222.9020 - val_loss: 83410696.9412\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77083136.0784 - val_loss: 83409355.2941\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77082953.1765 - val_loss: 83408465.7255\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 829us/step - loss: 166266639.2157 - val_loss: 109397458.7451\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 105532747.1373 - val_loss: 98062659.1373\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 98468983.8431 - val_loss: 88971025.8039\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 97485288.1569 - val_loss: 89355689.2549\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 93525733.0980 - val_loss: 86592620.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 91788554.8235 - val_loss: 86850713.2549\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 91090018.6667 - val_loss: 84181775.6863\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 91275203.7647 - val_loss: 83142132.7843\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 86936087.1373 - val_loss: 82178061.2549\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 91321144.9412 - val_loss: 82035040.7843\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 91274281.3333 - val_loss: 81522157.2549\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 87835145.1765 - val_loss: 81227651.2941\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 90336644.4706 - val_loss: 85876499.0588\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 87607158.2745 - val_loss: 80491073.1765\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 85016595.2157 - val_loss: 80586192.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 84736041.8824 - val_loss: 79943913.0980\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 83618715.0196 - val_loss: 81479631.2941\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 85221384.8627 - val_loss: 92297020.1569\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 89824539.0588 - val_loss: 79431118.5098\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 83374945.7255 - val_loss: 79316267.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 85614398.6667 - val_loss: 78911228.6275\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 83446390.5098 - val_loss: 79186047.0588\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 89826365.6471 - val_loss: 79279774.9020\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 84740324.4706 - val_loss: 79774417.1373\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 82004445.1765 - val_loss: 81238474.5098\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 82245379.3725 - val_loss: 78592416.7843\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 83954387.9216 - val_loss: 78862664.3922\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 84968783.5294 - val_loss: 86687194.1961\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 83450491.3725 - val_loss: 83841149.5686\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 81884731.2941 - val_loss: 79832473.3333\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 82596822.1176 - val_loss: 86505195.5294\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 80959724.1569 - val_loss: 78243393.4118\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 80260545.3333 - val_loss: 78125051.4510\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 80327407.1373 - val_loss: 78200883.1373\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 80066628.4706 - val_loss: 78837704.7843\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 80163800.3137 - val_loss: 79003818.7451\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 80043575.2549 - val_loss: 78080674.0392\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 79979266.4314 - val_loss: 78577519.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 80177521.8824 - val_loss: 78664618.2745\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 79878454.8235 - val_loss: 78014463.1373\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 79528810.9804 - val_loss: 80788943.6078\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 80042968.3137 - val_loss: 78319934.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 79919219.6078 - val_loss: 79095888.7059\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79726550.1176 - val_loss: 77875009.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79841688.0000 - val_loss: 78048596.2353\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79780013.3725 - val_loss: 77871013.7255\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 80072658.7451 - val_loss: 78590140.0784\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79850678.2745 - val_loss: 77943690.3529\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 88us/step - loss: 79804601.6471 - val_loss: 78181030.3529\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79699153.8824 - val_loss: 79634230.9804\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 80066804.2353 - val_loss: 78456549.1765\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79519289.1765 - val_loss: 78234946.6667\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79509021.5686 - val_loss: 78270630.1961\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79482878.9804 - val_loss: 78250895.1373\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79514159.2157 - val_loss: 78202150.0392\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 79488227.3725 - val_loss: 78261656.7059\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79467265.8039 - val_loss: 78263601.2549\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79465927.4510 - val_loss: 78260747.1373\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 79468886.6667 - val_loss: 78258175.8431\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 79468364.0000 - val_loss: 78256866.4314\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79465775.9216 - val_loss: 78257685.9608\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79461715.9216 - val_loss: 78258447.7647\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79462120.5490 - val_loss: 78256316.4706\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79462196.5490 - val_loss: 78256555.0588\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79461963.6078 - val_loss: 78257937.9608\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79461502.5098 - val_loss: 78257258.1961\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 816us/step - loss: 160503691.0588 - val_loss: 122343913.4118\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 105302894.1961 - val_loss: 105713483.6863\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 99177097.4118 - val_loss: 99711250.1961\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 91948745.4118 - val_loss: 109306292.7059\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 93529054.1961 - val_loss: 94066598.7451\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 89374360.6275 - val_loss: 91625064.5490\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 89122697.6471 - val_loss: 87727334.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 88537429.6471 - val_loss: 91436759.3725\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 86811622.9804 - val_loss: 94247732.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87770977.4118 - val_loss: 86584418.3529\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 87869163.0588 - val_loss: 84250660.1569\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 86815263.9216 - val_loss: 85205120.0000\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 85589223.6078 - val_loss: 83660945.4902\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 83495368.0784 - val_loss: 83192168.2353\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 84414561.1765 - val_loss: 84816139.9216\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 82966033.4118 - val_loss: 82767666.8235\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 84784321.6471 - val_loss: 82534641.4118\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 83405026.7451 - val_loss: 80200147.6078\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 83392628.3922 - val_loss: 84804020.2353\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 84221148.7843 - val_loss: 80236158.3529\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 82681327.0588 - val_loss: 80035496.8627\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82992495.7647 - val_loss: 81482424.8627\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 82701400.2353 - val_loss: 79757893.5686\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 82711273.0196 - val_loss: 80295913.7255\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87900038.2745 - val_loss: 82827730.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 81321005.0980 - val_loss: 79846107.0588\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 82164297.1373 - val_loss: 79165952.0784\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82754631.2941 - val_loss: 81456684.3922\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 82500038.7451 - val_loss: 79352610.9020\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 80709553.0980 - val_loss: 81288881.0980\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81587346.7451 - val_loss: 81790712.9412\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 82905307.3725 - val_loss: 87726462.5098\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81360762.1176 - val_loss: 78909183.9216\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79460841.7255 - val_loss: 78282589.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 79461501.4902 - val_loss: 79139069.1765\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79438160.4706 - val_loss: 78618985.0980\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 79571246.9020 - val_loss: 78486696.6275\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 79491330.5882 - val_loss: 78872636.7059\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79446718.5098 - val_loss: 78522182.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79157264.5882 - val_loss: 78353759.3725\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79103514.2745 - val_loss: 78300483.6078\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79139144.3922 - val_loss: 78320985.3333\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79108116.1569 - val_loss: 78253493.1765\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 79108530.1176 - val_loss: 78280774.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79115517.3333 - val_loss: 78279882.4314\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79102577.0196 - val_loss: 78327639.7647\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79089594.1961 - val_loss: 78307222.9020\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79097627.0588 - val_loss: 78291430.5098\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79069144.8627 - val_loss: 78289830.6667\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79069570.0392 - val_loss: 78287871.6863\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 79068381.8824 - val_loss: 78289668.8627\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79067360.0392 - val_loss: 78288112.8627\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 109us/step - loss: 79069153.8824 - val_loss: 78292456.8627\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79064670.8235 - val_loss: 78292840.6275\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 79064043.4510 - val_loss: 78292321.6471\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 79063926.3529 - val_loss: 78291532.0000\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79064134.9804 - val_loss: 78291856.6275\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 79063940.2353 - val_loss: 78291605.2549\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79063592.0000 - val_loss: 78291588.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79063595.2157 - val_loss: 78291581.4118\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79063586.1961 - val_loss: 78291585.7255\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 79063616.7059 - val_loss: 78291601.7255\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 79063616.0000 - val_loss: 78291604.5490\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_7 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 818us/step - loss: 213724590.7451 - val_loss: 102536294.3529\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 93730012.0000 - val_loss: 92588372.4706\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 90818775.3725 - val_loss: 92366884.3922\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 90470424.7059 - val_loss: 90519173.5686\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 87889739.3725 - val_loss: 92055105.9608\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 87507276.1569 - val_loss: 89205338.9804\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 85955443.6863 - val_loss: 89138011.3725\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 85516181.6471 - val_loss: 91170161.5686\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 84660237.9608 - val_loss: 92285380.2353\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 81899192.7843 - val_loss: 87520634.7451\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 83390548.4706 - val_loss: 86604120.3137\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82710737.2549 - val_loss: 85491737.2549\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 81317283.4510 - val_loss: 86820100.7843\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 82670225.9608 - val_loss: 85162621.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 80437289.1765 - val_loss: 85508076.4706\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 80650200.6275 - val_loss: 87964408.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 80765637.2549 - val_loss: 84382730.7451\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79884262.5490 - val_loss: 86058466.5882\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79110828.4706 - val_loss: 90318013.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 80491793.0980 - val_loss: 85734150.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 79597318.3529 - val_loss: 84470656.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 79514265.4118 - val_loss: 85179759.2941\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 77630460.7059 - val_loss: 83596701.2549\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 77653647.2157 - val_loss: 83489694.4314\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 77562428.9412 - val_loss: 84416645.9608\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 77512907.1373 - val_loss: 84126611.8431\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 77349627.0588 - val_loss: 83381745.4118\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 77425695.4118 - val_loss: 83373492.7843\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 77366676.3137 - val_loss: 83696957.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 77259090.7451 - val_loss: 83558350.8235\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 77237799.2157 - val_loss: 83110812.7059\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 77048212.3922 - val_loss: 83942245.4902\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 77277056.4314 - val_loss: 83321940.7843\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77264559.7647 - val_loss: 83825721.5686\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77330369.1765 - val_loss: 83542519.8431\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 77167179.6471 - val_loss: 82906878.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 77189598.3529 - val_loss: 83213867.1373\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 76874058.5098 - val_loss: 82766675.4510\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 76857480.7059 - val_loss: 83269273.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76813557.0980 - val_loss: 82888745.0980\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76849270.4314 - val_loss: 83024335.9216\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 76848790.9412 - val_loss: 83091113.4902\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76781543.3333 - val_loss: 82867458.9804\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 76422119.6078 - val_loss: 82684909.3333\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 76395428.3922 - val_loss: 82730273.4118\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 76378342.6667 - val_loss: 82635419.4510\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76372492.4706 - val_loss: 82621477.7255\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 76377096.3529 - val_loss: 82594562.1176\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76358605.6471 - val_loss: 82650999.6078\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 76352971.6863 - val_loss: 82574297.6471\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 76346919.9216 - val_loss: 82605108.5490\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 76355128.0784 - val_loss: 82613355.7647\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 76335437.2549 - val_loss: 82632850.6667\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 76346201.1765 - val_loss: 82562084.6275\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76364542.4314 - val_loss: 82626263.3725\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76387587.3725 - val_loss: 82554152.0784\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76367237.3333 - val_loss: 82575028.5490\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76323863.7647 - val_loss: 82568488.2353\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76322590.0392 - val_loss: 82535795.4510\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76334361.5686 - val_loss: 82546516.3137\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 76331735.0588 - val_loss: 82578930.2745\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 76332302.0392 - val_loss: 82536935.7647\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 76352376.6275 - val_loss: 82542758.0392\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 76312828.7843 - val_loss: 82571850.1961\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 76279287.3725 - val_loss: 82564443.0588\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 76273433.0980 - val_loss: 82564860.7843\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 76272336.5490 - val_loss: 82562544.1569\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76270650.1176 - val_loss: 82565517.0196\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76272648.7843 - val_loss: 82567795.3725\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 76268198.2353 - val_loss: 82567382.1176\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 76268049.0588 - val_loss: 82566685.1765\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76267932.0000 - val_loss: 82566916.7059\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76267812.1569 - val_loss: 82566744.5490\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 76268025.4902 - val_loss: 82566837.8039\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 76267483.2157 - val_loss: 82566833.3333\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76267479.6863 - val_loss: 82566796.3137\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 76267475.0588 - val_loss: 82566788.3137\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 76267464.5490 - val_loss: 82566771.2157\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 76267498.9804 - val_loss: 82566816.0000\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_8 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 843us/step - loss: 208236590.9804 - val_loss: 87371934.5882\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 91506375.5294 - val_loss: 85930965.8824\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 90424060.0000 - val_loss: 89200661.8039\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 89430441.8039 - val_loss: 85039004.6275\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 88374083.7647 - val_loss: 83070494.5882\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 86882559.1373 - val_loss: 82523499.1373\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 88398969.8235 - val_loss: 86964850.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 85914029.8824 - val_loss: 82392996.7059\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 85512976.5490 - val_loss: 81861073.0196\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 84733351.3725 - val_loss: 81762732.4706\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 83981954.9020 - val_loss: 82067242.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 83988631.7647 - val_loss: 81313656.5490\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 83496227.9216 - val_loss: 84081457.5686\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 84731810.8235 - val_loss: 80555935.5294\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 82744796.7059 - val_loss: 80147506.3529\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 82224448.2353 - val_loss: 81763602.6667\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 82394619.2941 - val_loss: 86845100.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 84173189.4902 - val_loss: 82467210.0392\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 83784776.1569 - val_loss: 81478129.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 83406215.6863 - val_loss: 80309756.2353\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 82471134.4314 - val_loss: 80029707.8431\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 81394933.7647 - val_loss: 79891215.5294\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 81522802.2745 - val_loss: 80862500.0000\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 81262523.9216 - val_loss: 80523218.7451\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 81133781.3333 - val_loss: 79972317.8824\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 80968541.6078 - val_loss: 79739128.5490\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 81409950.5882 - val_loss: 80435173.4902\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 81003451.6863 - val_loss: 79743451.8431\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 81009814.2353 - val_loss: 79708206.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 80739536.1569 - val_loss: 80927213.9608\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 80777202.1961 - val_loss: 80830206.3529\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 80581018.7451 - val_loss: 79801162.1176\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 80517099.5686 - val_loss: 79787854.5882\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 80427350.9412 - val_loss: 79644839.4510\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 80336957.8039 - val_loss: 79699356.7843\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 80291791.8431 - val_loss: 80888262.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 80274745.3333 - val_loss: 79722092.9412\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 80094977.8039 - val_loss: 80000285.9608\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79986840.0000 - val_loss: 79524470.6667\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 80011467.4510 - val_loss: 79553559.4510\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79833704.3137 - val_loss: 80018089.8039\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79848038.0392 - val_loss: 79687260.1569\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79870603.7647 - val_loss: 79549168.2353\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79737410.6667 - val_loss: 80351360.8627\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79754568.7059 - val_loss: 80028687.2157\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79649756.8627 - val_loss: 79878176.5490\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79624934.9804 - val_loss: 79798195.9216\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 79609159.2157 - val_loss: 79813828.2353\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 79599115.5294 - val_loss: 79712698.5098\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 79575356.3137 - val_loss: 79717107.7647\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79576270.2745 - val_loss: 79719862.3529\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79574315.2157 - val_loss: 79716592.7843\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79574667.2941 - val_loss: 79716606.5098\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79574378.1176 - val_loss: 79718702.2745\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79571493.7255 - val_loss: 79717964.1569\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79571340.2353 - val_loss: 79717723.3725\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 79571209.8824 - val_loss: 79718263.2157\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79571333.3333 - val_loss: 79718565.8824\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79571139.9216 - val_loss: 79718379.8431\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_9 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 915us/step - loss: 135953817.8824 - val_loss: 96976651.0588\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 91109233.0196 - val_loss: 92439895.5294\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 90664792.3922 - val_loss: 90336155.4510\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 88443198.5098 - val_loss: 88514929.6471\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 87339977.8039 - val_loss: 88246529.7255\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 87688131.7647 - val_loss: 85554693.4902\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 86645661.6471 - val_loss: 88414197.4902\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 85689850.6667 - val_loss: 83625509.8039\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 84680011.1373 - val_loss: 84641883.2941\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 109us/step - loss: 84840807.7647 - val_loss: 89138936.3137\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 85192539.7647 - val_loss: 83478465.3333\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 84391319.5294 - val_loss: 81239040.7843\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 84567717.4118 - val_loss: 81352105.0196\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 84425283.7647 - val_loss: 81415212.0784\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 83762512.0784 - val_loss: 83104634.9020\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 85512250.4314 - val_loss: 82707360.3922\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 83234975.7647 - val_loss: 81162506.7451\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 82868038.4314 - val_loss: 81087706.4314\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 84538447.0588 - val_loss: 79790745.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 81745658.8235 - val_loss: 79581256.7059\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 82464051.9216 - val_loss: 90310402.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 82043512.5490 - val_loss: 79351926.1176\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 81445286.2745 - val_loss: 79939240.1569\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 80850200.8627 - val_loss: 80404616.4706\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 83546872.7843 - val_loss: 79830581.7255\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 81899190.0000 - val_loss: 80243149.0980\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 81788605.9216 - val_loss: 80206535.5294\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 80189010.4314 - val_loss: 78212420.0000\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79544185.1765 - val_loss: 78102109.8824\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 79507264.1569 - val_loss: 78292548.1569\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79540191.2157 - val_loss: 78212863.4510\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 79278480.3137 - val_loss: 78050878.8235\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79345772.4706 - val_loss: 77993163.8431\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79350273.6471 - val_loss: 78298563.6078\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 79416296.4706 - val_loss: 77998973.2549\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 79156768.2353 - val_loss: 77959572.3922\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78942064.2353 - val_loss: 78069934.1961\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79202137.6863 - val_loss: 79313670.1176\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79219417.8824 - val_loss: 78021311.4510\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 78909705.8431 - val_loss: 78115246.7451\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78893241.0196 - val_loss: 77945120.0000\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78810431.6863 - val_loss: 77829901.3333\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 78964372.5490 - val_loss: 77928069.3333\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 78681836.6275 - val_loss: 77836806.0392\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 78755785.3333 - val_loss: 77853288.0784\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 78655470.2745 - val_loss: 77830080.3922\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 78629887.2941 - val_loss: 78401561.5686\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 78748600.4706 - val_loss: 77927258.8235\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78487919.6863 - val_loss: 77859261.8824\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78418417.5686 - val_loss: 77819054.5098\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78413110.9804 - val_loss: 77793733.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 78441716.5490 - val_loss: 77810767.5294\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 78431426.5882 - val_loss: 77769536.0784\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 118us/step - loss: 78408497.6078 - val_loss: 77793385.9608\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78416566.1176 - val_loss: 77783128.4706\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 78406778.4314 - val_loss: 77806482.9804\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 78398578.5490 - val_loss: 77779556.8627\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 78392490.8235 - val_loss: 77784606.7451\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78365999.3725 - val_loss: 77787908.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78368747.2157 - val_loss: 77786245.3333\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78364798.5490 - val_loss: 77786615.5294\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 78365829.8039 - val_loss: 77785705.0196\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 78367917.3333 - val_loss: 77782983.4510\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 78360979.4510 - val_loss: 77783001.1765\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 78361302.7451 - val_loss: 77783267.4510\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78362261.6471 - val_loss: 77782605.7255\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78360837.8824 - val_loss: 77782965.1765\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 78360927.1765 - val_loss: 77783015.4510\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 127us/step - loss: 78360619.3725 - val_loss: 77783020.0784\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 78360616.4706 - val_loss: 77783018.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 78360614.5882 - val_loss: 77783017.5686\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 78360616.6667 - val_loss: 77783023.6863\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 78360616.4706 - val_loss: 77783018.6667\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 494285776.9412 - val_loss: 156438493.8039\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 138931980.0784 - val_loss: 112372941.8824\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 113740850.3529 - val_loss: 106236763.9216\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 110909238.7451 - val_loss: 101310583.6863\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 104883198.2745 - val_loss: 97303520.3137\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 100271682.9804 - val_loss: 99478035.7647\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 98861416.5490 - val_loss: 99059819.4510\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 111264492.7059 - val_loss: 108328921.1765\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 107584831.5294 - val_loss: 102184961.4902\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 103874750.4314 - val_loss: 99256614.6667\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 98852231.2941 - val_loss: 99208967.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 97726495.0588 - val_loss: 99522613.9608\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 99934937.7255 - val_loss: 99638750.2745\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 98816310.3529 - val_loss: 98281887.4510\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 98130376.6275 - val_loss: 98072254.7451\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 97329132.3922 - val_loss: 97896851.8431\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 97090086.1961 - val_loss: 98065913.1765\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 97112633.0980 - val_loss: 97634243.7647\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 97131849.4902 - val_loss: 96975057.4902\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 96981260.8627 - val_loss: 96891074.3529\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 96963664.1569 - val_loss: 96594383.2941\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 96993045.1765 - val_loss: 96561632.5490\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 230us/step - loss: 96855605.3333 - val_loss: 96862034.9804\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 97312870.7451 - val_loss: 96681665.3333\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 96918835.2157 - val_loss: 96771576.7059\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 97001043.0588 - val_loss: 96791917.0980\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96901162.4314 - val_loss: 96806971.4510\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 96930874.7451 - val_loss: 96728174.5098\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 96785188.4706 - val_loss: 97063736.0000\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 96689699.2941 - val_loss: 96701973.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 96702271.2941 - val_loss: 96554643.3725\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 96615998.7451 - val_loss: 96815439.3725\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 96591394.8235 - val_loss: 96748623.2941\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 96575704.1569 - val_loss: 96781917.8824\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 96611180.0784 - val_loss: 96913454.6667\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 96598452.6275 - val_loss: 96907907.0588\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 96573417.4902 - val_loss: 96906224.9412\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96573571.1373 - val_loss: 96899922.7451\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96572572.8627 - val_loss: 96900379.9216\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96573222.1176 - val_loss: 96901911.7647\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 96571494.4314 - val_loss: 96902376.3137\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 96570400.9412 - val_loss: 96902410.8235\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 96570358.5882 - val_loss: 96902382.6667\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 96570342.3529 - val_loss: 96902408.0784\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 96570314.9020 - val_loss: 96902400.4706\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 96570280.4706 - val_loss: 96902394.3529\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 96570252.7059 - val_loss: 96902395.9216\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 96570253.1765 - val_loss: 96902394.9020\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 96570250.5882 - val_loss: 96902394.2745\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 96570252.2353 - val_loss: 96902394.6667\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 96570251.4510 - val_loss: 96902394.5098\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 426244818.5098 - val_loss: 163206097.2549\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 130738648.9412 - val_loss: 119624087.7647\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 119020760.1569 - val_loss: 138237734.9020\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 132354556.8627 - val_loss: 102501016.3137\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 108892387.5294 - val_loss: 113778157.2549\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 110865417.7255 - val_loss: 114159373.6471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 108939485.6471 - val_loss: 111233822.4314\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 110819786.4314 - val_loss: 132673629.7255\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 142691937.2549 - val_loss: 110280754.1176\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 225us/step - loss: 112905996.9412 - val_loss: 109285466.0392\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 109892279.6078 - val_loss: 109242301.7255\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 109425825.0980 - val_loss: 107965519.6863\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 105687623.7647 - val_loss: 103451909.9608\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 224us/step - loss: 105766994.1961 - val_loss: 104392716.7843\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 106569866.0392 - val_loss: 103613029.5686\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 106056493.4902 - val_loss: 103899222.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 105717336.7843 - val_loss: 103475693.4118\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 105804453.0196 - val_loss: 103730097.8039\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 105618924.7059 - val_loss: 103449592.1569\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 105570179.2157 - val_loss: 103387827.6078\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 106277553.0196 - val_loss: 103563784.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 106091227.7647 - val_loss: 104717031.6863\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 106090904.9412 - val_loss: 104775477.7255\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 105700108.2353 - val_loss: 104765620.0784\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 313867770.3529 - val_loss: 144243500.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 128542064.4706 - val_loss: 128268490.9804\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 125539354.7451 - val_loss: 128898265.5686\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 111493032.6275 - val_loss: 127047080.4706\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 114454669.8039 - val_loss: 116225869.4902\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 105794367.2157 - val_loss: 112260505.0980\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 114526382.0392 - val_loss: 127449116.2353\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 117062977.8039 - val_loss: 123178639.8431\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 112955348.0784 - val_loss: 114367032.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 229us/step - loss: 104334621.5686 - val_loss: 107965076.1569\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 99550998.4314 - val_loss: 103347578.8235\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 97516005.8824 - val_loss: 101466654.1176\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 94673263.3725 - val_loss: 98102318.9020\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 93500883.2157 - val_loss: 97079084.6275\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 91806661.2549 - val_loss: 92776525.0196\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 90793722.4314 - val_loss: 93930332.0000\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 91483149.4118 - val_loss: 91375992.7843\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 89884833.9608 - val_loss: 91257579.1373\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 90364069.8039 - val_loss: 96524776.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 89296985.4118 - val_loss: 94508978.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 88982799.2941 - val_loss: 89551091.5294\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 220us/step - loss: 89056907.4510 - val_loss: 89014724.9412\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 228us/step - loss: 89388667.6078 - val_loss: 89446232.4706\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 89246160.0784 - val_loss: 88947594.5882\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 89493457.8039 - val_loss: 88417104.0784\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 88691375.3725 - val_loss: 88308164.5490\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 87539072.8627 - val_loss: 92103535.0588\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 88667190.7451 - val_loss: 89230947.9216\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 88801607.2941 - val_loss: 88920344.7059\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 88251366.1961 - val_loss: 87769068.7059\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 87850215.4510 - val_loss: 87880941.3333\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 87813968.3137 - val_loss: 88271850.1176\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 88244458.8235 - val_loss: 87605515.5294\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 87198071.2941 - val_loss: 87374079.7647\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 87840486.5098 - val_loss: 90925083.1373\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 90418764.2353 - val_loss: 88175458.4314\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 87286130.1176 - val_loss: 89179778.7451\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 87832618.7451 - val_loss: 90008825.1765\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 87689834.8235 - val_loss: 87915756.4706\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 245us/step - loss: 86808519.1373 - val_loss: 86737648.5490\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 86797131.7647 - val_loss: 86795015.8431\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 1s 224us/step - loss: 86683860.0784 - val_loss: 86926961.7255\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 86683033.8824 - val_loss: 87036563.7647\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 86704396.1569 - val_loss: 86863297.9608\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 1s 212us/step - loss: 86659135.2941 - val_loss: 86814094.8235\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 86585356.6275 - val_loss: 86821088.2353\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 86569319.3725 - val_loss: 86793669.8039\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 86579217.4118 - val_loss: 86810974.0392\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 1s 228us/step - loss: 86579597.3333 - val_loss: 86798735.1373\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 86568187.3725 - val_loss: 86771225.5686\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 86561491.8039 - val_loss: 86770697.0980\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 86561442.0392 - val_loss: 86770655.8431\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 86560382.1961 - val_loss: 86771439.2941\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 86561979.5294 - val_loss: 86773301.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 86559798.7451 - val_loss: 86772931.4510\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 86559403.5294 - val_loss: 86773091.7647\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 86559307.6078 - val_loss: 86772809.4902\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 86559260.7059 - val_loss: 86772881.6471\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 86559387.4118 - val_loss: 86772701.2549\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 86559384.5490 - val_loss: 86772905.4902\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "        ['ARIMA', ARIMA],\n",
    "        ['SARIMAX', sarimax.SARIMAX],\n",
    "        ['ANN', build_ann],\n",
    "        ['CNN', build_cnn],\n",
    "        ['RNN', build_rnn],\n",
    "        ['LSTM', build_lstm]]\n",
    "initialization()\n",
    "X = data_X.values[:,:,np.newaxis]\n",
    "performance_y_test = {}\n",
    "performance_y_test[5]={}\n",
    "performance_y_test[5][\"MORN\"]={}\n",
    "performance_y_test[5][\"MORN\"][\"MAE\"] = 2435\n",
    "performance_y_test[5][\"MORN\"][\"RMSE\"] = 3567\n",
    "performance_y_test[5][\"MORN\"][\"R2\"] = 0.94\n",
    "performance_y_test[7]={}\n",
    "performance_y_test[7][\"MORN\"]={}\n",
    "performance_y_test[7][\"MORN\"][\"MAE\"] = 3028\n",
    "performance_y_test[7][\"MORN\"][\"RMSE\"] = 4389\n",
    "performance_y_test[7][\"MORN\"][\"R2\"] = 0.91\n",
    "performance_y_test[10]={}\n",
    "performance_y_test[10][\"MORN\"]={}\n",
    "performance_y_test[10][\"MORN\"][\"MAE\"] = 3580\n",
    "performance_y_test[10][\"MORN\"][\"RMSE\"] = 5367\n",
    "performance_y_test[10][\"MORN\"][\"R2\"] = 0.871\n",
    "\n",
    "n_splits = 3\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=3)\n",
    "for lead in LEAD:\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%% start experiments with lead time \"+str(lead)+\" %%%%%%%%%%%%%%%%%%%%\")\n",
    "    y = data_y.loc[:,'Q_'+str(lead)].values \n",
    "    X_train, y_train, X_test, y_test = X[:TRAIN_TEST], y[:TRAIN_TEST], X[TRAIN_TEST:], y[TRAIN_TEST:]    \n",
    "    \n",
    "    for name, model in models:\n",
    "        mae=0\n",
    "        rmse=0\n",
    "        r2=0\n",
    "        performance_y_test[lead][name]={}\n",
    "        if name == \"ARIMA\":            \n",
    "            prediction = list()\n",
    "            for t in X_test:\n",
    "                clf = model(t, order=(1,1,0))\n",
    "                clf_fit = clf.fit()\n",
    "                yhat = clf_fit.forecast(steps=lead)[0][-1]\n",
    "                prediction.append(yhat)\n",
    "            m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "            performance_y_test[lead][name][\"MAE\"] = m_mae\n",
    "            performance_y_test[lead][name][\"RMSE\"] = m_rmse\n",
    "            performance_y_test[lead][name][\"R2\"] = m_r2\n",
    "            print (m_mae,m_rmse,m_r2)\n",
    "        elif name == 'SARIMAX':\n",
    "            prediction = list()\n",
    "            for index, t in enumerate(X_test):\n",
    "                total_index = TRAIN_TEST+index\n",
    "                history = X[(total_index-3*YEAR_DAYS):total_index,-1]                 \n",
    "                print (history.shape)\n",
    "                clf = model(history, order=(1,1,0), seasonal_order=(1, 1, 0, YEAR_DAYS))\n",
    "                clf_fit = clf.fit()\n",
    "                yhat = clf_fit.forecast(steps=lead)[-1]\n",
    "                prediction.append(yhat)\n",
    "            m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "            performance_y_test[lead][name][\"MAE\"] = m_mae\n",
    "            performance_y_test[lead][name][\"RMSE\"] = m_rmse\n",
    "            performance_y_test[lead][name][\"R2\"] = m_r2\n",
    "            print (m_mae,m_rmse,m_r2)\n",
    "        else:\n",
    "            for train, validation in cv.split(X_train, y_train):\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=20, mode='auto')\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4)    \n",
    "                callbacks = [early_stopping,reduce_lr]\n",
    "                clf = model()\n",
    "                history = clf.fit(X_train[train],y_train[train],\n",
    "                            epochs=NUM_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_train[validation],y_train[validation]),               \n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1)\n",
    "                prediction = clf.predict(X_test,verbose=0)\n",
    "                m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "                mae +=m_mae\n",
    "                rmse +=m_rmse\n",
    "                r2 +=m_r2\n",
    "            performance_y_test[lead][name][\"MAE\"] = mae/n_splits\n",
    "            performance_y_test[lead][name][\"RMSE\"] = rmse/n_splits\n",
    "            performance_y_test[lead][name][\"R2\"] = r2/n_splits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame.from_dict({(i,j): performance_y_test[i][j] \n",
    "                           for i in performance_y_test.keys() \n",
    "                           for j in performance_y_test[i].keys()},\n",
    "                       orient='index')\n",
    "df_result.index = df_result.index.set_names(['Lead','Model'])\n",
    "df_result.to_csv(PATH_RESULT+'/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lead Time</th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2701.884346</td>\n",
       "      <td>0.914794</td>\n",
       "      <td>4404.546378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>2907.660568</td>\n",
       "      <td>0.894906</td>\n",
       "      <td>4892.254640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3150.898134</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>5124.110765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3569.834829</td>\n",
       "      <td>0.849574</td>\n",
       "      <td>5843.150588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>CASTLE</td>\n",
       "      <td>2435.000000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>3567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>RNN</td>\n",
       "      <td>2417.655039</td>\n",
       "      <td>0.927982</td>\n",
       "      <td>4049.808209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3721.454978</td>\n",
       "      <td>0.844290</td>\n",
       "      <td>5931.832537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>4298.941025</td>\n",
       "      <td>0.773887</td>\n",
       "      <td>7148.259285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>CNN</td>\n",
       "      <td>4175.108759</td>\n",
       "      <td>0.806314</td>\n",
       "      <td>6615.654289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>4810.258784</td>\n",
       "      <td>0.747491</td>\n",
       "      <td>7490.662851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>CASTLE</td>\n",
       "      <td>3028.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>4389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>RNN</td>\n",
       "      <td>3523.188519</td>\n",
       "      <td>0.857708</td>\n",
       "      <td>5670.353989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5140.832692</td>\n",
       "      <td>0.718258</td>\n",
       "      <td>7921.027170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>6192.628560</td>\n",
       "      <td>0.522447</td>\n",
       "      <td>10312.562536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>CNN</td>\n",
       "      <td>5455.382293</td>\n",
       "      <td>0.685068</td>\n",
       "      <td>8374.505501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>6352.202501</td>\n",
       "      <td>0.585776</td>\n",
       "      <td>9592.290642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>CASTLE</td>\n",
       "      <td>3580.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>5367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>RNN</td>\n",
       "      <td>4921.466528</td>\n",
       "      <td>0.738032</td>\n",
       "      <td>7637.932898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lead Time   Model          MAE        R2          RMSE\n",
       "0           5     ANN  2701.884346  0.914794   4404.546378\n",
       "1           5   ARIMA  2907.660568  0.894906   4892.254640\n",
       "2           5     CNN  3150.898134  0.884700   5124.110765\n",
       "3           5    LSTM  3569.834829  0.849574   5843.150588\n",
       "4           5  CASTLE  2435.000000  0.940000   3567.000000\n",
       "5           5     RNN  2417.655039  0.927982   4049.808209\n",
       "7           7     ANN  3721.454978  0.844290   5931.832537\n",
       "8           7   ARIMA  4298.941025  0.773887   7148.259285\n",
       "9           7     CNN  4175.108759  0.806314   6615.654289\n",
       "10          7    LSTM  4810.258784  0.747491   7490.662851\n",
       "11          7  CASTLE  3028.000000  0.910000   4389.000000\n",
       "12          7     RNN  3523.188519  0.857708   5670.353989\n",
       "14         10     ANN  5140.832692  0.718258   7921.027170\n",
       "15         10   ARIMA  6192.628560  0.522447  10312.562536\n",
       "16         10     CNN  5455.382293  0.685068   8374.505501\n",
       "17         10    LSTM  6352.202501  0.585776   9592.290642\n",
       "18         10  CASTLE  3580.000000  0.871000   5367.000000\n",
       "19         10     RNN  4921.466528  0.738032   7637.932898"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = pd.read_csv(PATH_RESULT+'/results1.csv')\n",
    "df_result = df_result[df_result['Model'] != \"SARIMAX\"]\n",
    "df_result.loc[(df_result.Model == 'MORN'),'Model'] = \"CASTLE\"\n",
    "df_result.rename(columns={\"Lead\": \"Lead Time\"}, inplace=True)\n",
    "display(df_result.head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Lead Time   Model Metrics        Values\n",
      "0           5     ANN     MAE   2701.884346\n",
      "1           5   ARIMA     MAE   2907.660568\n",
      "2           5     CNN     MAE   3150.898134\n",
      "3           5    LSTM     MAE   3569.834829\n",
      "4           5  CASTLE     MAE   2435.000000\n",
      "5           5     RNN     MAE   2417.655039\n",
      "6           7     ANN     MAE   3721.454978\n",
      "7           7   ARIMA     MAE   4298.941025\n",
      "8           7     CNN     MAE   4175.108759\n",
      "9           7    LSTM     MAE   4810.258784\n",
      "10          7  CASTLE     MAE   3028.000000\n",
      "11          7     RNN     MAE   3523.188519\n",
      "12         10     ANN     MAE   5140.832692\n",
      "13         10   ARIMA     MAE   6192.628560\n",
      "14         10     CNN     MAE   5455.382293\n",
      "15         10    LSTM     MAE   6352.202501\n",
      "16         10  CASTLE     MAE   3580.000000\n",
      "17         10     RNN     MAE   4921.466528\n",
      "18          5     ANN      R2      0.914794\n",
      "19          5   ARIMA      R2      0.894906\n",
      "20          5     CNN      R2      0.884700\n",
      "21          5    LSTM      R2      0.849574\n",
      "22          5  CASTLE      R2      0.940000\n",
      "23          5     RNN      R2      0.927982\n",
      "24          7     ANN      R2      0.844290\n",
      "25          7   ARIMA      R2      0.773887\n",
      "26          7     CNN      R2      0.806314\n",
      "27          7    LSTM      R2      0.747491\n",
      "28          7  CASTLE      R2      0.910000\n",
      "29          7     RNN      R2      0.857708\n",
      "30         10     ANN      R2      0.718258\n",
      "31         10   ARIMA      R2      0.522447\n",
      "32         10     CNN      R2      0.685068\n",
      "33         10    LSTM      R2      0.585776\n",
      "34         10  CASTLE      R2      0.871000\n",
      "35         10     RNN      R2      0.738032\n",
      "36          5     ANN    RMSE   4404.546378\n",
      "37          5   ARIMA    RMSE   4892.254640\n",
      "38          5     CNN    RMSE   5124.110765\n",
      "39          5    LSTM    RMSE   5843.150588\n",
      "40          5  CASTLE    RMSE   3567.000000\n",
      "41          5     RNN    RMSE   4049.808209\n",
      "42          7     ANN    RMSE   5931.832537\n",
      "43          7   ARIMA    RMSE   7148.259285\n",
      "44          7     CNN    RMSE   6615.654289\n",
      "45          7    LSTM    RMSE   7490.662851\n",
      "46          7  CASTLE    RMSE   4389.000000\n",
      "47          7     RNN    RMSE   5670.353989\n",
      "48         10     ANN    RMSE   7921.027170\n",
      "49         10   ARIMA    RMSE  10312.562536\n",
      "50         10     CNN    RMSE   8374.505501\n",
      "51         10    LSTM    RMSE   9592.290642\n",
      "52         10  CASTLE    RMSE   5367.000000\n",
      "53         10     RNN    RMSE   7637.932898\n"
     ]
    }
   ],
   "source": [
    "melted_df_metrics = df_result.melt(id_vars=[\"Lead Time\", \"Model\"], value_vars=[\"MAE\", \"R2\", \"RMSE\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:645: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n",
      "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:645: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n",
      "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:645: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fe91c73b208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = 6\n",
    "g = sns.FacetGrid(df_result, col=\"Lead Time\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"MAE\")\n",
    "g = sns.FacetGrid(df_result, col=\"Lead Time\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"RMSE\")\n",
    "g = sns.FacetGrid(df_result, col=\"Lead Time\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:645: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n",
      "/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:650: UserWarning: Using the barplot function without specifying `hue_order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1504.12x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the style and font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Set the height\n",
    "height = 4  # adjust this as needed\n",
    "\n",
    "# Create the FacetGrid and plot for MAE\n",
    "g = sns.FacetGrid(melted_df_metrics, col=\"Metric\", margin_titles=True, height=height,aspect=1.5,  sharey=False)\n",
    "g.map(sns.barplot, \"Model\", \"Value\", \"Lead Time\", palette=\"Set1\")\n",
    "# Adjust column titles font size\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title(), fontsize=14)\n",
    "\n",
    "# Adjust x and y labels font size\n",
    "g.set_axis_labels(x_var=\"Model\", y_var=\"Performance\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n",
    "g.add_legend(title=\"Lead Time(days)\", bbox_to_anchor=(0.1, 0.9), loc=\"lower center\", ncol=3)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Set the height\n",
    "height = 4  # adjust this as needed\n",
    "\n",
    "# Create the FacetGrid and plot for Metrics\n",
    "g = sns.FacetGrid(melted_df_metrics, col=\"Metrics\", margin_titles=True, height=height, sharey=False)\n",
    "g.map(sns.barplot, \"Model\", \"Values\", hue=\"Lead Time\", palette=\"Set1\")\n",
    "\n",
    "# Adjust column titles font size\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_title(ax.get_title(), fontsize=14)\n",
    "\n",
    "# Adjust x and y labels font size\n",
    "g.set_axis_labels(x_var=\"Model\", y_var=\"Performance\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-acc8638374d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MAE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MAE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RMSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RMSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"R2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"R2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, bar_kws)\u001b[0m\n\u001b[1;32m   1637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;34m\"\"\"Make the plot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mdraw_bars\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m   1596\u001b[0m         \u001b[0;34m\"\"\"Draw the bars onto `ax`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;31m# Get the right matplotlib function depending on the orientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m         \u001b[0mbarfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"v\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1599\u001b[0m         \u001b[0mbarpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'bar'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# Get unique lead times\n",
    "unique_lead_times = df_result[\"Lead Time\"].unique()\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(3, len(unique_lead_times), figsize=(15, 5), sharey=True)\n",
    "\n",
    "for i, lead_time in enumerate(unique_lead_times):\n",
    "    data_subset = df_result[df_result[\"Lead Time\"] == lead_time]\n",
    "    \n",
    "    ax = axes[i]\n",
    "    sns.barplot(data=data_subset, x=\"Model\", y=\"MAE\", ax=ax, label=\"MAE\")\n",
    "    sns.barplot(data=data_subset, x=\"Model\", y=\"RMSE\", ax=ax, label=\"RMSE\")\n",
    "    sns.barplot(data=data_subset, x=\"Model\", y=\"R2\", ax=ax, label=\"R2\")\n",
    "    \n",
    "    ax.set_title(f\"Lead Time: {lead_time}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    ax.set_ylabel(\"Score\", fontsize=12)\n",
    "    ax.set_xticklabels(data_subset[\"Model\"], rotation=45, ha=\"right\")\n",
    "    ax.legend(title=\"Metric\", loc=\"upper left\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
