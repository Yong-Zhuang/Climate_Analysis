{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and figure setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import set_random_seed, get_seed\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace import sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Conv1D,Flatten, MaxPooling1D, InputLayer\n",
    "from keras import initializers, regularizers\n",
    "from keras.callbacks import EarlyStopping, Callback, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_info_columns', 1000)\n",
    "pd.set_option('display.max_colwidth', 10000000000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 40, 10\n",
    "rcParams['figure.subplot.wspace'] = 0.05\n",
    "rcParams['figure.subplot.hspace'] = 0.2\n",
    "rcParams['axes.titlesize'] = 'x-large'\n",
    "rcParams['axes.titleweight'] = 'bold'\n",
    "rcParams['axes.labelsize'] = 'x-large'\n",
    "rcParams['axes.labelweight'] = 'bold'\n",
    "rcParams['axes.titleweight'] = 'bold'\n",
    "rcParams[\"legend.loc\"] = 'upper left'\n",
    "rcParams[\"legend.markerscale\"] = 2\n",
    "rcParams[\"xtick.labelsize\"] = 'x-large'\n",
    "rcParams[\"ytick.labelsize\"] = 'x-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "LOOK_BACK = 15\n",
    "LEAD = [5,7,10]\n",
    "TRAIN_TEST = 3825\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 50\n",
    "YEAR_DAYS = 153\n",
    "PATH_RESULT = 'results/result'\n",
    "PATH_LOG = 'results/log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # NumPy\n",
    "random.seed(3) # Python\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 2 3 1 0 0]\n",
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "original = [0, 1, 0, 0, 1, 1, 0, 0]\n",
    "impulse_response = [2, 1]\n",
    "recorded = signal.convolve(impulse_response, original)\n",
    "print (recorded)\n",
    "recovered, remainder = signal.deconvolve([1,2], impulse_response)\n",
    "print (recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_samples():\n",
    "    ganges = pd.read_csv('../Data/Ganges.csv')\n",
    "    dates = (ganges.Year > 1984) & (ganges.Year<2017)\n",
    "    ganges = ganges[dates]\n",
    "    ganges = ganges.reset_index()\n",
    "    for lag in np.arange(-20,16):\n",
    "        new_index = ganges.index + lag\n",
    "        x = ganges.loc[new_index, 'Q (m3/s)' ]\n",
    "        x = pd.DataFrame(x)\n",
    "        x.columns = [''.join(['Q_',str(lag)])]   \n",
    "        x.index = ganges.index\n",
    "        ganges = pd.concat([ganges,x],axis=1)\n",
    "    ganges = ganges[ (ganges.Month > 4) & (ganges.Month<10)]\n",
    "\n",
    "    columns = ['Date',]\n",
    "    for lag in np.arange(1,16):\n",
    "        columns.append(''.join(['Q_',str(lag)]))\n",
    "\n",
    "    Y_Ganges = ganges[columns]\n",
    "\n",
    "    columns = ['Date']\n",
    "    for lag in np.arange(-20,1):\n",
    "        columns.append(''.join(['Q_',str(lag)]))    \n",
    "    X_Ganges = ganges[columns]\n",
    "\n",
    "    X_Ganges.to_csv(''.join([dirname,'X_Ganges.csv']))\n",
    "    Y_Ganges.to_csv(''.join([dirname,'Y_Ganges.csv']))\n",
    "def load_data(river='g'): #g:Ganges; b: Brahmaputra; m: Meghna;\n",
    "    #1934-4-01  2018-07-09\n",
    "    if river =='g':\n",
    "        river_name = 'Ganges'\n",
    "    elif river =='b':\n",
    "        river_name = 'Brahmaputra'\n",
    "    else:\n",
    "        river_name = 'Meghna'\n",
    "    Qx = pd.read_csv('../../data/streamflw_precipitation/X_'+river_name+'.csv', index_col=1,header=0,parse_dates=True)\n",
    "    X = Qx.iloc[:, -LOOK_BACK:]\n",
    "    Qy = pd.read_csv('../../data/streamflw_precipitation/Y_'+river_name+'.csv', index_col=1,header=0,parse_dates=True)\n",
    "    #print (Qy.head())\n",
    "    idy = []\n",
    "    for i in LEAD:\n",
    "        idy.append('Q_'+str(i))\n",
    "    y = Qy.loc[:,idy]\n",
    "    display(X.head())\n",
    "    display(y.head())\n",
    "    return X, y\n",
    "\n",
    "def initialization():\n",
    "    if os.path.isdir(PATH_RESULT) is False:\n",
    "        os.mkdir(PATH_RESULT)\n",
    "    if os.path.isdir(PATH_LOG) is False:\n",
    "        os.mkdir(PATH_LOG)\n",
    "\n",
    "def get_metrics(y, pred):\n",
    "    m_mae = metrics.mean_absolute_error(y, pred)\n",
    "    m_rmse = metrics.mean_squared_error(y, pred)** 0.5\n",
    "    m_r2 = metrics.r2_score(y, pred) \n",
    "    return m_mae,m_rmse,m_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_-14</th>\n",
       "      <th>Q_-13</th>\n",
       "      <th>Q_-12</th>\n",
       "      <th>Q_-11</th>\n",
       "      <th>Q_-10</th>\n",
       "      <th>Q_-9</th>\n",
       "      <th>Q_-8</th>\n",
       "      <th>Q_-7</th>\n",
       "      <th>Q_-6</th>\n",
       "      <th>Q_-5</th>\n",
       "      <th>Q_-4</th>\n",
       "      <th>Q_-3</th>\n",
       "      <th>Q_-2</th>\n",
       "      <th>Q_-1</th>\n",
       "      <th>Q_0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-05-01</th>\n",
       "      <td>824.00</td>\n",
       "      <td>814.00</td>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-02</th>\n",
       "      <td>814.00</td>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-03</th>\n",
       "      <td>815.00</td>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-04</th>\n",
       "      <td>803.00</td>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "      <td>934.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-05</th>\n",
       "      <td>793.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>805.00</td>\n",
       "      <td>813.00</td>\n",
       "      <td>824.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>852.00</td>\n",
       "      <td>810.00</td>\n",
       "      <td>756.00</td>\n",
       "      <td>751.00</td>\n",
       "      <td>830.00</td>\n",
       "      <td>903.00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>952.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Q_-14  Q_-13  Q_-12  Q_-11  Q_-10   Q_-9   Q_-8   Q_-7   Q_-6  \\\n",
       "Date                                                                        \n",
       "1985-05-01 824.00 814.00 815.00 803.00 793.00 800.00 805.00 813.00 824.00   \n",
       "1985-05-02 814.00 815.00 803.00 793.00 800.00 805.00 813.00 824.00 829.00   \n",
       "1985-05-03 815.00 803.00 793.00 800.00 805.00 813.00 824.00 829.00 829.00   \n",
       "1985-05-04 803.00 793.00 800.00 805.00 813.00 824.00 829.00 829.00 852.00   \n",
       "1985-05-05 793.00 800.00 805.00 813.00 824.00 829.00 829.00 852.00 810.00   \n",
       "\n",
       "             Q_-5   Q_-4   Q_-3   Q_-2   Q_-1    Q_0  \n",
       "Date                                                  \n",
       "1985-05-01 829.00 829.00 852.00 810.00 756.00 751.00  \n",
       "1985-05-02 829.00 852.00 810.00 756.00 751.00 830.00  \n",
       "1985-05-03 852.00 810.00 756.00 751.00 830.00 903.00  \n",
       "1985-05-04 810.00 756.00 751.00 830.00 903.00 934.00  \n",
       "1985-05-05 756.00 751.00 830.00 903.00 934.00 952.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q_5</th>\n",
       "      <th>Q_7</th>\n",
       "      <th>Q_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-05-01</th>\n",
       "      <td>958.00</td>\n",
       "      <td>918.00</td>\n",
       "      <td>922.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-02</th>\n",
       "      <td>968.00</td>\n",
       "      <td>783.00</td>\n",
       "      <td>893.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-03</th>\n",
       "      <td>918.00</td>\n",
       "      <td>838.00</td>\n",
       "      <td>874.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-04</th>\n",
       "      <td>783.00</td>\n",
       "      <td>922.00</td>\n",
       "      <td>879.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-05-05</th>\n",
       "      <td>838.00</td>\n",
       "      <td>893.00</td>\n",
       "      <td>861.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q_5    Q_7   Q_10\n",
       "Date                           \n",
       "1985-05-01 958.00 918.00 922.00\n",
       "1985-05-02 968.00 783.00 893.00\n",
       "1985-05-03 918.00 838.00 874.00\n",
       "1985-05-04 783.00 922.00 879.00\n",
       "1985-05-05 838.00 893.00 861.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_X, data_y = load_data('g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann():\n",
    "    ann = Sequential()\n",
    "    ann.add(InputLayer((LOOK_BACK, 1)))\n",
    "    ann.add(Flatten())\n",
    "    ann.add(Dense(100, activation='relu'))\n",
    "    ann.add(Dense(50, activation='relu'))\n",
    "    ann.add(Dense(1))\n",
    "    ann.compile(optimizer='adam', loss='mse')\n",
    "    ann.summary()\n",
    "    return ann\n",
    "\n",
    "def build_cnn():\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    cnn.add(MaxPooling1D(pool_size=2))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(100, activation='relu'))\n",
    "    cnn.add(Dense(1))\n",
    "    cnn.compile(optimizer='adam', loss='mse')\n",
    "    cnn.summary()\n",
    "    return cnn\n",
    "def build_rnn():\n",
    "    rnn = Sequential()\n",
    "    rnn.add(SimpleRNN(50, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    rnn.add(Dense(100, activation='relu'))\n",
    "    rnn.add(Dense(1))\n",
    "    rnn.compile(optimizer='adam', loss='mse')\n",
    "    rnn.summary()\n",
    "    return rnn\n",
    "def build_lstm():\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(50, activation='relu', input_shape=(LOOK_BACK, 1)))\n",
    "    lstm.add(Dense(100, activation='relu'))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(optimizer='adam', loss='mse')\n",
    "    lstm.summary()\n",
    "    return lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 5 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1341: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out_full[ind] += zi\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out = out_full[ind]\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/signal/signaltools.py:1350: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  zf = out_full[ind]\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/kalmanf/kalmanfilter.py:649: RuntimeWarning: divide by zero encountered in divide\n",
      "  R_mat, T_mat)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:606: RuntimeWarning: overflow encountered in exp\n",
      "  newparams = ((1-np.exp(-params))/\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:607: RuntimeWarning: overflow encountered in exp\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:607: RuntimeWarning: invalid value encountered in divide\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:608: RuntimeWarning: overflow encountered in exp\n",
      "  tmp = ((1-np.exp(-params))/\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:609: RuntimeWarning: overflow encountered in exp\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/tsa/tsatools.py:609: RuntimeWarning: invalid value encountered in divide\n",
      "  (1+np.exp(-params))).copy()\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2907.660567646018, 4892.254639918654, 0.8949061242718296)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 152us/step - loss: 93892786.5686 - val_loss: 49083719.3333\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 47680731.5098 - val_loss: 41553499.0196\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 43514948.4314 - val_loss: 40401708.4706\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 39807087.0588 - val_loss: 38885143.6078\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 38462405.8039 - val_loss: 35500718.8039\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 37684494.8627 - val_loss: 33738885.1569\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 35394002.4118 - val_loss: 36884714.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 34092730.0784 - val_loss: 37953228.3529\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 35494491.7353 - val_loss: 30744695.6863\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 31867926.5882 - val_loss: 30371349.6863\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 32991187.5294 - val_loss: 29610792.3725\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 30986127.1176 - val_loss: 29355224.3725\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 30544525.5490 - val_loss: 29371800.2353\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 29098685.4118 - val_loss: 28429402.2549\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 29274626.1569 - val_loss: 27746516.6078\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 28376591.4902 - val_loss: 32158698.3333\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 30769881.9216 - val_loss: 31337699.3922\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 32310047.2941 - val_loss: 31648359.6471\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 29195937.4118 - val_loss: 31658256.9020\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28208878.5098 - val_loss: 28466031.2941\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 27027072.2941 - val_loss: 26983944.2941\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 26637760.3725 - val_loss: 26929327.3725\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 26606213.6667 - val_loss: 27157053.0588\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26597062.3922 - val_loss: 26899191.2157\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 26501125.9020 - val_loss: 27233178.4902\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 26554328.0784 - val_loss: 26931229.5882\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 27us/step - loss: 26314383.3725 - val_loss: 27081707.3333\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26589494.4510 - val_loss: 26787771.9020\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 26371275.5490 - val_loss: 26797295.9608\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26322974.0588 - val_loss: 26761860.3725\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26296928.5294 - val_loss: 27107920.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26718618.1569 - val_loss: 26802472.6667\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26230774.1176 - val_loss: 26642742.1373\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26665186.4118 - val_loss: 26857373.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 26293608.9804 - val_loss: 26847182.4118\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 26165151.0000 - val_loss: 26643495.7255\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26340792.5882 - val_loss: 27171678.5686\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26206081.1373 - val_loss: 26706090.0784\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25984745.7843 - val_loss: 26518956.7059\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25923911.2941 - val_loss: 26539909.3529\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25894429.8627 - val_loss: 26517164.6471\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25892622.4510 - val_loss: 26536433.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 25901273.1765 - val_loss: 26516339.9216\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25882777.7647 - val_loss: 26518779.2941\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25884427.8333 - val_loss: 26536823.9608\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25891208.8039 - val_loss: 26523700.1961\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25878771.7451 - val_loss: 26509996.5098\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25854853.6275 - val_loss: 26505809.7059\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25902706.9020 - val_loss: 26521245.2549\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25865475.8039 - val_loss: 26530064.0980\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25877409.6078 - val_loss: 26500765.5882\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25842797.0588 - val_loss: 26494871.8039\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25870502.4510 - val_loss: 26492756.4510\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25840951.5686 - val_loss: 26492532.2157\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25856152.1961 - val_loss: 26500683.1961\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25840211.2745 - val_loss: 26491935.3333\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25876204.7059 - val_loss: 26495394.7843\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25843834.1765 - val_loss: 26487394.8235\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25830703.3333 - val_loss: 26481321.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25845190.9608 - val_loss: 26489341.8235\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25810050.5294 - val_loss: 26479905.2353\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25806737.7647 - val_loss: 26474288.3529\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25810150.6471 - val_loss: 26483884.4902\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25822737.4118 - val_loss: 26496339.1961\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25802467.1569 - val_loss: 26470040.9216\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25809947.1373 - val_loss: 26466939.5294\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25801348.8431 - val_loss: 26479908.0784\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25797207.1569 - val_loss: 26465536.1569\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25798658.5490 - val_loss: 26465266.4510\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25813152.3137 - val_loss: 26456787.8039\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25792397.1569 - val_loss: 26462089.9216\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25771485.1569 - val_loss: 26451971.0000\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25780081.6078 - val_loss: 26454265.8431\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25779980.6078 - val_loss: 26454936.7647\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25808563.0392 - val_loss: 26464452.4706\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25779977.5294 - val_loss: 26445052.0980\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25786732.4706 - val_loss: 26445250.1176\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25806727.5490 - val_loss: 26442423.6667\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25785659.6667 - val_loss: 26436435.4118\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25803053.4510 - val_loss: 26433024.2157\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25811260.5490 - val_loss: 26431714.2549\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25791133.4902 - val_loss: 26429385.6275\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25784913.1961 - val_loss: 26438202.9608\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 25774315.7451 - val_loss: 26421062.1961\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 25725960.3529 - val_loss: 26421842.1765\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 25us/step - loss: 25736264.3725 - val_loss: 26427478.7647\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25752254.0980 - val_loss: 26439717.8431\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25765213.2941 - val_loss: 26412208.6863\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25742535.2745 - val_loss: 26413162.9608\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25797232.9608 - val_loss: 26474068.2549\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25726975.3922 - val_loss: 26403549.5294\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25728117.6471 - val_loss: 26417133.2941\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25714150.9020 - val_loss: 26400461.4510\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25714411.8431 - val_loss: 26389701.2745\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25704808.2353 - val_loss: 26390446.4510\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25735060.2353 - val_loss: 26388605.8235\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25702030.3137 - val_loss: 26422187.6863\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25694997.2157 - val_loss: 26387962.7843\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 27us/step - loss: 25695685.7255 - val_loss: 26385308.7451\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25687813.8235 - val_loss: 26388196.5686\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25692825.5882 - val_loss: 26380028.7255\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25690613.6078 - val_loss: 26387935.6078\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25724778.7843 - val_loss: 26397523.5098\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25690715.8627 - val_loss: 26365480.0392\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25696784.6471 - val_loss: 26366546.1373\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25690033.6078 - val_loss: 26394412.0392\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25724834.7451 - val_loss: 26389441.9020\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25661560.9804 - val_loss: 26360510.2549\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25660258.6667 - val_loss: 26353560.3333\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25671408.0980 - val_loss: 26359812.8235\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25658880.4216 - val_loss: 26354793.9412\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25648020.4314 - val_loss: 26360784.6078\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25654357.9804 - val_loss: 26350709.8431\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25646822.6078 - val_loss: 26351317.8235\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25652326.0000 - val_loss: 26360808.4118\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25638363.6275 - val_loss: 26339549.7843\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25666498.2353 - val_loss: 26347835.9020\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25643912.4706 - val_loss: 26338330.1961\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25668355.3333 - val_loss: 26369759.1765\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25629628.9804 - val_loss: 26364703.4118\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25634915.3137 - val_loss: 26336387.2745\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25681237.5686 - val_loss: 26349917.5294\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25672662.3137 - val_loss: 26327109.0980\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25630503.6275 - val_loss: 26328092.3922\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25629006.3333 - val_loss: 26348630.5686\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25630546.7451 - val_loss: 26316139.7059\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25633215.1176 - val_loss: 26348138.4314\n",
      "Epoch 128/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 38us/step - loss: 25656724.8235 - val_loss: 26321653.1373\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25593307.1569 - val_loss: 26336891.2353\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25643034.7647 - val_loss: 26307951.7843\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25590267.5882 - val_loss: 26318507.5490\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25633674.1961 - val_loss: 26315495.7647\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25615743.4902 - val_loss: 26301898.7647\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25606563.6667 - val_loss: 26300349.6863\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25581068.1078 - val_loss: 26300653.8235\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25588011.7255 - val_loss: 26296084.2745\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25587831.4314 - val_loss: 26305654.7843\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25585180.4902 - val_loss: 26288175.9804\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25592703.5294 - val_loss: 26306660.0980\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25568389.0588 - val_loss: 26307566.8824\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25589480.2353 - val_loss: 26276217.0196\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 26us/step - loss: 25631471.0784 - val_loss: 26295337.3529\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 22us/step - loss: 25560010.7647 - val_loss: 26277172.0784\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25616816.8431 - val_loss: 26295456.2549\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25540858.4706 - val_loss: 26266840.7647\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25593666.2157 - val_loss: 26277435.0980\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25585860.6471 - val_loss: 26266385.3922\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25584942.3922 - val_loss: 26321706.8824\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25596169.6471 - val_loss: 26264352.8627\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25542082.5490 - val_loss: 26313875.8235\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25590239.4510 - val_loss: 26258735.2157\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25569523.8725 - val_loss: 26256447.1765\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25544350.8824 - val_loss: 26281367.3529\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25575748.7059 - val_loss: 26292481.0980\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25569512.0588 - val_loss: 26248491.6667\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25552509.4902 - val_loss: 26261163.9020\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25551840.5882 - val_loss: 26242138.4314\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25551885.9216 - val_loss: 26306259.8039\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25545783.9020 - val_loss: 26239862.2745\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25525784.3529 - val_loss: 26228196.7059\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25523667.9608 - val_loss: 26235653.0000\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25517237.4706 - val_loss: 26232716.9216\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25504528.9020 - val_loss: 26227851.8235\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25494571.1176 - val_loss: 26229655.3922\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25503577.1961 - val_loss: 26224284.6471\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25509072.6275 - val_loss: 26224819.7843\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25514654.6275 - val_loss: 26230129.3333\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25487270.4706 - val_loss: 26229974.9020\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25520142.0784 - val_loss: 26217401.2549\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25486724.0392 - val_loss: 26217572.2941\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25474241.8235 - val_loss: 26210721.0392\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25503981.5294 - val_loss: 26215448.8235\n",
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25510853.3725 - val_loss: 26217471.0392\n",
      "Epoch 174/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25513216.2549 - val_loss: 26210248.9608\n",
      "Epoch 175/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25492484.5490 - val_loss: 26193913.1373\n",
      "Epoch 176/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 25468315.7843 - val_loss: 26209428.1765\n",
      "Epoch 177/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25482057.0196 - val_loss: 26195454.5686\n",
      "Epoch 178/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25524726.6471 - val_loss: 26222345.8627\n",
      "Epoch 179/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25465555.2451 - val_loss: 26196587.3922\n",
      "Epoch 180/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25520042.5294 - val_loss: 26279965.5098\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 181/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25485771.2941 - val_loss: 26249912.8627\n",
      "Epoch 182/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25461050.2941 - val_loss: 26220955.8627\n",
      "Epoch 183/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25442036.4902 - val_loss: 26209290.2941\n",
      "Epoch 184/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25439818.8824 - val_loss: 26203038.1765\n",
      "Epoch 185/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25431086.3333 - val_loss: 26194925.9216\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 186/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25426238.0784 - val_loss: 26194371.7451\n",
      "Epoch 187/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25426039.2059 - val_loss: 26194193.6863\n",
      "Epoch 188/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25426044.1176 - val_loss: 26193915.4706\n",
      "Epoch 189/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25426458.4510 - val_loss: 26193987.2549\n",
      "Epoch 190/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425824.1176 - val_loss: 26193312.5686\n",
      "Epoch 191/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25425448.5490 - val_loss: 26193269.1176\n",
      "Epoch 192/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425483.3333 - val_loss: 26192784.7843\n",
      "Epoch 193/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25425499.4314 - val_loss: 26192559.3922\n",
      "Epoch 194/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25426160.5490 - val_loss: 26192078.3333\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 35us/step - loss: 25426027.4902 - val_loss: 26192241.1176\n",
      "Epoch 196/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25424993.4902 - val_loss: 26192053.8235\n",
      "Epoch 197/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25425101.2353 - val_loss: 26191973.9608\n",
      "Epoch 198/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25424643.8431 - val_loss: 26191939.5490\n",
      "Epoch 199/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424803.9020 - val_loss: 26191481.8824\n",
      "Epoch 200/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424733.0000 - val_loss: 26191629.3725\n",
      "Epoch 201/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25424589.6667 - val_loss: 26191369.9608\n",
      "Epoch 202/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25424330.6667 - val_loss: 26191317.9412\n",
      "Epoch 203/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25424329.4706 - val_loss: 26191265.4118\n",
      "Epoch 204/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424457.5490 - val_loss: 26190998.5294\n",
      "Epoch 205/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25424093.4510 - val_loss: 26190872.8627\n",
      "Epoch 206/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424418.1961 - val_loss: 26190579.2941\n",
      "Epoch 207/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25424001.0784 - val_loss: 26190684.6667\n",
      "Epoch 208/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25424065.5686 - val_loss: 26190495.8039\n",
      "Epoch 209/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25424310.4706 - val_loss: 26190561.2353\n",
      "Epoch 210/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423740.3725 - val_loss: 26190408.5686\n",
      "Epoch 211/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423709.4314 - val_loss: 26190168.9020\n",
      "Epoch 212/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25423924.4510 - val_loss: 26190051.4902\n",
      "Epoch 213/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25423988.5490 - val_loss: 26189930.6078\n",
      "Epoch 214/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25423685.1176 - val_loss: 26189786.6471\n",
      "Epoch 215/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423512.9706 - val_loss: 26189756.1373\n",
      "Epoch 216/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25423607.1961 - val_loss: 26189669.0000\n",
      "Epoch 217/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423723.9020 - val_loss: 26189386.2157\n",
      "Epoch 218/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423362.3137 - val_loss: 26189493.8235\n",
      "Epoch 219/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25423448.6863 - val_loss: 26189390.5686\n",
      "Epoch 220/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423242.9412 - val_loss: 26189386.2745\n",
      "Epoch 221/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25423453.4118 - val_loss: 26189103.6078\n",
      "Epoch 222/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25424145.0196 - val_loss: 26188917.8824\n",
      "Epoch 223/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423167.5686 - val_loss: 26189227.4902\n",
      "Epoch 224/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25423533.8627 - val_loss: 26189071.4118\n",
      "Epoch 225/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25423319.2549 - val_loss: 26188903.7255\n",
      "Epoch 226/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25423189.9020 - val_loss: 26188961.0392\n",
      "Epoch 227/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25423245.1961 - val_loss: 26188718.9804\n",
      "Epoch 228/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422956.6667 - val_loss: 26188799.7255\n",
      "Epoch 229/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423010.3922 - val_loss: 26188567.3137\n",
      "Epoch 230/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25423153.8039 - val_loss: 26188761.3725\n",
      "Epoch 231/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422896.5196 - val_loss: 26188661.6471\n",
      "Epoch 232/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25422948.7647 - val_loss: 26188625.9216\n",
      "Epoch 233/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25422754.3137 - val_loss: 26188554.5882\n",
      "Epoch 234/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25422891.8627 - val_loss: 26188414.0980\n",
      "Epoch 235/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25423249.8529 - val_loss: 26188563.9216\n",
      "Epoch 236/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25422586.3725 - val_loss: 26188403.1961\n",
      "Epoch 237/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25423008.3333 - val_loss: 26188332.0000\n",
      "Epoch 238/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422592.7843 - val_loss: 26188287.5882\n",
      "Epoch 239/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25423293.9216 - val_loss: 26188352.7255\n",
      "Epoch 240/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25422999.8824 - val_loss: 26187989.4510\n",
      "Epoch 241/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 25422556.7843 - val_loss: 26188002.3137\n",
      "Epoch 242/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25422435.9510 - val_loss: 26188027.0392\n",
      "Epoch 243/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25422482.6667 - val_loss: 26187888.9608\n",
      "Epoch 244/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25423289.3725 - val_loss: 26187885.9608\n",
      "Epoch 245/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25422645.1765 - val_loss: 26187970.4510\n",
      "Epoch 246/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422371.0000 - val_loss: 26187901.3922\n",
      "Epoch 247/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25422353.6176 - val_loss: 26187686.8235\n",
      "Epoch 248/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25422201.4020 - val_loss: 26187703.9216\n",
      "Epoch 249/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25422861.6863 - val_loss: 26187898.6275\n",
      "Epoch 250/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422344.8824 - val_loss: 26187733.7843\n",
      "Epoch 251/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422233.9020 - val_loss: 26187596.4706\n",
      "Epoch 252/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422387.7059 - val_loss: 26187470.9412\n",
      "Epoch 253/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422353.2353 - val_loss: 26187393.0000\n",
      "Epoch 254/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25422295.8824 - val_loss: 26187403.8431\n",
      "Epoch 255/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422277.1765 - val_loss: 26187281.1961\n",
      "Epoch 256/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422342.7647 - val_loss: 26187337.5098\n",
      "Epoch 257/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 25421856.9216 - val_loss: 26187293.9804\n",
      "Epoch 258/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422543.9412 - val_loss: 26187362.4118\n",
      "Epoch 259/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422729.9020 - val_loss: 26187215.8431\n",
      "Epoch 260/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25422245.4314 - val_loss: 26187318.7843\n",
      "Epoch 261/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421781.5686 - val_loss: 26187187.8627\n",
      "Epoch 262/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25422900.9216 - val_loss: 26186996.5098\n",
      "Epoch 263/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25421951.7843 - val_loss: 26187087.8431\n",
      "Epoch 264/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421949.9804 - val_loss: 26187019.5294\n",
      "Epoch 265/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25422121.8725 - val_loss: 26187022.0000\n",
      "Epoch 266/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25422058.4314 - val_loss: 26187111.9412\n",
      "Epoch 267/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421608.8235 - val_loss: 26186969.3529\n",
      "Epoch 268/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421764.7157 - val_loss: 26186942.2549\n",
      "Epoch 269/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421715.5882 - val_loss: 26186802.4706\n",
      "Epoch 270/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421724.7843 - val_loss: 26186855.3725\n",
      "Epoch 271/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25421714.4314 - val_loss: 26186785.4118\n",
      "Epoch 272/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421659.3725 - val_loss: 26186737.3333\n",
      "Epoch 273/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421608.6078 - val_loss: 26186698.1961\n",
      "Epoch 274/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421626.5000 - val_loss: 26186871.2549\n",
      "Epoch 275/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421766.7647 - val_loss: 26186797.7451\n",
      "Epoch 276/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421680.8235 - val_loss: 26186626.7451\n",
      "Epoch 277/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421381.1961 - val_loss: 26186568.0784\n",
      "Epoch 278/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421374.5490 - val_loss: 26186534.7451\n",
      "Epoch 279/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421530.5098 - val_loss: 26186596.2157\n",
      "Epoch 280/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421163.0000 - val_loss: 26186484.9804\n",
      "Epoch 281/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25421341.7059 - val_loss: 26186503.2941\n",
      "Epoch 282/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421760.3137 - val_loss: 26186617.0392\n",
      "Epoch 283/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421030.2549 - val_loss: 26186450.1373\n",
      "Epoch 284/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421309.3922 - val_loss: 26186386.1176\n",
      "Epoch 285/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421939.6471 - val_loss: 26186496.4706\n",
      "Epoch 286/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25421978.7451 - val_loss: 26186278.8039\n",
      "Epoch 287/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421062.3137 - val_loss: 26186285.8431\n",
      "Epoch 288/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25421240.7843 - val_loss: 26186328.8235\n",
      "Epoch 289/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25421251.8627 - val_loss: 26186176.2549\n",
      "Epoch 290/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25421727.7255 - val_loss: 26186139.1961\n",
      "Epoch 291/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25421368.4902 - val_loss: 26186099.9020\n",
      "Epoch 292/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 25421487.5000 - val_loss: 26186155.2353\n",
      "Epoch 293/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420826.8824 - val_loss: 26186097.3725\n",
      "Epoch 294/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25420889.4902 - val_loss: 26186045.3725\n",
      "Epoch 295/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25421057.1569 - val_loss: 26185963.4510\n",
      "Epoch 296/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25420923.3725 - val_loss: 26185928.1961\n",
      "Epoch 297/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420924.5686 - val_loss: 26185911.2353\n",
      "Epoch 298/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420845.9216 - val_loss: 26185901.4118\n",
      "Epoch 299/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25421769.6863 - val_loss: 26186086.5686\n",
      "Epoch 300/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25420516.6471 - val_loss: 26185847.1569\n",
      "Epoch 301/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25421263.9020 - val_loss: 26185970.1373\n",
      "Epoch 302/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420638.7647 - val_loss: 26185846.6471\n",
      "Epoch 303/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25420930.2745 - val_loss: 26185710.5882\n",
      "Epoch 304/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420836.7255 - val_loss: 26185761.2745\n",
      "Epoch 305/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420704.5882 - val_loss: 26185756.3725\n",
      "Epoch 306/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420642.6471 - val_loss: 26185793.8431\n",
      "Epoch 307/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25420560.1373 - val_loss: 26185530.1961\n",
      "Epoch 308/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420499.4118 - val_loss: 26185583.2745\n",
      "Epoch 309/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25420340.0000 - val_loss: 26185583.4902\n",
      "Epoch 310/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25420987.0980 - val_loss: 26185565.8824\n",
      "Epoch 311/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420524.9216 - val_loss: 26185613.9804\n",
      "Epoch 312/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25420687.5294 - val_loss: 26185409.9020\n",
      "Epoch 313/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420736.9804 - val_loss: 26185302.3333\n",
      "Epoch 314/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 25421065.1373 - val_loss: 26185608.7647\n",
      "Epoch 315/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25420454.1765 - val_loss: 26185427.2941\n",
      "Epoch 316/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 25420336.2157 - val_loss: 26185332.6078\n",
      "Epoch 317/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25421142.9608 - val_loss: 26185389.8431\n",
      "Epoch 318/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25420342.8235 - val_loss: 26185357.1373\n",
      "\n",
      "Epoch 00318: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 319/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25419875.9902 - val_loss: 26185356.6863\n",
      "Epoch 320/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25419902.4510 - val_loss: 26185351.2549\n",
      "Epoch 321/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 25419878.0588 - val_loss: 26185357.2549\n",
      "Epoch 322/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25419897.7451 - val_loss: 26185350.5490\n",
      "Epoch 323/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 25419878.4314 - val_loss: 26185349.3333\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 324/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 25419870.7451 - val_loss: 26185350.4706\n",
      "Epoch 325/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.1176 - val_loss: 26185350.2353\n",
      "Epoch 326/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 25419870.1176 - val_loss: 26185350.3725\n",
      "Epoch 327/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25419870.6667 - val_loss: 26185350.2549\n",
      "Epoch 328/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25419870.3137 - val_loss: 26185349.8039\n",
      "\n",
      "Epoch 00328: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.8039 - val_loss: 26185349.8039\n",
      "Epoch 330/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 25419870.7451 - val_loss: 26185349.8039\n",
      "Epoch 331/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 25419870.9608 - val_loss: 26185349.6471\n",
      "Epoch 332/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25419870.4118 - val_loss: 26185349.7647\n",
      "Epoch 333/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 25419870.2941 - val_loss: 26185349.7647\n",
      "\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 1.00000008274e-11.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 120544575.0196 - val_loss: 46537400.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 45517998.0784 - val_loss: 44576693.0980\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 41233442.1765 - val_loss: 37326838.8235\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 38820193.3529 - val_loss: 35634166.1569\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 37208767.4118 - val_loss: 38850934.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 35596801.0588 - val_loss: 33311926.0784\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 33663811.5294 - val_loss: 32328503.8824\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 32742940.2745 - val_loss: 32285271.5098\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 34567445.6471 - val_loss: 39390784.5882\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 33603220.0196 - val_loss: 30123633.8235\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 31309100.2941 - val_loss: 31207081.4118\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 32556031.5882 - val_loss: 29302807.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 32045132.1176 - val_loss: 32304040.1961\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 29590610.3922 - val_loss: 28690491.5882\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 29232220.9608 - val_loss: 30272915.0980\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 30135781.1765 - val_loss: 28783906.2353\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 30966215.6471 - val_loss: 28016813.7843\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28930208.2549 - val_loss: 28285920.2745\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 30254504.1373 - val_loss: 31089218.6667\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 31465016.3333 - val_loss: 30428420.2549\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 28601711.7059 - val_loss: 28855708.0000\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 31634053.6471 - val_loss: 26966624.6667\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 27983275.1569 - val_loss: 26541010.1176\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 27528148.6471 - val_loss: 34581467.0588\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 28877569.4902 - val_loss: 29056742.5882\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 28765944.7451 - val_loss: 26285034.1176\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 27742265.2549 - val_loss: 31963261.5294\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 28821930.3137 - val_loss: 26228787.5882\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 26925705.8235 - val_loss: 26163466.4706\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27971340.9216 - val_loss: 26621642.3922\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27402208.6275 - val_loss: 30465266.6471\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 26460298.7451 - val_loss: 25932493.6078\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 29787598.7843 - val_loss: 25397549.8039\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27512321.0392 - val_loss: 30556507.8431\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 28624302.3922 - val_loss: 30441015.5686\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26082519.9020 - val_loss: 25072037.5490\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 27574119.4118 - val_loss: 27666322.6863\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 26713316.0784 - val_loss: 27111568.7059\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 28338924.0588 - val_loss: 25630703.3529\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 28497190.8627 - val_loss: 27134779.7059\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 29880654.7647 - val_loss: 35070799.9608\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25828696.3529 - val_loss: 25656031.9216\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24758450.1373 - val_loss: 25053691.0196\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 24725144.0392 - val_loss: 25071203.7843\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24548551.2941 - val_loss: 25099698.3137\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24620061.9608 - val_loss: 25116285.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24542732.4902 - val_loss: 25946609.1961\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24584189.4118 - val_loss: 25083228.2157\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24335361.7647 - val_loss: 25043519.1765\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24323402.8039 - val_loss: 25065827.7059\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 29us/step - loss: 24311683.7843 - val_loss: 25029255.5882\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 24312188.3725 - val_loss: 25072113.0784\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24314488.5490 - val_loss: 25102830.1373\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 24305297.6667 - val_loss: 25067493.2941\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24309984.0980 - val_loss: 25050231.4118\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24314180.1569 - val_loss: 25031917.4902\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24285684.8824 - val_loss: 25037085.2353\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24287243.0980 - val_loss: 25025241.3725\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 24284117.1176 - val_loss: 25039447.7451\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24282863.0000 - val_loss: 25044486.0000\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24284050.3529 - val_loss: 25041139.4902\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24283976.9412 - val_loss: 25046077.5098\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24284771.4314 - val_loss: 25041660.1765\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24279470.6667 - val_loss: 25042460.4706\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24279536.1176 - val_loss: 25042623.1569\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279455.7647 - val_loss: 25043259.7255\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 24279445.5098 - val_loss: 25042417.8627\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279439.7255 - val_loss: 25043284.6667\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279144.7451 - val_loss: 25043285.0980\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24279135.2745 - val_loss: 25043276.9020\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24279128.1569 - val_loss: 25043279.3725\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279131.8627 - val_loss: 25043297.9804\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 24279128.0196 - val_loss: 25043301.7255\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24279120.2549 - val_loss: 25043302.0392\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24279120.6275 - val_loss: 25043300.9608\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24279120.4706 - val_loss: 25043302.5294\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24279120.1667 - val_loss: 25043302.2745\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 24279120.1569 - val_loss: 25043302.1569\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 65826311.9216 - val_loss: 47074178.1176\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 39830344.2353 - val_loss: 43613818.2745\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 38016291.5686 - val_loss: 37088118.7451\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 36815741.1569 - val_loss: 34504696.6863\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 35344467.0196 - val_loss: 39566928.4706\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 34466731.9608 - val_loss: 32690925.6863\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 31770075.2941 - val_loss: 30976415.4902\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 31316624.8039 - val_loss: 33849190.8627\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 32614712.3529 - val_loss: 33021039.3333\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 29377246.3137 - val_loss: 32825637.6078\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 32069478.8431 - val_loss: 29151170.5490\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 30058976.1961 - val_loss: 35337605.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 28171072.5686 - val_loss: 28861497.1961\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27609409.3725 - val_loss: 28108267.7451\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 27474984.0980 - val_loss: 29141887.6863\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 26842688.1569 - val_loss: 27453919.5882\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 27997702.4706 - val_loss: 28285493.1569\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 27973147.0784 - val_loss: 28935434.3725\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 30720476.0588 - val_loss: 40944016.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 27084622.5882 - val_loss: 26947928.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 28000554.8431 - val_loss: 30103337.0980\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 29572562.8431 - val_loss: 27377386.5098\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 29258425.6863 - val_loss: 28522151.0784\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 26804227.1176 - val_loss: 27295753.0196\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 26009668.2353 - val_loss: 32755472.3922\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25846565.7451 - val_loss: 26783962.6471\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 25457765.9608 - val_loss: 27573895.3333\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 25167025.8235 - val_loss: 26463732.1569\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24970371.1176 - val_loss: 26396109.2941\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24868424.6863 - val_loss: 26304594.9804\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 25111585.9412 - val_loss: 26515050.6863\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24973016.2549 - val_loss: 26532267.2549\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 24996420.6569 - val_loss: 26990568.3137\n",
      "Epoch 34/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 40us/step - loss: 25032524.7059 - val_loss: 26100072.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 25548655.3333 - val_loss: 26119802.1765\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 25075140.4706 - val_loss: 26657515.0392\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 25190916.6667 - val_loss: 26517141.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 24872277.5686 - val_loss: 26138300.5294\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24792706.3235 - val_loss: 26151555.1373\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 24666553.4510 - val_loss: 26337137.6667\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 24689450.1961 - val_loss: 26271354.2353\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24637654.9608 - val_loss: 26170484.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24642096.0980 - val_loss: 26243117.6275\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 24637666.2157 - val_loss: 26181644.0000\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24620708.8824 - val_loss: 26181838.9216\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24621101.5490 - val_loss: 26178688.8039\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 24622445.4706 - val_loss: 26187756.1765\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24621964.7059 - val_loss: 26180701.4314\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24620925.2157 - val_loss: 26189804.0980\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 24618395.7451 - val_loss: 26191223.8235\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 24618454.1373 - val_loss: 26191503.6078\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 24617762.2157 - val_loss: 26190873.1765\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 24617895.7843 - val_loss: 26189948.6863\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 24617698.2745 - val_loss: 26190494.7843\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 243us/step - loss: 93162582.1961 - val_loss: 52219979.6471\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 52366867.5686 - val_loss: 43531344.0784\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 43364544.9020 - val_loss: 35891263.2941\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 37137540.0000 - val_loss: 38599103.8431\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 35710214.6275 - val_loss: 33873025.1765\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 36704742.8824 - val_loss: 32032433.4314\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33987347.3922 - val_loss: 29654903.1569\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 30903245.2549 - val_loss: 29911226.7451\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 30339585.3725 - val_loss: 28505497.1569\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 30469382.7255 - val_loss: 28369478.7451\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 28918814.9020 - val_loss: 28077086.2157\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 29998911.2941 - val_loss: 28573383.9608\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 29583466.2549 - val_loss: 31179843.4314\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 28723108.7647 - val_loss: 30994227.0196\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 32349858.9020 - val_loss: 40296663.6078\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 29888783.9804 - val_loss: 33922150.7451\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 28362534.4902 - val_loss: 27031481.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 27344286.8824 - val_loss: 26900842.3333\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26990724.4314 - val_loss: 26838942.0196\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 27041716.3725 - val_loss: 26644403.7843\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 27129312.6863 - val_loss: 26696462.7843\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 26919980.9804 - val_loss: 26522661.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26858876.0588 - val_loss: 26813890.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26875542.7843 - val_loss: 27091620.2549\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 27001768.3235 - val_loss: 26435145.8039\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26928417.7941 - val_loss: 26407837.2549\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26791972.5686 - val_loss: 26400676.0392\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 27103226.1961 - val_loss: 26588791.4510\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26713049.5490 - val_loss: 26556805.4510\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26737770.0784 - val_loss: 26455830.1961\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27005964.6471 - val_loss: 26509732.1765\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26720136.1373 - val_loss: 26304904.8431\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26698572.7647 - val_loss: 26356809.1961\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26751212.3137 - val_loss: 26265809.8627\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 26734156.5098 - val_loss: 27393481.4314\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 26555007.7647 - val_loss: 26198288.4510\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 75us/step - loss: 26650113.2157 - val_loss: 26212170.5686\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26747355.1569 - val_loss: 26262710.0784\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26742194.9020 - val_loss: 26188332.6667\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 26532055.0588 - val_loss: 26308077.4902\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26351771.0882 - val_loss: 26522606.2157\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 26528997.6275 - val_loss: 26112812.8039\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 26609002.6667 - val_loss: 26086024.3922\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 26645498.9216 - val_loss: 26245784.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 26400337.6471 - val_loss: 26202825.2353\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 26473532.0980 - val_loss: 26007431.8627\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 26527851.2549 - val_loss: 26052001.4902\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26424215.6275 - val_loss: 25987807.0784\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26361284.4510 - val_loss: 25959310.1765\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26499003.6863 - val_loss: 26153111.3922\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 26712495.0196 - val_loss: 26266373.8627\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26351843.4510 - val_loss: 26900832.7647\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 26828695.0588 - val_loss: 25945881.4510\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26568616.1961 - val_loss: 26086135.7255\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26243200.9020 - val_loss: 25919311.0392\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26289221.7255 - val_loss: 25886160.7647\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26224088.9608 - val_loss: 25857599.7059\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 26162313.16 - 0s 78us/step - loss: 26107873.1961 - val_loss: 25838205.8235\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26369776.5882 - val_loss: 25983228.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 26337257.9314 - val_loss: 25818209.8039\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 26283633.9216 - val_loss: 26714433.2157\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 26312934.3922 - val_loss: 25753595.7451\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26248755.7451 - val_loss: 26016117.8235\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26429732.9216 - val_loss: 25753953.0980\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26118695.7451 - val_loss: 25848513.8039\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25973414.2941 - val_loss: 27340839.4314\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26411796.8039 - val_loss: 25714783.4314\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26595901.6471 - val_loss: 25674012.5294\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25888265.0784 - val_loss: 26125341.5294\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 26175949.3333 - val_loss: 25896648.3922\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 26011915.8235 - val_loss: 25783858.3137\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25913769.3725 - val_loss: 25934698.9020\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26255403.6471 - val_loss: 26535175.0980\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 26125441.9608 - val_loss: 25605844.1373\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25809634.7255 - val_loss: 25613259.9804\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25809655.7059 - val_loss: 25604381.0000\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25820071.1176 - val_loss: 25612690.8824\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25843904.4706 - val_loss: 25649532.4118\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25873809.9412 - val_loss: 25594960.0392\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25830629.7647 - val_loss: 25604698.3529\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25810221.8235 - val_loss: 25593035.9216\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25816231.0980 - val_loss: 25590408.0392\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25790979.2941 - val_loss: 25591031.0196\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25797385.2255 - val_loss: 25589364.1176\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25790781.8235 - val_loss: 25589141.8627\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25803878.7059 - val_loss: 25587724.3725\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25831724.4314 - val_loss: 25610472.0980\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25867188.3529 - val_loss: 25644601.7647\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25827592.2941 - val_loss: 25587738.5686\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25777225.7059 - val_loss: 25581447.2157\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25788780.9608 - val_loss: 25585866.5098\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25804256.3725 - val_loss: 25577827.0784\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25790989.9608 - val_loss: 25585986.7451\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25817176.0588 - val_loss: 25608827.8627\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25822276.6471 - val_loss: 25571564.1373\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25801463.0588 - val_loss: 25575310.5294\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25784677.7255 - val_loss: 25572677.0588\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25782337.9020 - val_loss: 25576574.2353\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25854073.7647 - val_loss: 25570677.5490\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25777168.6471 - val_loss: 25570719.9804\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25792362.8039 - val_loss: 25567405.8431\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25784531.9412 - val_loss: 25564182.1569\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25774500.6078 - val_loss: 25561087.3529\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25759770.6275 - val_loss: 25564828.4902\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 85us/step - loss: 25791133.7451 - val_loss: 25568448.3333\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25766249.5882 - val_loss: 25565172.0196\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25758004.5000 - val_loss: 25561499.4706\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25770748.1176 - val_loss: 25560332.4314\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25762761.1078 - val_loss: 25560439.5098\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25751125.8235 - val_loss: 25557815.9608\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25752598.8235 - val_loss: 25555953.7059\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25749758.0196 - val_loss: 25557232.0196\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25742867.0098 - val_loss: 25556890.1569\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25741275.3137 - val_loss: 25550931.2549\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25770953.8431 - val_loss: 25551477.6275\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25756742.4510 - val_loss: 25554161.8039\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 25775884.3039 - val_loss: 25572859.5490\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25796504.6667 - val_loss: 25560648.3137\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25739219.6667 - val_loss: 25553238.6275\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25728960.9020 - val_loss: 25548851.0784\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25724626.1569 - val_loss: 25549018.5098\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25721137.0196 - val_loss: 25544809.2745\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25720331.8333 - val_loss: 25544533.3725\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25721331.5098 - val_loss: 25543485.8824\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25719039.2745 - val_loss: 25543471.0784\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25718354.3529 - val_loss: 25543391.4510\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25722508.1373 - val_loss: 25542656.6275\n",
      "Epoch 128/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25719284.4608 - val_loss: 25543145.2157\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25721536.5686 - val_loss: 25543191.2353\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25719170.8039 - val_loss: 25542367.8824\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25716919.4510 - val_loss: 25542494.7059\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25723158.6471 - val_loss: 25542561.8431\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25719389.9608 - val_loss: 25541968.2157\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25719379.0980 - val_loss: 25542006.1373\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25718088.3529 - val_loss: 25541582.4706\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25719379.2745 - val_loss: 25541490.0784\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25719122.7647 - val_loss: 25541850.5098\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25720343.6275 - val_loss: 25541839.8627\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25717644.0784 - val_loss: 25541043.8039\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25717600.0196 - val_loss: 25541092.4118\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25720038.9216 - val_loss: 25541283.9804\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25720635.2843 - val_loss: 25540775.3333\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25715716.9804 - val_loss: 25540860.0980\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25720553.0784 - val_loss: 25540755.0588\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25714896.8824 - val_loss: 25540192.4118\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25718027.7451 - val_loss: 25540598.9412\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25716006.3725 - val_loss: 25540171.7843\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25715212.2353 - val_loss: 25539963.4902\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25722184.0588 - val_loss: 25540082.0588\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25718933.3529 - val_loss: 25540288.1765\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25714452.0392 - val_loss: 25539821.6078\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25715568.0588 - val_loss: 25539843.4510\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25716528.7059 - val_loss: 25539898.2745\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25715027.3725 - val_loss: 25539571.7059\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25716078.5490 - val_loss: 25539320.6863\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25714088.3039 - val_loss: 25539573.4510\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25717070.8627 - val_loss: 25539195.9804\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25713170.4608 - val_loss: 25538959.1961\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25716385.7647 - val_loss: 25538898.0000\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25725885.6863 - val_loss: 25539525.7647\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25712867.2745 - val_loss: 25538533.0588\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25718018.2353 - val_loss: 25538635.4118\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25713694.4510 - val_loss: 25538373.1176\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25716471.7255 - val_loss: 25538653.0588\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25713137.7255 - val_loss: 25538257.9804\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25715961.6471 - val_loss: 25538237.9608\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25712807.7647 - val_loss: 25537880.1176\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25716229.9804 - val_loss: 25537650.0980\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25716731.7549 - val_loss: 25538063.1373\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25712756.1961 - val_loss: 25537651.9020\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25712683.8725 - val_loss: 25537428.4314\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25717142.1961 - val_loss: 25537589.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25712133.1765 - val_loss: 25537264.2745\n",
      "Epoch 174/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25710632.1176 - val_loss: 25537400.3725\n",
      "Epoch 175/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25711110.4216 - val_loss: 25537230.1569\n",
      "Epoch 176/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25713193.4902 - val_loss: 25536996.4314\n",
      "Epoch 177/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25714105.0784 - val_loss: 25537083.6078\n",
      "Epoch 178/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25714172.9804 - val_loss: 25536993.0392\n",
      "Epoch 179/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25714011.4706 - val_loss: 25536845.6471\n",
      "Epoch 180/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25710955.0000 - val_loss: 25536484.1961\n",
      "Epoch 181/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25711637.7941 - val_loss: 25536651.7451\n",
      "Epoch 182/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25710556.4118 - val_loss: 25536555.2549\n",
      "Epoch 183/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25715554.4216 - val_loss: 25536502.3529\n",
      "Epoch 184/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25708060.2157 - val_loss: 25536551.5490\n",
      "Epoch 185/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25712405.7451 - val_loss: 25536217.2353\n",
      "Epoch 186/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25710913.9804 - val_loss: 25536385.9216\n",
      "Epoch 187/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25712438.2549 - val_loss: 25536162.0196\n",
      "Epoch 188/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25708870.4118 - val_loss: 25536102.0000\n",
      "Epoch 189/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25711268.7059 - val_loss: 25535645.0784\n",
      "Epoch 190/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25712769.6275 - val_loss: 25536250.5882\n",
      "Epoch 191/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25708031.9020 - val_loss: 25535926.7647\n",
      "Epoch 192/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25714519.1961 - val_loss: 25535656.5098\n",
      "Epoch 193/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25718434.7059 - val_loss: 25535542.1176\n",
      "Epoch 194/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25707909.2157 - val_loss: 25534833.2941\n",
      "Epoch 195/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25710341.3333 - val_loss: 25535067.7647\n",
      "Epoch 196/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25708654.9216 - val_loss: 25534962.2353\n",
      "Epoch 197/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25709720.2745 - val_loss: 25535034.6667\n",
      "Epoch 198/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25709162.9412 - val_loss: 25534857.2157\n",
      "Epoch 199/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25707848.5098 - val_loss: 25534624.0588\n",
      "Epoch 200/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25708286.4902 - val_loss: 25534840.2941\n",
      "Epoch 201/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25709233.1373 - val_loss: 25534685.5098\n",
      "Epoch 202/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25707007.8235 - val_loss: 25534259.0980\n",
      "Epoch 203/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25708780.2549 - val_loss: 25534573.4510\n",
      "Epoch 204/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25711730.7255 - val_loss: 25534463.7255\n",
      "Epoch 205/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25711381.7843 - val_loss: 25534350.8824\n",
      "Epoch 206/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25710846.9020 - val_loss: 25533802.8431\n",
      "Epoch 207/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25707467.7059 - val_loss: 25533605.9216\n",
      "Epoch 208/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25714627.1961 - val_loss: 25533563.3725\n",
      "Epoch 209/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25710259.3922 - val_loss: 25533898.3922\n",
      "Epoch 210/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25707487.0784 - val_loss: 25533853.3137\n",
      "Epoch 211/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25708194.8333 - val_loss: 25533810.0980\n",
      "Epoch 212/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25705624.7549 - val_loss: 25533078.4510\n",
      "Epoch 213/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25708607.0588 - val_loss: 25533177.0588\n",
      "Epoch 214/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25705202.9608 - val_loss: 25533247.1765\n",
      "Epoch 215/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25705188.2157 - val_loss: 25533160.3333\n",
      "Epoch 216/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25708568.0392 - val_loss: 25533010.1569\n",
      "Epoch 217/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25713508.3333 - val_loss: 25532708.5098\n",
      "Epoch 218/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25714792.5490 - val_loss: 25533654.9020\n",
      "Epoch 219/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25703558.5490 - val_loss: 25532382.7647\n",
      "Epoch 220/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25706116.8431 - val_loss: 25532456.9804\n",
      "Epoch 221/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25703699.5686 - val_loss: 25532547.0784\n",
      "Epoch 222/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25704222.0588 - val_loss: 25532503.2745\n",
      "Epoch 223/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25705598.5882 - val_loss: 25532153.7647\n",
      "Epoch 224/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25704280.8824 - val_loss: 25532259.1765\n",
      "Epoch 225/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25705213.0000 - val_loss: 25531805.9412\n",
      "Epoch 226/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25708898.9804 - val_loss: 25532824.6863\n",
      "Epoch 227/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25706451.7647 - val_loss: 25531492.0980\n",
      "Epoch 228/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25706671.3137 - val_loss: 25531631.9216\n",
      "Epoch 229/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25707624.6667 - val_loss: 25531771.2353\n",
      "Epoch 230/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25704321.5294 - val_loss: 25531433.1961\n",
      "Epoch 231/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25705389.6275 - val_loss: 25531456.7843\n",
      "Epoch 232/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25704171.6863 - val_loss: 25531522.6078\n",
      "Epoch 233/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25708127.9608 - val_loss: 25531101.5098\n",
      "Epoch 234/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25706351.2549 - val_loss: 25531510.0392\n",
      "Epoch 235/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25702439.5294 - val_loss: 25530885.1176\n",
      "Epoch 236/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25704628.3235 - val_loss: 25531042.2941\n",
      "Epoch 237/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25705257.3529 - val_loss: 25530429.3725\n",
      "Epoch 238/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25703237.8039 - val_loss: 25530631.0588\n",
      "Epoch 239/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 25430295.28 - 0s 80us/step - loss: 25706297.2745 - val_loss: 25530879.1373\n",
      "Epoch 240/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25706954.9216 - val_loss: 25530723.7647\n",
      "Epoch 241/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 89us/step - loss: 25703109.3529 - val_loss: 25530159.0392\n",
      "Epoch 242/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25705636.1373 - val_loss: 25530103.7843\n",
      "Epoch 243/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25701943.7255 - val_loss: 25530007.9608\n",
      "Epoch 244/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25707330.8333 - val_loss: 25530410.7059\n",
      "Epoch 245/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25701273.2745 - val_loss: 25529955.9804\n",
      "Epoch 246/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25704221.2941 - val_loss: 25529451.9804\n",
      "Epoch 247/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25700456.8235 - val_loss: 25529608.0588\n",
      "Epoch 248/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25700544.7059 - val_loss: 25529397.2941\n",
      "Epoch 249/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25709417.6078 - val_loss: 25529786.1765\n",
      "Epoch 250/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25707875.8431 - val_loss: 25530080.4902\n",
      "Epoch 251/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25702173.8824 - val_loss: 25529204.7255\n",
      "Epoch 252/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25700582.4706 - val_loss: 25529056.6275\n",
      "Epoch 253/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25703804.8824 - val_loss: 25529748.8627\n",
      "Epoch 254/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25709528.3725 - val_loss: 25528965.1569\n",
      "Epoch 255/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25700192.0000 - val_loss: 25528664.2353\n",
      "Epoch 256/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25701264.4314 - val_loss: 25528528.1961\n",
      "Epoch 257/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25700579.0000 - val_loss: 25528576.2941\n",
      "Epoch 258/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25701295.0392 - val_loss: 25528207.2549\n",
      "Epoch 259/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25703932.5294 - val_loss: 25527939.5294\n",
      "Epoch 260/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25699522.5490 - val_loss: 25528386.6863\n",
      "Epoch 261/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25702512.6275 - val_loss: 25528130.6275\n",
      "Epoch 262/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25699170.3137 - val_loss: 25528084.0784\n",
      "Epoch 263/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25698533.9608 - val_loss: 25527677.8431\n",
      "Epoch 264/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25701496.2843 - val_loss: 25527719.1961\n",
      "Epoch 265/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25700688.9412 - val_loss: 25528052.3137\n",
      "Epoch 266/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25702236.4510 - val_loss: 25527222.2549\n",
      "Epoch 267/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25698212.0392 - val_loss: 25527448.7843\n",
      "Epoch 268/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25700640.2353 - val_loss: 25527323.0000\n",
      "Epoch 269/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25698329.0000 - val_loss: 25527347.6863\n",
      "Epoch 270/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25697624.9412 - val_loss: 25527154.5490\n",
      "Epoch 271/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697610.8235 - val_loss: 25527216.8824\n",
      "Epoch 272/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25699259.3333 - val_loss: 25526714.2549\n",
      "Epoch 273/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25701864.6667 - val_loss: 25526602.2941\n",
      "Epoch 274/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697226.3137 - val_loss: 25526433.9608\n",
      "Epoch 275/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25697754.0196 - val_loss: 25526381.3529\n",
      "Epoch 276/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25697879.4706 - val_loss: 25526202.9020\n",
      "Epoch 277/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25702347.4706 - val_loss: 25526111.8824\n",
      "Epoch 278/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25700316.1569 - val_loss: 25526323.5294\n",
      "Epoch 279/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25696782.2157 - val_loss: 25525806.1176\n",
      "Epoch 280/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25697330.7255 - val_loss: 25526171.3725\n",
      "Epoch 281/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25696079.2353 - val_loss: 25525636.4314\n",
      "Epoch 282/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25701253.7647 - val_loss: 25525859.1961\n",
      "Epoch 283/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25715551.0000 - val_loss: 25525689.1569\n",
      "Epoch 284/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25698404.8235 - val_loss: 25525486.8235\n",
      "Epoch 285/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25695751.2549 - val_loss: 25525237.0588\n",
      "Epoch 286/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25696671.8431 - val_loss: 25525471.1765\n",
      "Epoch 287/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25698789.4118 - val_loss: 25525655.5686\n",
      "Epoch 288/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25696897.5490 - val_loss: 25524811.6078\n",
      "Epoch 289/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25695906.6471 - val_loss: 25524810.7059\n",
      "Epoch 290/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25697081.0392 - val_loss: 25524783.0784\n",
      "Epoch 291/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25697229.0000 - val_loss: 25524710.1765\n",
      "Epoch 292/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25698439.9608 - val_loss: 25524404.9216\n",
      "Epoch 293/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25697465.7647 - val_loss: 25524432.6863\n",
      "Epoch 294/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25696282.9804 - val_loss: 25524383.7843\n",
      "Epoch 295/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25699613.3333 - val_loss: 25525112.3922\n",
      "Epoch 296/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25696731.1961 - val_loss: 25524358.7843\n",
      "Epoch 297/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25693915.6863 - val_loss: 25524128.6667\n",
      "Epoch 298/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25697219.0392 - val_loss: 25523887.6275\n",
      "Epoch 299/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25695314.5490 - val_loss: 25523746.0000\n",
      "Epoch 300/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25695570.7843 - val_loss: 25523790.1373\n",
      "Epoch 301/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25693056.4118 - val_loss: 25523697.4902\n",
      "Epoch 302/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25694515.0196 - val_loss: 25523596.7255\n",
      "Epoch 303/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25693292.8039 - val_loss: 25523316.4902\n",
      "Epoch 304/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25694612.7843 - val_loss: 25523236.0588\n",
      "Epoch 305/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25699334.6471 - val_loss: 25523431.8627\n",
      "Epoch 306/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25693893.9608 - val_loss: 25522981.0784\n",
      "Epoch 307/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25695285.9020 - val_loss: 25523083.7255\n",
      "Epoch 308/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25696035.7255 - val_loss: 25522569.4902\n",
      "Epoch 309/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25693543.8627 - val_loss: 25522599.4706\n",
      "Epoch 310/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25698510.9608 - val_loss: 25522701.6667\n",
      "Epoch 311/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25696855.0000 - val_loss: 25523485.4510\n",
      "Epoch 312/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25691933.1765 - val_loss: 25522223.6078\n",
      "Epoch 313/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25695191.7843 - val_loss: 25522222.7451\n",
      "Epoch 314/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25692482.3725 - val_loss: 25522139.2549\n",
      "Epoch 315/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25693100.7843 - val_loss: 25522108.0000\n",
      "Epoch 316/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 25691869.8235 - val_loss: 25522138.5098\n",
      "Epoch 317/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25694602.9216 - val_loss: 25521968.8627\n",
      "Epoch 318/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25700458.8039 - val_loss: 25521587.6471\n",
      "Epoch 319/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25694575.7843 - val_loss: 25521551.1961\n",
      "Epoch 320/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25694192.3137 - val_loss: 25522304.0392\n",
      "Epoch 321/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25693173.6471 - val_loss: 25521428.6275\n",
      "Epoch 322/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25692416.0980 - val_loss: 25521107.7843\n",
      "Epoch 323/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25693487.9608 - val_loss: 25521366.0392\n",
      "Epoch 324/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25689755.9216 - val_loss: 25521065.2549\n",
      "Epoch 325/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25694699.5098 - val_loss: 25521261.4706\n",
      "Epoch 326/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25692108.9118 - val_loss: 25520752.4902\n",
      "Epoch 327/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25697476.4314 - val_loss: 25520876.4902\n",
      "Epoch 328/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25691839.9510 - val_loss: 25520375.0000\n",
      "Epoch 329/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25689447.4118 - val_loss: 25520659.2745\n",
      "Epoch 330/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25690923.1373 - val_loss: 25520143.6667\n",
      "Epoch 331/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25688323.0588 - val_loss: 25520024.3333\n",
      "Epoch 332/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25689945.6471 - val_loss: 25520060.2745\n",
      "Epoch 333/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25691238.9608 - val_loss: 25519766.2157\n",
      "Epoch 334/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25690691.3725 - val_loss: 25520268.8627\n",
      "Epoch 335/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25689981.9216 - val_loss: 25519947.5098\n",
      "Epoch 336/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25688574.7647 - val_loss: 25519458.5882\n",
      "Epoch 337/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25689705.0392 - val_loss: 25519626.1176\n",
      "Epoch 338/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 25691444.0000 - val_loss: 25519192.6078\n",
      "Epoch 339/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25688543.3333 - val_loss: 25519288.4510\n",
      "Epoch 340/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25689195.3922 - val_loss: 25519290.0588\n",
      "Epoch 341/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25688793.8431 - val_loss: 25518788.5882\n",
      "Epoch 342/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25689324.1667 - val_loss: 25519099.3922\n",
      "Epoch 343/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25694144.8824 - val_loss: 25520371.5294\n",
      "Epoch 344/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25689439.8235 - val_loss: 25518676.8235\n",
      "Epoch 345/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25688674.7059 - val_loss: 25518192.0196\n",
      "Epoch 346/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25686359.4510 - val_loss: 25518310.6667\n",
      "Epoch 347/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25688371.0980 - val_loss: 25518360.2353\n",
      "Epoch 348/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25690201.5686 - val_loss: 25518028.1961\n",
      "Epoch 349/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25687786.8039 - val_loss: 25518086.6863\n",
      "Epoch 350/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25693034.0588 - val_loss: 25518047.3529\n",
      "Epoch 351/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25686597.3333 - val_loss: 25517786.6667\n",
      "Epoch 352/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25687311.3333 - val_loss: 25517928.8627\n",
      "Epoch 353/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25691637.7059 - val_loss: 25518347.7451\n",
      "Epoch 354/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25683948.6863 - val_loss: 25517427.4314\n",
      "Epoch 355/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25687522.1373 - val_loss: 25517236.1765\n",
      "Epoch 356/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25684292.0784 - val_loss: 25517292.6863\n",
      "Epoch 357/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 25691297.5098 - val_loss: 25517327.0980\n",
      "Epoch 358/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25686092.3333 - val_loss: 25517159.6667\n",
      "Epoch 359/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25688422.3137 - val_loss: 25517150.9020\n",
      "Epoch 360/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25685668.9706 - val_loss: 25517104.8824\n",
      "Epoch 361/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25688290.6078 - val_loss: 25516473.6078\n",
      "Epoch 362/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25686394.0196 - val_loss: 25516349.5294\n",
      "Epoch 363/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25686407.6275 - val_loss: 25516735.1373\n",
      "Epoch 364/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25683692.1765 - val_loss: 25516134.1765\n",
      "Epoch 365/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25688178.1569 - val_loss: 25516179.8824\n",
      "Epoch 366/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25684961.1176 - val_loss: 25516529.9804\n",
      "Epoch 367/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25687732.4216 - val_loss: 25516031.3333\n",
      "Epoch 368/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25693072.8039 - val_loss: 25516164.6078\n",
      "Epoch 369/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25685447.7059 - val_loss: 25516005.9412\n",
      "Epoch 370/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25686738.3137 - val_loss: 25515791.6471\n",
      "Epoch 371/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25685348.4510 - val_loss: 25515734.2745\n",
      "Epoch 372/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25683599.9608 - val_loss: 25515538.9216\n",
      "Epoch 373/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25692658.8431 - val_loss: 25516223.7451\n",
      "Epoch 374/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25684378.3725 - val_loss: 25515453.3922\n",
      "Epoch 375/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25683532.5392 - val_loss: 25515378.0588\n",
      "Epoch 376/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25684417.6863 - val_loss: 25515298.9804\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 70us/step - loss: 25682670.3137 - val_loss: 25514992.1373\n",
      "Epoch 378/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25685851.3922 - val_loss: 25514655.1765\n",
      "Epoch 379/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25682877.6961 - val_loss: 25514951.7843\n",
      "Epoch 380/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25686253.2157 - val_loss: 25515457.2157\n",
      "Epoch 381/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25682016.9804 - val_loss: 25514770.3922\n",
      "Epoch 382/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25684835.8235 - val_loss: 25514531.4510\n",
      "Epoch 383/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25691286.2353 - val_loss: 25514659.4510\n",
      "Epoch 384/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25682945.4314 - val_loss: 25514072.2353\n",
      "Epoch 385/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25685699.4118 - val_loss: 25514185.9608\n",
      "Epoch 386/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25685273.2941 - val_loss: 25514618.7255\n",
      "Epoch 387/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25680091.5294 - val_loss: 25513822.6471\n",
      "Epoch 388/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25681678.4902 - val_loss: 25514253.2549\n",
      "Epoch 389/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25683473.4902 - val_loss: 25513674.6471\n",
      "Epoch 390/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25682226.0000 - val_loss: 25513448.8431\n",
      "Epoch 391/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25682506.8235 - val_loss: 25513912.6863\n",
      "Epoch 392/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25683402.7745 - val_loss: 25513231.4314\n",
      "Epoch 393/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25684637.1373 - val_loss: 25513867.5882\n",
      "Epoch 394/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25679684.2353 - val_loss: 25513055.0000\n",
      "Epoch 395/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25680387.6078 - val_loss: 25513260.2353\n",
      "Epoch 396/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25685581.3922 - val_loss: 25512979.0980\n",
      "Epoch 397/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25678876.4118 - val_loss: 25512820.3333\n",
      "Epoch 398/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25680507.0980 - val_loss: 25512843.1176\n",
      "Epoch 399/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25680839.3529 - val_loss: 25512717.4510\n",
      "Epoch 400/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25678949.5686 - val_loss: 25512680.8824\n",
      "Epoch 401/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25682670.6078 - val_loss: 25512332.1373\n",
      "Epoch 402/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25678049.8824 - val_loss: 25512354.6863\n",
      "Epoch 403/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25683519.2157 - val_loss: 25512133.7451\n",
      "Epoch 404/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677830.8039 - val_loss: 25512062.3333\n",
      "Epoch 405/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25679584.9608 - val_loss: 25511711.0392\n",
      "Epoch 406/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25679785.8824 - val_loss: 25511855.0392\n",
      "Epoch 407/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25678953.9216 - val_loss: 25511862.0784\n",
      "Epoch 408/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25679995.4902 - val_loss: 25511697.2549\n",
      "Epoch 409/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25678950.8627 - val_loss: 25511466.5490\n",
      "Epoch 410/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25681967.0000 - val_loss: 25511554.9020\n",
      "Epoch 411/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25680683.9412 - val_loss: 25511596.0392\n",
      "Epoch 412/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25678283.1373 - val_loss: 25510955.5098\n",
      "Epoch 413/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25681441.2549 - val_loss: 25511097.0588\n",
      "Epoch 414/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25681484.4902 - val_loss: 25511211.5490\n",
      "Epoch 415/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25679560.1569 - val_loss: 25510854.1765\n",
      "Epoch 416/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25679530.8824 - val_loss: 25510673.3529\n",
      "Epoch 417/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25678139.0882 - val_loss: 25510616.1373\n",
      "Epoch 418/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 25679115.9608 - val_loss: 25510326.8627\n",
      "Epoch 419/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25679783.4510 - val_loss: 25510278.3333\n",
      "Epoch 420/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25676741.3725 - val_loss: 25510310.3137\n",
      "Epoch 421/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677748.5686 - val_loss: 25510420.3922\n",
      "Epoch 422/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25675992.7451 - val_loss: 25510362.7451\n",
      "Epoch 423/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25677417.5098 - val_loss: 25509946.4510\n",
      "Epoch 424/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25679605.1961 - val_loss: 25509723.6275\n",
      "Epoch 425/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25676173.3137 - val_loss: 25509780.1176\n",
      "Epoch 426/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25680237.1961 - val_loss: 25509746.3333\n",
      "Epoch 427/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675996.0196 - val_loss: 25509789.0588\n",
      "Epoch 428/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25675291.1569 - val_loss: 25509855.5490\n",
      "Epoch 429/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25678651.3725 - val_loss: 25509322.9216\n",
      "Epoch 430/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 25675510.4020 - val_loss: 25509200.3725\n",
      "Epoch 431/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 25675748.7647 - val_loss: 25509226.5490\n",
      "Epoch 432/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25676359.8824 - val_loss: 25509431.3137\n",
      "Epoch 433/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25675170.0784 - val_loss: 25508991.8431\n",
      "Epoch 434/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25674314.3529 - val_loss: 25508844.5882\n",
      "Epoch 435/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25676871.9412 - val_loss: 25508522.8431\n",
      "Epoch 436/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675479.1373 - val_loss: 25508399.9216\n",
      "Epoch 437/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25680021.8627 - val_loss: 25508528.7843\n",
      "Epoch 438/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25674162.6275 - val_loss: 25508393.9412\n",
      "Epoch 439/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25674480.0294 - val_loss: 25507946.6863\n",
      "Epoch 440/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25673987.6471 - val_loss: 25507827.7255\n",
      "Epoch 441/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25676403.6471 - val_loss: 25508089.0980\n",
      "Epoch 442/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25677171.3922 - val_loss: 25507923.4314\n",
      "Epoch 443/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25684117.9216 - val_loss: 25507834.5882\n",
      "Epoch 444/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25679537.4706 - val_loss: 25508099.0000\n",
      "Epoch 445/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 25402005.97 - 0s 78us/step - loss: 25681364.8431 - val_loss: 25507580.5882\n",
      "Epoch 446/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25677450.1176 - val_loss: 25507749.9020\n",
      "Epoch 447/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25673081.2843 - val_loss: 25507354.6078\n",
      "Epoch 448/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25675854.9608 - val_loss: 25507119.7059\n",
      "Epoch 449/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25673080.5294 - val_loss: 25507483.2353\n",
      "Epoch 450/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25672300.3725 - val_loss: 25506882.8235\n",
      "Epoch 451/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25672824.3333 - val_loss: 25506744.5098\n",
      "Epoch 452/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25674981.8824 - val_loss: 25506854.8824\n",
      "Epoch 453/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25676017.3725 - val_loss: 25506989.3333\n",
      "Epoch 454/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25674206.8824 - val_loss: 25507030.5882\n",
      "Epoch 455/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25676664.4902 - val_loss: 25506319.8824\n",
      "Epoch 456/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25673122.4902 - val_loss: 25506324.4118\n",
      "Epoch 457/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25671619.6078 - val_loss: 25506459.0588\n",
      "Epoch 458/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25673103.5686 - val_loss: 25506061.9608\n",
      "Epoch 459/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25675077.2941 - val_loss: 25505992.4902\n",
      "Epoch 460/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25671439.6667 - val_loss: 25506228.0980\n",
      "Epoch 461/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25672290.4510 - val_loss: 25505785.6863\n",
      "Epoch 462/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25675722.6471 - val_loss: 25505950.1176\n",
      "Epoch 463/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 25682794.5882 - val_loss: 25505522.6078\n",
      "Epoch 464/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25678123.7059 - val_loss: 25505698.0392\n",
      "Epoch 465/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25672556.7647 - val_loss: 25505065.2745\n",
      "Epoch 466/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25679363.0588 - val_loss: 25505489.8039\n",
      "Epoch 467/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25672813.3137 - val_loss: 25504881.7843\n",
      "Epoch 468/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25674576.7647 - val_loss: 25505027.8431\n",
      "Epoch 469/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25671734.9804 - val_loss: 25505218.3137\n",
      "Epoch 470/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25669920.4902 - val_loss: 25504704.2549\n",
      "Epoch 471/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 25669637.6275 - val_loss: 25504685.5882\n",
      "Epoch 472/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25674714.7843 - val_loss: 25504308.6471\n",
      "Epoch 473/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25672252.3725 - val_loss: 25504322.2745\n",
      "Epoch 474/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25673218.9804 - val_loss: 25504888.5098\n",
      "Epoch 475/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25672873.0784 - val_loss: 25504255.3137\n",
      "Epoch 476/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25671242.4314 - val_loss: 25503756.1765\n",
      "Epoch 477/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25670504.3922 - val_loss: 25504112.0392\n",
      "Epoch 478/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25673864.4118 - val_loss: 25504575.3137\n",
      "Epoch 479/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 25669941.7059 - val_loss: 25503563.0392\n",
      "Epoch 480/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25674898.3431 - val_loss: 25503743.7059\n",
      "Epoch 481/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25667963.3137 - val_loss: 25503611.2745\n",
      "Epoch 482/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25672233.8137 - val_loss: 25503552.1373\n",
      "Epoch 483/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25669225.9902 - val_loss: 25503113.2353\n",
      "Epoch 484/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25674987.8431 - val_loss: 25503033.9608\n",
      "Epoch 485/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25669587.4314 - val_loss: 25503273.2353\n",
      "Epoch 486/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25674161.6765 - val_loss: 25503001.1373\n",
      "Epoch 487/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25669687.8725 - val_loss: 25503039.5686\n",
      "Epoch 488/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25667535.2353 - val_loss: 25502544.9216\n",
      "Epoch 489/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25669746.7255 - val_loss: 25502917.8627\n",
      "Epoch 490/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25673101.1765 - val_loss: 25502541.5098\n",
      "Epoch 491/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25669089.0980 - val_loss: 25502630.0196\n",
      "Epoch 492/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25668483.8627 - val_loss: 25502257.5490\n",
      "Epoch 493/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25670566.7647 - val_loss: 25503157.8235\n",
      "Epoch 494/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 25672528.3137 - val_loss: 25502126.9216\n",
      "Epoch 495/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25667468.6863 - val_loss: 25501967.1961\n",
      "Epoch 496/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25666715.2745 - val_loss: 25502356.5098\n",
      "Epoch 497/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25667745.2745 - val_loss: 25502002.6667\n",
      "Epoch 498/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25666157.5294 - val_loss: 25501892.9608\n",
      "Epoch 499/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25667632.3725 - val_loss: 25501862.7451\n",
      "Epoch 500/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25667546.4118 - val_loss: 25501808.0196\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 82946778.9804 - val_loss: 59721666.9804\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 51390808.5098 - val_loss: 46493818.5882\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 44553900.6667 - val_loss: 39583364.0392\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 40352690.5490 - val_loss: 37321797.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 37948536.9412 - val_loss: 37709264.6667\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 34997566.9804 - val_loss: 34277705.2549\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 34819948.3725 - val_loss: 42126162.0000\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 34708454.0000 - val_loss: 30128186.5294\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 32233651.3725 - val_loss: 31018821.6471\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33018381.1569 - val_loss: 34972100.9020\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 34030645.7843 - val_loss: 28890710.9412\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 30980059.9020 - val_loss: 29335250.8627\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 29583558.0392 - val_loss: 27912522.5294\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 30192600.5882 - val_loss: 27551970.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 28919453.4804 - val_loss: 28178441.6471\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 30327976.9608 - val_loss: 29095154.7647\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 28926893.3137 - val_loss: 27594425.4510\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 28362243.1373 - val_loss: 26708582.8627\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 28341436.8235 - val_loss: 26726154.9804\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 27471091.7059 - val_loss: 26919381.6471\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 28800522.6471 - val_loss: 26636041.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 30273411.5686 - val_loss: 27022335.1765\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 29576127.2549 - val_loss: 45416530.8235\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 29123969.4510 - val_loss: 26573367.5882\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 28276563.4706 - val_loss: 27028726.1569\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 29152622.9216 - val_loss: 26237803.7451\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 27165934.5686 - val_loss: 26269912.2745\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 28971347.6471 - val_loss: 29388564.0784\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 27356388.8824 - val_loss: 27788380.3529\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 26958624.1569 - val_loss: 26725952.4314\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27199138.0588 - val_loss: 26528076.0000\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 26025675.5294 - val_loss: 26009434.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25598945.7647 - val_loss: 26196297.9608\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25618391.9216 - val_loss: 26120719.3137\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25610775.3529 - val_loss: 26166324.6471\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25482755.0980 - val_loss: 26261289.6078\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25510269.1961 - val_loss: 26589966.4902\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25503763.1765 - val_loss: 26172221.6667\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25373699.9216 - val_loss: 26138137.8039\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25381695.4510 - val_loss: 26087105.5098\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25363109.7353 - val_loss: 26140071.7059\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25389411.7255 - val_loss: 26112121.8431\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25354612.5294 - val_loss: 26113062.5686\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25351844.2157 - val_loss: 26116639.0392\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25351273.4902 - val_loss: 26117161.9412\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25350898.6078 - val_loss: 26118361.5294\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 25352085.7745 - val_loss: 26127261.2941\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 25349723.8824 - val_loss: 26126466.0588\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 25349732.3137 - val_loss: 26127188.6275\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 25350005.2745 - val_loss: 26126837.3529\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 25349664.9608 - val_loss: 26126632.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25349620.0784 - val_loss: 26126960.1961\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 271us/step - loss: 91782332.3137 - val_loss: 59672409.1373\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 47173485.7647 - val_loss: 47204361.4118\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 40496850.6667 - val_loss: 40404836.2745\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 35929500.4314 - val_loss: 36417344.1176\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 35199159.9412 - val_loss: 34505865.7255\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 34088930.1961 - val_loss: 34205457.5294\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 31450674.7451 - val_loss: 31540641.4510\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 31199857.1373 - val_loss: 33053099.3137\n",
      "Epoch 9/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 73us/step - loss: 33523884.9412 - val_loss: 31555560.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 31176775.5098 - val_loss: 34681539.1961\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 30122155.1569 - val_loss: 35287318.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 29912483.4510 - val_loss: 30468553.3725\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 28712861.6667 - val_loss: 35443550.5098\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 28589489.7451 - val_loss: 33150980.7843\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 28147537.9020 - val_loss: 28027794.2941\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 27811565.4902 - val_loss: 30160031.8824\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 27449481.0196 - val_loss: 29074900.0392\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 27724137.3137 - val_loss: 28100571.8627\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 29390495.4804 - val_loss: 31797294.6667\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 27623131.4902 - val_loss: 27994412.6471\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 27161977.8627 - val_loss: 27629192.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26617729.2353 - val_loss: 26927668.8039\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 27693152.8235 - val_loss: 28653036.4118\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 27871095.1765 - val_loss: 29372523.2549\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 27490789.5098 - val_loss: 27873033.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 27434450.6667 - val_loss: 26859492.5294\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 28502153.5098 - val_loss: 35336093.0196\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 27744901.4314 - val_loss: 32550237.8824\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 28182435.5882 - val_loss: 30289542.7255\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 27535056.4510 - val_loss: 26707000.9804\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 27834761.9216 - val_loss: 27201667.4902\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 27352160.6863 - val_loss: 26882251.3333\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 26427065.0392 - val_loss: 27942884.0588\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 26877436.1961 - val_loss: 27772624.0196\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 26964514.7647 - val_loss: 29191848.3137\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 26110991.8431 - val_loss: 26414715.2745\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25546812.4510 - val_loss: 26394750.8039\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 25528601.2745 - val_loss: 27130954.7843\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25647612.0784 - val_loss: 26522132.3333\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25555928.8039 - val_loss: 26373997.2353\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 25529719.1961 - val_loss: 26319625.8235\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25708189.3922 - val_loss: 26372280.4902\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25717426.9608 - val_loss: 27129180.8431\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 25628774.1569 - val_loss: 26456925.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 25492344.9020 - val_loss: 26515135.1176\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25652070.4510 - val_loss: 26479785.9020\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 25372392.3922 - val_loss: 26338789.9804\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25348140.6078 - val_loss: 26384502.4706\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 25358454.8235 - val_loss: 26328286.2745\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25346250.3922 - val_loss: 26347990.1176\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25374283.6863 - val_loss: 26349128.7059\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 25334662.2941 - val_loss: 26340621.8235\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 25336179.8627 - val_loss: 26336090.7843\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25337545.8039 - val_loss: 26346933.4118\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25335149.6471 - val_loss: 26338951.1961\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 25334905.2549 - val_loss: 26338207.3137\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25333418.2451 - val_loss: 26338471.3333\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25333463.9216 - val_loss: 26338751.8824\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 25333442.3922 - val_loss: 26338982.0196\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25333222.7647 - val_loss: 26338213.7255\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 25333310.0196 - val_loss: 26338867.8039\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 329us/step - loss: 118263792.0784 - val_loss: 42715792.2745\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 41898732.5882 - val_loss: 38583136.9020\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 39087490.9412 - val_loss: 35245440.0000\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 35199048.3529 - val_loss: 33101889.0392\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 33288501.7255 - val_loss: 36056351.0784\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 32631250.5098 - val_loss: 30015323.3333\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 30608631.7647 - val_loss: 30558668.2549\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 29693668.3137 - val_loss: 29297594.8235\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 28322629.6471 - val_loss: 27791851.9804\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 27944775.2549 - val_loss: 27962289.0784\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 27165172.9608 - val_loss: 26253950.2353\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 27114924.0000 - val_loss: 25675070.1765\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 25771009.2157 - val_loss: 25401173.3725\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 26049826.4314 - val_loss: 31582816.0000\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 26334134.8824 - val_loss: 24906350.0980\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 24682684.9216 - val_loss: 25150368.0000\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25038873.7451 - val_loss: 25835424.7255\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 25052825.0490 - val_loss: 24920511.3529\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24476969.9608 - val_loss: 26578019.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24711898.3725 - val_loss: 26123545.0588\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23792402.1373 - val_loss: 24322083.2745\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23458198.6471 - val_loss: 24241275.4510\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 23424090.5882 - val_loss: 24217778.8431\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23408108.1569 - val_loss: 24205910.6863\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23373400.8431 - val_loss: 24424043.1765\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23355975.3529 - val_loss: 24172935.1765\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 23306036.0196 - val_loss: 24211568.5490\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23280886.1961 - val_loss: 24157576.7059\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 23282916.2353 - val_loss: 24117001.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23203136.9216 - val_loss: 24512203.5294\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 23191117.3333 - val_loss: 24311569.9216\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 23174068.1961 - val_loss: 24378043.1961\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23185416.4902 - val_loss: 24112872.6078\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23154778.4314 - val_loss: 24099745.2157\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23119197.6471 - val_loss: 24095905.0588\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 23334749.3529 - val_loss: 24094599.0588\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23071229.9412 - val_loss: 24108057.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23091561.3922 - val_loss: 24349731.1765\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23149947.4314 - val_loss: 24522125.0980\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23253210.6373 - val_loss: 24537581.7647\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23120670.4706 - val_loss: 24138338.3137\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22981257.9804 - val_loss: 24010689.5882\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22934140.3137 - val_loss: 23997574.6863\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22930883.9902 - val_loss: 23997319.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22927508.7647 - val_loss: 24003744.4314\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22920614.2353 - val_loss: 24004229.5882\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22918080.3529 - val_loss: 24000236.8627\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22915901.7843 - val_loss: 24004114.5882\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 22918106.0196 - val_loss: 24002910.2549\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22900801.9902 - val_loss: 24002763.7059\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 22900325.5588 - val_loss: 24003455.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22899042.6373 - val_loss: 24003717.0980\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 22899154.9902 - val_loss: 24003959.7451\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 22898137.8627 - val_loss: 24004440.3725\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22897089.3333 - val_loss: 24004399.6863\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22897032.2157 - val_loss: 24004529.1765\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22897441.2353 - val_loss: 24004545.0588\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 22896887.5000 - val_loss: 24004537.1569\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 22896974.1176 - val_loss: 24004516.2549\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22896812.5882 - val_loss: 24004519.6078\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 22896811.4118 - val_loss: 24004513.8235\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22896808.8627 - val_loss: 24004519.7451\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22896821.4314 - val_loss: 24004523.7451\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22896806.6176 - val_loss: 24004520.8039\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 1s 250us/step - loss: 126534560.7451 - val_loss: 38835256.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 39599536.3529 - val_loss: 35459830.1569\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 38019695.6078 - val_loss: 33322305.5098\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 33558984.0980 - val_loss: 30644346.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 30549968.6863 - val_loss: 28448031.9020\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 30443045.9608 - val_loss: 28174112.7843\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 28671546.2353 - val_loss: 27324752.6078\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 28606383.6275 - val_loss: 27447076.9216\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 27481411.8039 - val_loss: 29500628.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 28214865.0392 - val_loss: 26950521.5490\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 27461660.9608 - val_loss: 31558550.3922\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 26597962.1765 - val_loss: 25900246.1765\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 26987821.5294 - val_loss: 25803659.9804\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 26030728.6078 - val_loss: 25911930.3922\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 25368182.9216 - val_loss: 26221916.4706\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 26770714.9412 - val_loss: 25537807.7255\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 25810973.7843 - val_loss: 25506607.5294\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 26876238.4118 - val_loss: 26870477.9020\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 25793857.7843 - val_loss: 28723725.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 24754711.6471 - val_loss: 24707536.0196\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 26196981.1765 - val_loss: 25629985.1373\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 25291450.6863 - val_loss: 25782056.2157\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25068476.6667 - val_loss: 25596600.9412\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 24585660.7255 - val_loss: 24790632.0392\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 24215369.5098 - val_loss: 25380909.5294\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 24015109.4314 - val_loss: 24506127.2353\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 23754095.6275 - val_loss: 24436059.9216\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 23701556.2549 - val_loss: 24384433.0392\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23781459.2549 - val_loss: 24276731.2157\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23629858.1765 - val_loss: 24391373.9412\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23563769.1176 - val_loss: 24582113.6471\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 23579401.8431 - val_loss: 24288692.2941\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 23579220.4314 - val_loss: 24405809.7255\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23491073.0980 - val_loss: 24181633.2353\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 23534819.7255 - val_loss: 24484757.4510\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 23536729.9020 - val_loss: 24296272.2353\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 23451778.1765 - val_loss: 24180451.4314\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 23574892.8824 - val_loss: 24194979.5294\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 23471472.1961 - val_loss: 24325536.0196\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23505552.9412 - val_loss: 24700875.7451\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 23584976.0882 - val_loss: 24157957.9020\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23425303.1961 - val_loss: 24195449.8039\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 23715365.0980 - val_loss: 24113476.5686\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 23463119.0000 - val_loss: 24080454.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23394959.7059 - val_loss: 24079891.0196\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 23366078.6471 - val_loss: 24103818.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 23343197.8824 - val_loss: 24219675.7647\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 23324282.3431 - val_loss: 24071808.8235\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23431956.3137 - val_loss: 24166314.9216\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 23106118.0980 - val_loss: 24561505.9608\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 23158733.8627 - val_loss: 24125956.7059\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 23146631.1961 - val_loss: 24101370.7451\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23294820.8235 - val_loss: 23994159.7255\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 23023179.9804 - val_loss: 24439037.5098\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 23282023.3529 - val_loss: 24031546.2353\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 23075108.4902 - val_loss: 24002735.6471\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23062361.2157 - val_loss: 23988398.4314\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22991334.4216 - val_loss: 24052498.4510\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 23018030.1373 - val_loss: 24422462.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23090225.7451 - val_loss: 24300058.7647\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 23059677.6275 - val_loss: 24006467.7059\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 22937177.3529 - val_loss: 24285111.7647\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22937568.1373 - val_loss: 24093279.8235\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 22863575.1961 - val_loss: 24021994.3137\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 22852598.0980 - val_loss: 23989614.3333\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 22857490.5686 - val_loss: 23978399.7451\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22850432.6275 - val_loss: 23993968.7059\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 22870454.1176 - val_loss: 23996797.0588\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 22854384.0196 - val_loss: 23977162.4902\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22860236.5686 - val_loss: 23993742.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22855134.4902 - val_loss: 23982494.4510\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22851631.3137 - val_loss: 23984645.0000\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22843299.9804 - val_loss: 23999546.7059\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 22843506.1961 - val_loss: 23988705.9216\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22830215.3529 - val_loss: 23988497.5490\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22829077.4118 - val_loss: 23985898.4706\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22829050.2941 - val_loss: 23982882.9020\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 22828099.1176 - val_loss: 23983026.1765\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 22827412.3725 - val_loss: 23982231.7647\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22826166.6667 - val_loss: 23982263.4118\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22826195.0588 - val_loss: 23982362.4314\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 22826276.8824 - val_loss: 23982197.3529\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 22826214.3039 - val_loss: 23982166.8039\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 22826181.3333 - val_loss: 23982389.3922\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 22825981.8235 - val_loss: 23982387.5490\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22825989.3137 - val_loss: 23982372.6863\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22825985.3333 - val_loss: 23982360.7843\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22825976.6078 - val_loss: 23982371.3725\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22825972.4314 - val_loss: 23982364.6275\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 365us/step - loss: 188411387.6078 - val_loss: 46044743.4510\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 38068344.2549 - val_loss: 37013242.8235\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 33784286.4706 - val_loss: 33215039.3725\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 34617741.1961 - val_loss: 32351768.5686\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 30464655.9608 - val_loss: 33053275.2157\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 29039302.3922 - val_loss: 31231439.1961\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 28453388.9608 - val_loss: 31046997.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 28773784.7255 - val_loss: 28136898.2157\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 27970094.5098 - val_loss: 28102582.2353\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26949277.3137 - val_loss: 27891071.3725\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 27629367.4314 - val_loss: 27497914.9412\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 27018101.2157 - val_loss: 31827598.2941\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 27228265.5490 - val_loss: 27458633.4902\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 26504292.6471 - val_loss: 29688548.8627\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 25656777.8235 - val_loss: 25804448.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 26860562.5882 - val_loss: 30061555.2157\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 26787371.1569 - val_loss: 26008004.0784\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25540745.9804 - val_loss: 26351110.6667\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 25827711.3922 - val_loss: 30475428.9608\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 25968552.3725 - val_loss: 25324330.7843\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 25442411.1961 - val_loss: 25576539.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 25903129.4510 - val_loss: 27307794.4902\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 24876747.5294 - val_loss: 24547734.8824\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 24739550.7843 - val_loss: 24541399.3333\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 24961004.4706 - val_loss: 28987265.4706\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 25552252.6863 - val_loss: 25932402.5686\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 25703343.2549 - val_loss: 30677695.7647\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 24994236.5686 - val_loss: 24651693.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 24241675.9804 - val_loss: 24069432.2941\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 26083473.3333 - val_loss: 25292427.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 24690236.7059 - val_loss: 23748899.9608\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24645339.3333 - val_loss: 24119995.2549\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 24465573.2549 - val_loss: 25060943.1373\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24571040.9216 - val_loss: 23828889.8235\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 24333717.8235 - val_loss: 23789646.6078\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 24006949.2941 - val_loss: 23715695.3922\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 24064238.9608 - val_loss: 24843601.4510\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 24403238.4118 - val_loss: 24257585.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 24086675.9804 - val_loss: 23445022.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23962993.0784 - val_loss: 23765076.1373\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 24228290.1373 - val_loss: 23528088.1176\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23961780.7549 - val_loss: 23665412.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 24088762.9902 - val_loss: 25789395.6667\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 24006538.9902 - val_loss: 23387551.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23535500.0392 - val_loss: 23247013.4118\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 23917638.0392 - val_loss: 25458118.9608\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 23797788.9412 - val_loss: 23311499.0196\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 24291929.6863 - val_loss: 23060936.6078\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 23391507.0784 - val_loss: 25873744.9020\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 24647350.7647 - val_loss: 23500446.5686\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 23700977.1373 - val_loss: 23396441.9216\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 23273258.2941 - val_loss: 24256198.1373\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 23451122.0196 - val_loss: 23604952.0392\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 23026823.5882 - val_loss: 22858827.3529\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22702006.0980 - val_loss: 22911793.8039\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22643471.3529 - val_loss: 22825833.1765\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22536191.9804 - val_loss: 23040526.2353\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 22651604.5588 - val_loss: 22886361.3529\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 22574145.8333 - val_loss: 22955208.1765\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22507235.2647 - val_loss: 22862621.9020\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 22495783.3824 - val_loss: 22799820.5294\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22482532.8627 - val_loss: 22737156.7843\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22495691.6275 - val_loss: 22771335.1569\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22493940.6275 - val_loss: 22773654.3922\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 22433698.7941 - val_loss: 22799281.2157\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 22432990.6863 - val_loss: 22816959.3529\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 22419460.2549 - val_loss: 22978573.0196\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 22300392.0000 - val_loss: 22843087.6471\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22272207.3137 - val_loss: 22820832.2745\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 22261868.5294 - val_loss: 22814571.1961\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22248478.6275 - val_loss: 22794638.2941\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 22249047.0784 - val_loss: 22799666.7647\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 22235510.0196 - val_loss: 22802114.5098\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 22234896.6961 - val_loss: 22804191.8627\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 22233607.4902 - val_loss: 22803465.6078\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22234314.9216 - val_loss: 22803448.2941\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 22235481.6275 - val_loss: 22805477.9020\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 22231922.6667 - val_loss: 22805422.1961\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 22231800.6863 - val_loss: 22805425.8235\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 22231783.2745 - val_loss: 22805605.1373\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 22231889.4118 - val_loss: 22805711.6078\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 22231845.6863 - val_loss: 22805499.7843\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 572us/step - loss: 236093481.5686 - val_loss: 86871950.5098\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 74215839.4510 - val_loss: 127136287.6863\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 74196395.6078 - val_loss: 57332914.7843\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 58661151.0588 - val_loss: 49560105.4118\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 54151350.9020 - val_loss: 46580192.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 48224615.0196 - val_loss: 51496940.8627\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 58245508.3137 - val_loss: 68641306.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 72812609.8039 - val_loss: 58124100.6275\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 65863459.4510 - val_loss: 61539441.7255\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 76019799.8039 - val_loss: 83749897.0196\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 72725872.7451 - val_loss: 59979632.7843\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 66185629.0196 - val_loss: 64554599.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 65329413.7255 - val_loss: 65080089.8039\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 60554247.5294 - val_loss: 56663354.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 61853634.0000 - val_loss: 56505628.0000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 62000869.9216 - val_loss: 54365905.9608\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 61022247.9608 - val_loss: 53812063.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 60416192.2353 - val_loss: 52401017.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 59917956.0784 - val_loss: 53118877.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 59421984.0000 - val_loss: 53094746.2745\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 59424207.4902 - val_loss: 52981206.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 59359133.8824 - val_loss: 52938559.8431\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 59461953.3333 - val_loss: 52957167.9216\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 59385824.9020 - val_loss: 52983766.6667\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 59324336.6275 - val_loss: 52996207.2157\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 588us/step - loss: 265568064.9412 - val_loss: 70625555.6863\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 69264747.8431 - val_loss: 53689811.8039\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 58062044.0784 - val_loss: 52002202.3529\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 50630834.9608 - val_loss: 51811420.9020\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 1s 215us/step - loss: 59543342.1176 - val_loss: 50853689.2157\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 227us/step - loss: 50147073.8431 - val_loss: 55272581.0588\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 85172164.3137 - val_loss: 63389489.0196\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 62902856.3922 - val_loss: 74484233.2941\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 69531525.2549 - val_loss: 49023075.4118\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 58219540.3137 - val_loss: 60381949.9608\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 54676276.0000 - val_loss: 57245270.5098\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54898758.7843 - val_loss: 54446991.5294\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 61046362.4314 - val_loss: 65688503.6863\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 62832880.3529 - val_loss: 56546635.5294\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 59468802.4314 - val_loss: 52789398.4314\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 58588170.2745 - val_loss: 53804553.5686\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 55527395.6863 - val_loss: 52498683.2941\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 53132464.8627 - val_loss: 53906304.0000\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 53373115.0980 - val_loss: 52010106.8627\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 51719941.9608 - val_loss: 51813492.5490\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 51710450.4314 - val_loss: 51344305.8431\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 51857829.8039 - val_loss: 52717042.5098\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 51588350.8235 - val_loss: 52450965.6471\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 51453741.8824 - val_loss: 52655783.9216\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 51360537.7255 - val_loss: 51455105.0980\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 51373229.7255 - val_loss: 52533384.2745\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 51358981.7255 - val_loss: 52504332.0392\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 167us/step - loss: 51351710.3529 - val_loss: 52511826.6667\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 51346249.3333 - val_loss: 52577559.8824\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 643us/step - loss: 104227161.7255 - val_loss: 62090245.7647\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 65490972.6667 - val_loss: 70477488.9412\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 73217837.4902 - val_loss: 78013570.9804\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 58231116.9412 - val_loss: 62762515.6078\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 54733682.7059 - val_loss: 52539658.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 50729703.4902 - val_loss: 55050194.6667\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 49737692.9412 - val_loss: 48825280.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 234us/step - loss: 51550888.6275 - val_loss: 50973036.8235\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 254us/step - loss: 49666638.3922 - val_loss: 70031927.4510\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 233us/step - loss: 68785659.9608 - val_loss: 70732202.6667\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 51271616.1569 - val_loss: 54608181.0196\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 188us/step - loss: 47239799.8431 - val_loss: 52169689.2157\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 45431005.2941 - val_loss: 52737803.2941\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 164us/step - loss: 45221952.3922 - val_loss: 52290478.0784\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 45141756.2745 - val_loss: 51954780.1176\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 45089189.2157 - val_loss: 51208064.7451\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 44750483.7647 - val_loss: 51379186.4314\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 44398362.9412 - val_loss: 51338924.9804\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 227us/step - loss: 44390968.0392 - val_loss: 51232526.0392\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 44389373.9216 - val_loss: 51097322.8235\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 234us/step - loss: 44337279.8824 - val_loss: 51196838.1961\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 44348374.6275 - val_loss: 51315601.9216\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 44282746.1765 - val_loss: 51298688.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 212us/step - loss: 44275974.8431 - val_loss: 51266432.4706\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 44272917.6471 - val_loss: 51249671.9216\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 44273980.0000 - val_loss: 51232288.8627\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 44268078.2353 - val_loss: 51227975.6863\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 7 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4298.941025214379, 7148.259284985343, 0.7738868599113026)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 344us/step - loss: 84808858.5882 - val_loss: 63445905.3333\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 68111017.1373 - val_loss: 61921716.4706\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 59857207.0588 - val_loss: 62318097.3333\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 56904943.1765 - val_loss: 58483435.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 54973909.7647 - val_loss: 57055785.7647\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 55677800.0392 - val_loss: 57409088.5882\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54287498.0784 - val_loss: 54696690.9412\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 53077111.2157 - val_loss: 54218346.6275\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 53617748.8627 - val_loss: 71874680.1569\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 52318959.0588 - val_loss: 53575464.0000\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 52846291.8824 - val_loss: 62851206.0392\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 53740366.4706 - val_loss: 53643618.1176\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 50582359.8431 - val_loss: 57252867.2157\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 52346878.8627 - val_loss: 52570931.1373\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 53080635.5294 - val_loss: 63859796.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 51839640.1176 - val_loss: 61521410.0392\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 55251955.6471 - val_loss: 63674761.6863\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 49330070.3529 - val_loss: 52390665.0980\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50654373.2549 - val_loss: 59358234.1961\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 50089277.8824 - val_loss: 54027628.4706\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 49324577.2157 - val_loss: 60879526.1961\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 50473687.7255 - val_loss: 51935568.0000\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 52195066.4314 - val_loss: 50836637.2941\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51198372.7059 - val_loss: 52784553.9608\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50840281.7647 - val_loss: 55135353.1373\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 50522118.0784 - val_loss: 53159367.1373\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 51658444.8627 - val_loss: 51436437.1765\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 51560110.7843 - val_loss: 68540537.9608\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 49282016.7059 - val_loss: 50686132.2353\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46355783.3725 - val_loss: 50592739.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46269562.3922 - val_loss: 50437314.6275\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 33us/step - loss: 46164456.6667 - val_loss: 50451795.0980\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46228020.5882 - val_loss: 50910948.2745\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45944910.1569 - val_loss: 50533209.0196\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - ETA: 0s - loss: 46293348.00 - 0s 40us/step - loss: 45883570.4706 - val_loss: 50370025.2549\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46223628.5490 - val_loss: 51107020.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45872397.4510 - val_loss: 50287677.3333\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45956052.2745 - val_loss: 50255819.0980\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45934514.0392 - val_loss: 50160725.4510\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 45977840.0392 - val_loss: 50065723.4510\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45775439.3725 - val_loss: 50127523.9216\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45587727.5294 - val_loss: 50398336.3922\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45852695.3333 - val_loss: 50045264.7843\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 45603039.8431 - val_loss: 50008201.3725\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45949907.8039 - val_loss: 51084777.4902\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45557439.6078 - val_loss: 49937181.2549\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45594556.9412 - val_loss: 50265413.2941\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 45472882.4314 - val_loss: 50454997.6863\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45598200.9412 - val_loss: 50033218.6667\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45592408.7843 - val_loss: 50012450.0000\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45652208.8235 - val_loss: 49783103.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45392372.3137 - val_loss: 50928750.4314\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45573331.8039 - val_loss: 49930922.9020\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45403176.0000 - val_loss: 50035615.7647\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45685569.1373 - val_loss: 49747940.3137\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45651518.0392 - val_loss: 50422479.9216\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45676761.7647 - val_loss: 49684618.6667\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45674778.4314 - val_loss: 50471529.9216\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45475239.6471 - val_loss: 50475982.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45397185.4902 - val_loss: 49763205.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45856468.9804 - val_loss: 51711886.7059\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 45338990.7059 - val_loss: 52428323.8824\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 46276799.4706 - val_loss: 49516067.2157\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45042438.6863 - val_loss: 49538663.0196\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 44873053.2157 - val_loss: 49444151.0980\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44808064.7451 - val_loss: 49443810.8627\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44919461.8039 - val_loss: 49441534.0784\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44815908.0784 - val_loss: 49420819.0196\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44808957.9608 - val_loss: 49428927.6471\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44824096.7843 - val_loss: 49422240.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44815705.6078 - val_loss: 49425937.4510\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44811366.1176 - val_loss: 49417545.9216\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44773745.5686 - val_loss: 49423421.6863\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44804307.2941 - val_loss: 49410523.2157\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 44816856.4706 - val_loss: 49423432.0000\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44792561.0588 - val_loss: 49407739.1765\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 44783188.2745 - val_loss: 49392141.0196\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44807281.1961 - val_loss: 49413742.7059\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44802917.1373 - val_loss: 49412365.9608\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44759997.9216 - val_loss: 49407584.2745\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44825014.9804 - val_loss: 49390517.6471\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 44794358.2353 - val_loss: 49384728.8235\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44750139.8431 - val_loss: 49391168.2745\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44838214.4314 - val_loss: 49419619.1765\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44762859.8824 - val_loss: 49378989.1765\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44782231.0980 - val_loss: 49392308.5490\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44763039.8039 - val_loss: 49387355.6863\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44792748.0784 - val_loss: 49372392.2745\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44756831.0588 - val_loss: 49379876.0784\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44768256.3922 - val_loss: 49409531.1765\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44804352.4706 - val_loss: 49394721.0196\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44736903.8824 - val_loss: 49362776.5490\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44727734.4314 - val_loss: 49358278.8235\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44727987.1765 - val_loss: 49364285.9608\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44715873.2157 - val_loss: 49360049.7255\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44761590.4706 - val_loss: 49371253.1765\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44748318.2353 - val_loss: 49350499.9216\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44722402.9020 - val_loss: 49369355.3725\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44682754.4706 - val_loss: 49356603.2549\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 44702338.5882 - val_loss: 49363319.7255\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44709641.0588 - val_loss: 49345669.0588\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44732324.2353 - val_loss: 49371057.4118\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44697591.6078 - val_loss: 49365475.6863\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44697173.1569 - val_loss: 49337346.0000\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44693247.8039 - val_loss: 49334443.0980\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44700859.1765 - val_loss: 49327455.6471\n",
      "Epoch 107/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44676248.8627 - val_loss: 49330397.5294\n",
      "Epoch 108/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 44684188.8431 - val_loss: 49345329.6863\n",
      "Epoch 109/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 44685299.9216 - val_loss: 49320283.7647\n",
      "Epoch 110/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44656601.2549 - val_loss: 49325138.6275\n",
      "Epoch 111/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44657721.4118 - val_loss: 49315912.8627\n",
      "Epoch 112/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44726202.1569 - val_loss: 49318348.0784\n",
      "Epoch 113/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44679583.6863 - val_loss: 49336399.5686\n",
      "Epoch 114/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44643353.5686 - val_loss: 49323646.9804\n",
      "Epoch 115/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 44644840.3529 - val_loss: 49304820.5490\n",
      "Epoch 116/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44635890.7059 - val_loss: 49311119.2549\n",
      "Epoch 117/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44639284.7059 - val_loss: 49301318.5882\n",
      "Epoch 118/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44633241.0588 - val_loss: 49327097.0196\n",
      "Epoch 119/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44722148.6275 - val_loss: 49338577.0196\n",
      "Epoch 120/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44693598.2745 - val_loss: 49304760.9020\n",
      "Epoch 121/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44667054.1176 - val_loss: 49313570.1961\n",
      "Epoch 122/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44601593.5294 - val_loss: 49303844.8627\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 123/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44588847.7451 - val_loss: 49302050.5882\n",
      "Epoch 124/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44591971.2549 - val_loss: 49302679.5294\n",
      "Epoch 125/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44585589.0196 - val_loss: 49300186.0000\n",
      "Epoch 126/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44585146.1569 - val_loss: 49298003.9216\n",
      "Epoch 127/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 44582575.1373 - val_loss: 49298327.4118\n",
      "Epoch 128/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44583960.8235 - val_loss: 49295655.2549\n",
      "Epoch 129/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44584304.5882 - val_loss: 49295628.3922\n",
      "Epoch 130/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 44581364.0784 - val_loss: 49296743.6471\n",
      "Epoch 131/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44579899.0196 - val_loss: 49294886.8627\n",
      "Epoch 132/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44578890.5882 - val_loss: 49295592.9412\n",
      "Epoch 133/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44579069.7647 - val_loss: 49292958.7843\n",
      "Epoch 134/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44578254.4314 - val_loss: 49294027.9216\n",
      "Epoch 135/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 44590820.2353 - val_loss: 49293506.2353\n",
      "Epoch 136/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 44575274.5098 - val_loss: 49293297.2941\n",
      "Epoch 137/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44582826.3922 - val_loss: 49291786.3137\n",
      "Epoch 138/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 44584593.9216 - val_loss: 49292402.7843\n",
      "Epoch 139/500\n",
      "2550/2550 [==============================] - 0s 28us/step - loss: 44575719.3725 - val_loss: 49291535.0980\n",
      "Epoch 140/500\n",
      "2550/2550 [==============================] - 0s 31us/step - loss: 44582223.3725 - val_loss: 49292381.6078\n",
      "Epoch 141/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 44584618.4314 - val_loss: 49291872.1961\n",
      "Epoch 142/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 44580581.6471 - val_loss: 49290471.8431\n",
      "Epoch 143/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44580073.3333 - val_loss: 49291006.7843\n",
      "Epoch 144/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 44576644.4706 - val_loss: 49291602.6275\n",
      "Epoch 145/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44580163.7647 - val_loss: 49289534.3137\n",
      "Epoch 146/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44575523.2353 - val_loss: 49289070.2353\n",
      "Epoch 147/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 44576504.6275 - val_loss: 49288815.3725\n",
      "Epoch 148/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 44578341.2549 - val_loss: 49288914.6667\n",
      "Epoch 149/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 44577035.8431 - val_loss: 49288441.0196\n",
      "Epoch 150/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44572522.3137 - val_loss: 49288856.4314\n",
      "Epoch 151/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44576677.2549 - val_loss: 49288960.4314\n",
      "Epoch 152/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44574660.2745 - val_loss: 49288043.8039\n",
      "Epoch 153/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44571443.1373 - val_loss: 49286419.8824\n",
      "Epoch 154/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44573742.4314 - val_loss: 49288026.1961\n",
      "Epoch 155/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 44578666.3529 - val_loss: 49286791.6078\n",
      "Epoch 156/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44584134.1569 - val_loss: 49288630.1176\n",
      "Epoch 157/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44573724.4706 - val_loss: 49287331.0588\n",
      "Epoch 158/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44569716.3137 - val_loss: 49286760.3529\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 159/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 44566492.6275 - val_loss: 49286731.1765\n",
      "Epoch 160/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566269.0588 - val_loss: 49286668.2353\n",
      "Epoch 161/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 44566029.7647 - val_loss: 49286618.0000\n",
      "Epoch 162/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566037.1765 - val_loss: 49286697.9608\n",
      "Epoch 163/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44566448.3922 - val_loss: 49286534.9804\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 164/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565574.5490 - val_loss: 49286533.0588\n",
      "Epoch 165/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565577.8431 - val_loss: 49286532.8627\n",
      "Epoch 166/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565582.4706 - val_loss: 49286532.7059\n",
      "Epoch 167/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 44565569.8039 - val_loss: 49286532.4706\n",
      "Epoch 168/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 44565568.5882 - val_loss: 49286530.1961\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 169/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44565553.9608 - val_loss: 49286530.8235\n",
      "Epoch 170/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 44565556.1176 - val_loss: 49286530.2745\n",
      "Epoch 171/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565555.9608 - val_loss: 49286529.6863\n",
      "Epoch 172/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 44565555.4902 - val_loss: 49286531.6078\n",
      "Epoch 173/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 44565555.3725 - val_loss: 49286529.6078\n",
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 382us/step - loss: 151757093.5686 - val_loss: 70957379.7647\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 67871070.8627 - val_loss: 61862141.4902\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 63894815.1765 - val_loss: 61375243.7647\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 61641311.6078 - val_loss: 57935817.1373\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 62012130.8235 - val_loss: 57103756.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 59800734.7059 - val_loss: 56564912.3137\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 60006956.5490 - val_loss: 55948337.2157\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 57755411.8824 - val_loss: 60359357.0980\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 57716062.4314 - val_loss: 54043472.9020\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 57476168.1961 - val_loss: 54761479.8824\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56157854.4706 - val_loss: 53319657.5294\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 57338013.6471 - val_loss: 55404265.5686\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 44us/step - loss: 56992183.0588 - val_loss: 63685062.0392\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 54539470.0784 - val_loss: 52979291.9216\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 53820956.0392 - val_loss: 52128031.3725\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 57217298.7059 - val_loss: 51499227.7647\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 55048909.1373 - val_loss: 56651560.5882\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 52751626.8627 - val_loss: 51272351.4118\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 54270295.3333 - val_loss: 51216875.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 53139499.0196 - val_loss: 51139117.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 53823830.9020 - val_loss: 56198084.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 52642337.1373 - val_loss: 62889883.1765\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 55128180.3529 - val_loss: 50137356.9804\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51342403.3725 - val_loss: 53046757.8039\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 51797853.1765 - val_loss: 51158496.9804\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 53197550.6667 - val_loss: 52900605.9608\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 54534866.3529 - val_loss: 49297509.6863\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 51295099.7255 - val_loss: 55421956.7843\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 51136053.2941 - val_loss: 52752438.9020\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53384643.0588 - val_loss: 51365643.0588\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 51183461.0980 - val_loss: 50161552.9804\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 49336292.0784 - val_loss: 48759556.3922\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 50176672.2353 - val_loss: 50918348.9804\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 51741539.3725 - val_loss: 51707609.9608\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 49576216.7059 - val_loss: 48032312.9412\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 49705560.1961 - val_loss: 50478614.3137\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 49251858.2745 - val_loss: 49776391.7647\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 51160394.9804 - val_loss: 63557485.0196\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 50248781.0588 - val_loss: 49277590.4706\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 50462796.0392 - val_loss: 47184744.1569\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 54162722.5098 - val_loss: 48964246.3922\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51591607.2549 - val_loss: 52736171.4510\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 48526353.2941 - val_loss: 47898125.3333\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 48052337.0196 - val_loss: 48471644.7451\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 48213416.7059 - val_loss: 47729563.1373\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 46835706.5098 - val_loss: 47177890.5882\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46518397.1569 - val_loss: 48503151.3725\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 46769006.6275 - val_loss: 48154142.7451\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 46528068.0784 - val_loss: 47068418.4314\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 46231941.0588 - val_loss: 47566160.9412\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46302962.7059 - val_loss: 47462827.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46261804.3137 - val_loss: 47630501.9216\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 46191358.5098 - val_loss: 47050818.6275\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46187244.6471 - val_loss: 47011085.4510\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 46396304.1569 - val_loss: 47043753.9608\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 46094771.8431 - val_loss: 46971362.0392\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 46494449.1765 - val_loss: 48091973.6471\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 46431469.4118 - val_loss: 49104558.5098\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 46637814.8235 - val_loss: 46955717.9216\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 46034756.9020 - val_loss: 47624038.0784\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 46450880.1961 - val_loss: 47731773.2549\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 46031953.6863 - val_loss: 47600178.0392\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 46170376.2353 - val_loss: 46855817.0196\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 45812045.1961 - val_loss: 47896598.4706\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 46339227.8824 - val_loss: 47776822.1569\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 45658733.6078 - val_loss: 48088061.6471\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 45789732.6667 - val_loss: 46777885.7647\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 45762277.4902 - val_loss: 47484349.3333\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45976010.0000 - val_loss: 47209092.5882\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45789620.9412 - val_loss: 47042756.5882\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45759657.9608 - val_loss: 47252992.1961\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45882837.0980 - val_loss: 46776981.0980\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 46104877.4510 - val_loss: 46659085.0588\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45764537.2549 - val_loss: 47689247.4902\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45928546.0196 - val_loss: 47780897.8039\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 45790127.9608 - val_loss: 47356927.8824\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 46116339.0196 - val_loss: 48312437.1765\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45465042.3529 - val_loss: 47669046.4706\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 45499819.0784 - val_loss: 46968777.8431\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 45316989.6863 - val_loss: 46844300.1569\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45294610.3529 - val_loss: 46752345.1373\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45280380.9804 - val_loss: 46827228.5490\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45282040.1961 - val_loss: 46800239.4902\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45253433.5686 - val_loss: 46773332.3922\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 45252868.5098 - val_loss: 46803821.8824\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 45251227.0980 - val_loss: 46785897.3333\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 45245094.3137 - val_loss: 46783865.0980\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45251200.0588 - val_loss: 46799581.2549\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 45239631.1373 - val_loss: 46798862.3137\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 45239845.5294 - val_loss: 46798685.5686\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 45239560.7843 - val_loss: 46799545.1373\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45239449.0588 - val_loss: 46798444.8235\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 45239868.9020 - val_loss: 46797098.4706\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 361us/step - loss: 276728817.4902 - val_loss: 81769053.8824\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 70548733.1765 - val_loss: 74022201.4902\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 64083016.8627 - val_loss: 73795471.8431\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 65638427.9608 - val_loss: 75441180.3137\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 62009262.9020 - val_loss: 64714881.0980\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 35us/step - loss: 62013097.3333 - val_loss: 66318156.3922\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 59893180.6667 - val_loss: 63554323.5294\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 61534086.6667 - val_loss: 59092849.1765\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 60179238.5882 - val_loss: 57949727.8824\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 58988382.9412 - val_loss: 60252445.5686\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 59416167.9216 - val_loss: 56481975.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 59024240.2353 - val_loss: 56084555.0588\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 56868783.3725 - val_loss: 55916652.7059\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 56171920.0000 - val_loss: 55222371.6078\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56759407.8824 - val_loss: 53918906.1176\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 55845213.4902 - val_loss: 54570867.8431\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 56872694.2353 - val_loss: 54398005.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 56009678.0000 - val_loss: 52874918.6667\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 55576945.6471 - val_loss: 53482350.5882\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 55251222.8627 - val_loss: 53558987.5294\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54833988.6275 - val_loss: 55941929.8039\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 54200301.6078 - val_loss: 51774944.5490\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 55083182.9804 - val_loss: 51289205.2157\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 53681563.7647 - val_loss: 54960497.7255\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 57819446.5490 - val_loss: 62782906.3529\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53554073.2549 - val_loss: 50843846.3922\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 55774873.4510 - val_loss: 50782002.1569\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 55072171.4706 - val_loss: 50568431.8039\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 55644650.9412 - val_loss: 57337693.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 53451262.4314 - val_loss: 51620042.5490\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 52621000.7059 - val_loss: 51924797.7647\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 53038456.0000 - val_loss: 53854837.8824\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 53051022.5882 - val_loss: 50313083.2549\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 54692450.7843 - val_loss: 51928049.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 53736774.1176 - val_loss: 50930255.4902\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51749991.0980 - val_loss: 49274942.7451\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 51706702.1961 - val_loss: 52387148.0000\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 51608374.6667 - val_loss: 53754945.7255\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 52661317.1373 - val_loss: 48622475.6078\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 51287937.4118 - val_loss: 49946533.8431\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 53577621.7647 - val_loss: 57127315.0588\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 51285951.8431 - val_loss: 49829124.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 50444889.6471 - val_loss: 49480473.2549\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 50370036.4706 - val_loss: 48036381.0196\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 52049246.1176 - val_loss: 59586267.1373\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 33us/step - loss: 50670565.0588 - val_loss: 50710077.4902\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 30us/step - loss: 51479450.1569 - val_loss: 51346206.3529\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 32us/step - loss: 50537306.5098 - val_loss: 47783653.0196\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 50044558.8627 - val_loss: 48696262.3137\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50698600.4314 - val_loss: 55415683.3725\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 49672549.7647 - val_loss: 52088335.9216\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 50121094.0392 - val_loss: 46928444.0392\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 49246268.1961 - val_loss: 48250927.8431\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 52334587.6863 - val_loss: 47120263.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 49610682.0196 - val_loss: 47250043.8824\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 50060924.5098 - val_loss: 47702532.2745\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 54407495.8824 - val_loss: 47028563.9608\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 48182087.7647 - val_loss: 47693942.0784\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 48021469.6471 - val_loss: 47846618.9804\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 47871769.2157 - val_loss: 48116281.6078\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47862642.7451 - val_loss: 47080237.3333\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 47772614.1176 - val_loss: 47926232.1961\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 47761766.4706 - val_loss: 47214812.0784\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 47638536.7451 - val_loss: 46938378.4314\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 47595872.3922 - val_loss: 46991451.3333\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 47614387.5490 - val_loss: 46935162.7843\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47593375.4902 - val_loss: 46990938.0000\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47574777.2157 - val_loss: 46977351.7255\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 37us/step - loss: 47572563.4510 - val_loss: 46974014.0784\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 47572014.7843 - val_loss: 46962582.7843\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 47572047.4118 - val_loss: 46956230.3529\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 47572665.8824 - val_loss: 46948058.1569\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 442us/step - loss: 165217343.8431 - val_loss: 84400593.9608\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 83623962.0784 - val_loss: 71464753.1765\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 73664486.5882 - val_loss: 68653560.9412\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 68159168.6275 - val_loss: 63151302.5098\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 63510081.4118 - val_loss: 62967203.8431\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 60642448.8627 - val_loss: 59694128.3922\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 59796708.0392 - val_loss: 59267080.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 58586828.1176 - val_loss: 58665242.2353\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 57485611.0196 - val_loss: 56668099.1765\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 55222602.7843 - val_loss: 55916239.6471\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 53891379.4902 - val_loss: 55451842.0000\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 53701932.6275 - val_loss: 55174225.0588\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 54697000.8235 - val_loss: 55930363.5294\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 54264085.5686 - val_loss: 56317873.5294\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 53381672.3922 - val_loss: 54840650.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 51793427.1569 - val_loss: 53575447.2549\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 51673586.2745 - val_loss: 57193372.0000\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 55050727.8824 - val_loss: 55991339.7647\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 50885840.0196 - val_loss: 60631127.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 51048498.3922 - val_loss: 52268165.6078\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49881437.6471 - val_loss: 53462322.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 52433323.6863 - val_loss: 51791701.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 50121147.2941 - val_loss: 55555787.2157\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 49371560.3725 - val_loss: 53671084.8627\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 49283667.8039 - val_loss: 52624595.8824\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 49661682.3137 - val_loss: 56104900.0000\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 53687689.1765 - val_loss: 58113904.0784\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48955089.7647 - val_loss: 50987030.7451\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48021310.1961 - val_loss: 51150142.9804\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 47843012.2745 - val_loss: 51342509.3725\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 79us/step - loss: 47800882.6667 - val_loss: 51207166.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47780209.6863 - val_loss: 51479031.6078\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 47788380.3529 - val_loss: 51222081.5294\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 47684122.0392 - val_loss: 51174582.9412\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47591860.2941 - val_loss: 51173431.1373\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47593294.8235 - val_loss: 51171959.9608\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 47578763.2549 - val_loss: 51172708.1569\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 47598698.7843 - val_loss: 51166660.7843\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47550766.2353 - val_loss: 51165941.7647\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47549047.2549 - val_loss: 51165629.0588\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 47549864.5098 - val_loss: 51166016.1569\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47548702.6667 - val_loss: 51165273.0980\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 47551992.5098 - val_loss: 51165357.1373\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 47545812.0980 - val_loss: 51165258.7059\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 47545466.0000 - val_loss: 51165222.4706\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47545621.6078 - val_loss: 51165142.5098\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47545344.9412 - val_loss: 51165094.5098\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47545288.3922 - val_loss: 51165101.2549\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 430us/step - loss: 98546263.6078 - val_loss: 72421729.8039\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 73018704.0392 - val_loss: 66627666.1961\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 67693964.0000 - val_loss: 72662208.3137\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 65448650.1176 - val_loss: 58256421.9216\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 64814476.4706 - val_loss: 57636051.6471\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 60697368.0000 - val_loss: 56169095.7647\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 59820010.4314 - val_loss: 64399023.3333\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 57264596.2353 - val_loss: 53936605.6078\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 56004599.2157 - val_loss: 53025199.6471\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 56043249.6863 - val_loss: 59037037.8824\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 53896680.2745 - val_loss: 52345772.9804\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 54254152.9412 - val_loss: 62373661.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 56288446.6667 - val_loss: 57669794.0392\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 53737886.0000 - val_loss: 57804530.9804\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 53884126.7059 - val_loss: 73104876.1569\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 54616575.8824 - val_loss: 53425998.6667\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51122934.1569 - val_loss: 51167718.8627\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 50548426.0784 - val_loss: 50213898.5490\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 50443654.5882 - val_loss: 50181491.6471\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 50430755.8039 - val_loss: 51219832.1569\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 50307540.1569 - val_loss: 50379632.5490\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50448305.2549 - val_loss: 50946157.6863\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 50099545.9608 - val_loss: 50032117.6863\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 50175831.2941 - val_loss: 50950686.9804\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50294675.6078 - val_loss: 50411934.3137\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50109648.2745 - val_loss: 50973627.6078\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 50421211.2941 - val_loss: 49968213.6471\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 50160562.1569 - val_loss: 50141293.1765\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49963061.9216 - val_loss: 49808244.6275\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 50106969.7647 - val_loss: 49962493.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49979502.2353 - val_loss: 50680243.5686\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49870324.5882 - val_loss: 49722495.3725\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49899614.5098 - val_loss: 50651212.5882\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 49770260.3922 - val_loss: 50268992.5098\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 50107912.4314 - val_loss: 50444058.3922\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 49772808.0392 - val_loss: 49815472.3529\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 49670399.9216 - val_loss: 49734013.5686\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 49443832.9412 - val_loss: 49886631.3333\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 49449644.1569 - val_loss: 49929191.0980\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49456874.8235 - val_loss: 49919778.0392\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 49428615.6078 - val_loss: 49921685.8431\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49448579.6078 - val_loss: 49982863.4510\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49390062.5098 - val_loss: 49951792.2745\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49382902.7843 - val_loss: 49954297.1373\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 49382879.6078 - val_loss: 49940294.6275\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 49382293.5294 - val_loss: 49934378.1569\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 49381787.2941 - val_loss: 49919954.0784\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377522.1961 - val_loss: 49920364.1961\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377283.6863 - val_loss: 49919114.3137\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49377290.4706 - val_loss: 49918797.2157\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 49377301.8431 - val_loss: 49916739.0196\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 49377242.3529 - val_loss: 49917461.9216\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 466us/step - loss: 112177520.4706 - val_loss: 83372344.3137\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 71842047.6471 - val_loss: 78766532.0784\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 69479413.2941 - val_loss: 69243020.3922\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 70751533.3333 - val_loss: 67551103.8431\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 65150425.7255 - val_loss: 67633144.2353\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 66820310.9020 - val_loss: 73827714.7451\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 64548796.3922 - val_loss: 66327726.6667\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 59350638.3922 - val_loss: 71554019.0588\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 58771430.6275 - val_loss: 57737968.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 57441551.2549 - val_loss: 56972480.6275\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 57469548.5098 - val_loss: 56730714.4314\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 58285306.7451 - val_loss: 55466163.4118\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 56906604.6667 - val_loss: 55172829.2549\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 55205127.4118 - val_loss: 55006370.5882\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 54781046.9020 - val_loss: 52801754.7451\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 58339633.3529 - val_loss: 55172138.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 57206888.0392 - val_loss: 60688200.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 61663206.4706 - val_loss: 53140303.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 54258785.5686 - val_loss: 52769429.0196\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 55069513.4902 - val_loss: 56194934.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 55548910.8627 - val_loss: 50396813.4510\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 52054097.4118 - val_loss: 50778420.8235\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 52410450.6667 - val_loss: 59361402.1961\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 52608247.4118 - val_loss: 49495281.5686\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 52483176.2745 - val_loss: 49255766.0392\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51305113.4510 - val_loss: 48699331.9608\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 52401943.2157 - val_loss: 48217532.5098\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 52595206.3922 - val_loss: 53922205.6471\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 53010363.9216 - val_loss: 47974611.5294\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 50833701.7647 - val_loss: 53847590.8235\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 50469925.7255 - val_loss: 47692848.8235\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 50699573.8431 - val_loss: 47260446.5098\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 55663395.6863 - val_loss: 59998776.0000\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 52969119.9608 - val_loss: 49580857.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50977271.4510 - val_loss: 50283216.3137\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 50517434.9020 - val_loss: 48918739.8824\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 52118240.1961 - val_loss: 54060439.9216\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 49918309.6078 - val_loss: 47639306.7451\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 49062218.3922 - val_loss: 47231805.3725\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49134087.6863 - val_loss: 47914447.8039\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 49189010.6667 - val_loss: 47892338.3922\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49096643.3333 - val_loss: 47126200.1961\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 49082305.2549 - val_loss: 47084206.0784\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 49093563.1373 - val_loss: 47640517.4118\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 48883693.3725 - val_loss: 46951184.3922\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 48906863.1765 - val_loss: 46844313.1765\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 48924788.4706 - val_loss: 48093998.1176\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49306968.3529 - val_loss: 47431369.0980\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 48977516.0784 - val_loss: 46736079.6471\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 49080132.6667 - val_loss: 47171688.1176\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49027625.2157 - val_loss: 47929931.1373\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 48971410.8235 - val_loss: 46722924.0784\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 48939153.8039 - val_loss: 46879219.2941\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48824874.5490 - val_loss: 46764624.0000\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 49103964.3333 - val_loss: 47446510.2745\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 48684386.0784 - val_loss: 46671456.0000\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 49012218.0000 - val_loss: 46523194.1176\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 49261053.2549 - val_loss: 47088597.1373\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48859058.9804 - val_loss: 46956840.0000\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48951141.2941 - val_loss: 46577363.8431\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48606977.3725 - val_loss: 46401222.5098\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 48991735.4902 - val_loss: 47068292.7059\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48707459.7647 - val_loss: 47475101.4510\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49046158.9412 - val_loss: 46507849.0196\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48616365.0980 - val_loss: 46370300.4314\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48670542.3529 - val_loss: 46326393.2941\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48960379.6078 - val_loss: 49245032.5882\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48973579.4510 - val_loss: 46950624.0000\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 48741067.8824 - val_loss: 46549504.6275\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 48614576.8235 - val_loss: 46581603.8824\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48707234.9412 - val_loss: 46234767.2941\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48874011.3333 - val_loss: 46808888.4314\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 48655914.2745 - val_loss: 46196898.4706\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48616577.6863 - val_loss: 46270821.2549\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48754743.2157 - val_loss: 46156820.0000\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48697146.7059 - val_loss: 46142193.4118\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48802484.7843 - val_loss: 46250157.6471\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 49038029.8431 - val_loss: 46080763.4510\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48461611.8824 - val_loss: 46086478.8235\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 48628061.3333 - val_loss: 46367287.4118\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48533904.3529 - val_loss: 46054790.3137\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48695353.7255 - val_loss: 46083341.4510\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48769452.5882 - val_loss: 46396371.4902\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 48466496.7451 - val_loss: 46145324.5882\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 48574766.9412 - val_loss: 46103161.7255\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48627729.4118 - val_loss: 45984489.7647\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 48602595.6078 - val_loss: 46795491.9216\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 48686074.4314 - val_loss: 47294454.5882\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48462661.2941 - val_loss: 45998203.4510\n",
      "Epoch 90/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48332021.6863 - val_loss: 46597420.5882\n",
      "Epoch 91/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48830690.7843 - val_loss: 46513611.1765\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 92/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48267379.4118 - val_loss: 46223622.5882\n",
      "Epoch 93/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 48240227.4902 - val_loss: 46190913.0196\n",
      "Epoch 94/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 48239573.2353 - val_loss: 46203365.0196\n",
      "Epoch 95/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48231709.4118 - val_loss: 46283467.3333\n",
      "Epoch 96/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 48234059.0588 - val_loss: 46167002.5882\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 97/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48206365.1765 - val_loss: 46179169.3333\n",
      "Epoch 98/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 48207276.0000 - val_loss: 46190679.2941\n",
      "Epoch 99/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48206133.2941 - val_loss: 46177492.6667\n",
      "Epoch 100/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 48209548.6667 - val_loss: 46169522.5490\n",
      "Epoch 101/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48206746.3922 - val_loss: 46176804.9412\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 102/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 48203126.1569 - val_loss: 46178531.0588\n",
      "Epoch 103/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48203168.1961 - val_loss: 46179219.3333\n",
      "Epoch 104/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48203687.2549 - val_loss: 46179582.3137\n",
      "Epoch 105/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 48204087.4510 - val_loss: 46177809.0196\n",
      "Epoch 106/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 48203231.2549 - val_loss: 46180651.2157\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 498us/step - loss: 135107947.3333 - val_loss: 61587299.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 58366253.8431 - val_loss: 60369020.0000\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 56600051.1765 - val_loss: 57204077.8431\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 54431024.7059 - val_loss: 57920596.9412\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 53066315.1373 - val_loss: 56251586.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 52317052.6667 - val_loss: 54204907.5686\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 51527718.8235 - val_loss: 55924467.3333\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 51375007.2941 - val_loss: 57133960.3137\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 50547762.5882 - val_loss: 52438415.8431\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 48760802.7059 - val_loss: 52885350.3529\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 48931229.8039 - val_loss: 51706756.8627\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48761950.4314 - val_loss: 51338311.4118\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 47779428.9804 - val_loss: 53826041.6471\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 48677362.1176 - val_loss: 51640131.2157\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 48453549.0980 - val_loss: 51073059.0588\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 46965916.6275 - val_loss: 49596915.1373\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48326496.4314 - val_loss: 49429024.4706\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 46913511.6863 - val_loss: 49236237.0196\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 47600780.3137 - val_loss: 49366979.3333\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 46802539.7451 - val_loss: 49494398.3922\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45949038.9020 - val_loss: 49271534.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 46790378.3922 - val_loss: 48847574.7843\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 46219966.4706 - val_loss: 48826151.8824\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45637438.8235 - val_loss: 49534403.4902\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 45780293.3725 - val_loss: 48395212.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 46969066.4706 - val_loss: 50097193.5294\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 46005415.4510 - val_loss: 54251890.4314\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 46614745.2157 - val_loss: 50298959.2941\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 45589526.4706 - val_loss: 51203094.7843\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 45474250.4706 - val_loss: 50750008.7843\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 44523411.4510 - val_loss: 48153978.1961\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43938631.2157 - val_loss: 47865620.5098\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 43995004.2157 - val_loss: 47820891.5294\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43827933.9216 - val_loss: 48092749.0588\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43982659.4510 - val_loss: 48074759.3725\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 43787381.6471 - val_loss: 47901938.2353\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43894770.0784 - val_loss: 48166186.6275\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 43903402.2353 - val_loss: 47836599.2941\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43629373.8824 - val_loss: 47822145.3725\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 43611262.2353 - val_loss: 47815912.4314\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 43589669.9216 - val_loss: 47823914.9804\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 43576566.0784 - val_loss: 47826686.0000\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43584302.5098 - val_loss: 47816938.5490\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 43571306.5098 - val_loss: 47822410.7843\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 43571777.5294 - val_loss: 47811437.6863\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43574222.8627 - val_loss: 47801278.8627\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 43581656.9804 - val_loss: 47826523.1373\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 43566348.8627 - val_loss: 47816326.6667\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 43560869.0196 - val_loss: 47807132.9412\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 43558766.8235 - val_loss: 47806459.3333\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43556774.0784 - val_loss: 47808474.2745\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 43538596.9216 - val_loss: 47806346.2353\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 43538870.6667 - val_loss: 47806668.5490\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43539003.6275 - val_loss: 47806951.3725\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43536263.2157 - val_loss: 47805741.3333\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 43536858.2353 - val_loss: 47804171.3725\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 43535128.0392 - val_loss: 47804181.6078\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 43534881.6078 - val_loss: 47804264.7451\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 43535333.4510 - val_loss: 47804413.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 43534810.6275 - val_loss: 47804213.6863\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43534903.8039 - val_loss: 47804344.9804\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 43534665.6471 - val_loss: 47804335.3725\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43534661.1765 - val_loss: 47804337.8039\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 43534667.5686 - val_loss: 47804338.3922\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 43534694.8235 - val_loss: 47804327.3725\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 43534688.9412 - val_loss: 47804330.9412\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_5 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 570us/step - loss: 125641743.9216 - val_loss: 64204435.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 63912883.4902 - val_loss: 57060183.6471\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 60340580.7843 - val_loss: 54601624.8235\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 58024927.0980 - val_loss: 53414166.9412\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 55444794.1176 - val_loss: 53875682.9020\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 54387859.9216 - val_loss: 51854104.5098\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 52574276.0784 - val_loss: 53970638.7843\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 53431876.9412 - val_loss: 50655439.7647\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 52611817.7647 - val_loss: 49962203.1765\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 51299453.7843 - val_loss: 50572598.0784\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 51789603.5294 - val_loss: 64218363.9216\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 52220955.4118 - val_loss: 52747463.4510\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50757355.2157 - val_loss: 48894541.4118\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 50249486.3529 - val_loss: 50057807.6078\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49834011.7255 - val_loss: 48357276.9020\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 48528842.6667 - val_loss: 49402103.9608\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 48394154.8235 - val_loss: 47578165.2157\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 48887472.7059 - val_loss: 50737506.2353\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 47776602.1176 - val_loss: 47976153.9608\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 49169859.6078 - val_loss: 48885811.2157\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 49335080.7843 - val_loss: 49227391.2549\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48640694.5098 - val_loss: 47307896.3137\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 47531245.8431 - val_loss: 48250952.5882\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 48043083.1961 - val_loss: 48423147.7647\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 49636391.8039 - val_loss: 51749091.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 48742329.0196 - val_loss: 47436688.6667\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 48500362.2745 - val_loss: 48420755.9608\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 46446771.8824 - val_loss: 47766405.7255\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 46073627.3725 - val_loss: 48192164.1569\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45902918.9020 - val_loss: 47368870.3529\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45968657.6078 - val_loss: 47978443.8824\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 46092370.5490 - val_loss: 47110898.7059\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 69us/step - loss: 45788049.3922 - val_loss: 47280965.8824\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45803838.0000 - val_loss: 47836380.6275\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 45657178.8235 - val_loss: 47061856.2353\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 45445105.6471 - val_loss: 46771180.7843\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 45580466.5882 - val_loss: 47458668.2353\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 45246024.3922 - val_loss: 47448523.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 45325658.0980 - val_loss: 46785121.8039\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 45163383.1373 - val_loss: 47785749.0196\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 45177750.7451 - val_loss: 46697551.1765\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45392294.8431 - val_loss: 47014961.1765\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 45182767.2157 - val_loss: 47089567.9216\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 45173457.6863 - val_loss: 47207206.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 65us/step - loss: 45176216.4314 - val_loss: 46991779.8039\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45299936.2353 - val_loss: 46754582.8627\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 44799785.2941 - val_loss: 46850552.0784\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44765610.0392 - val_loss: 46872336.2745\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 44745228.3922 - val_loss: 46923762.1569\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 44733547.4118 - val_loss: 46876903.6863\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44735923.8824 - val_loss: 46916216.5098\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 44694307.8431 - val_loss: 46913535.0588\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 44696476.3137 - val_loss: 46909375.6471\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44692743.6078 - val_loss: 46916496.5882\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 44691871.6471 - val_loss: 46916425.8039\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 44691506.9412 - val_loss: 46912952.7843\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44688221.1765 - val_loss: 46912703.2549\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 44687709.3725 - val_loss: 46912748.6275\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 74us/step - loss: 44687588.2353 - val_loss: 46913844.1569\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 44687593.4118 - val_loss: 46913207.2941\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 44687534.3922 - val_loss: 46913751.6471\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_6 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 1s 572us/step - loss: 135934471.2157 - val_loss: 66575241.5686\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 61455651.8039 - val_loss: 61285399.5294\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 59179938.1961 - val_loss: 58539831.4510\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 57825424.4706 - val_loss: 56256853.6471\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 57041693.2941 - val_loss: 54654387.4118\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 56510542.3529 - val_loss: 60888682.1176\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 53987605.5686 - val_loss: 51393261.3725\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 52894770.3922 - val_loss: 51074179.5686\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 52717993.8824 - val_loss: 50120949.9216\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 53722502.9412 - val_loss: 50941562.1961\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 50705880.3137 - val_loss: 48453647.7647\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 51416021.6078 - val_loss: 53195233.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 51477060.4314 - val_loss: 49976348.3529\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 50766060.3529 - val_loss: 47798348.4706\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 51728691.0196 - val_loss: 48626210.5490\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 49648431.2157 - val_loss: 47034602.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 49816523.4118 - val_loss: 48091201.4510\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 49046421.9608 - val_loss: 46441505.6863\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 50337202.0392 - val_loss: 47165662.4314\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 49831516.1176 - val_loss: 46989671.2549\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 50161551.2941 - val_loss: 46520168.0000\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48769195.4902 - val_loss: 47084317.4902\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 48328651.4510 - val_loss: 45715494.0392\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 48160046.3137 - val_loss: 47773586.9412\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48545141.1765 - val_loss: 45397799.6078\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 48486227.7647 - val_loss: 45231941.6471\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 48475065.3333 - val_loss: 46204745.9216\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 49009174.1569 - val_loss: 46313583.7255\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 49808304.1176 - val_loss: 45114554.5490\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47970048.6667 - val_loss: 46475199.4118\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 48247038.3529 - val_loss: 48360086.0392\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 47671535.2941 - val_loss: 44860403.0196\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 49019351.0588 - val_loss: 44702093.9216\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 48409679.0588 - val_loss: 49784610.1961\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 48216292.7059 - val_loss: 44476775.4902\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 47667667.2941 - val_loss: 44295538.5098\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 47940237.2941 - val_loss: 44102151.2157\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 47965438.5098 - val_loss: 45793064.7843\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 47041545.3333 - val_loss: 44186354.1569\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 46843725.7255 - val_loss: 43871218.3922\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 47761441.1373 - val_loss: 43752835.7647\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 47430273.0980 - val_loss: 49605499.9216\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 47456168.2353 - val_loss: 44452847.8039\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 48863348.2353 - val_loss: 46112301.8431\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 47583905.8039 - val_loss: 43868986.5098\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 47602143.8431 - val_loss: 44190301.6471\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 45967912.6863 - val_loss: 43466560.5490\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45942086.9804 - val_loss: 43686691.1765\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45735999.0784 - val_loss: 43486369.4902\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 45876465.7647 - val_loss: 43582003.8431\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 45713601.4902 - val_loss: 43551454.6667\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 45661354.1961 - val_loss: 43723714.7843\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45465351.6863 - val_loss: 43681683.6471\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 45457434.8627 - val_loss: 43661130.4706\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45448497.7255 - val_loss: 43592494.3529\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45442147.1765 - val_loss: 43600354.2353\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 45439914.1569 - val_loss: 43605647.4118\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 71us/step - loss: 45421595.5686 - val_loss: 43606217.3725\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 45421051.4314 - val_loss: 43606852.3137\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 45421118.4314 - val_loss: 43608980.7059\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 45420063.5686 - val_loss: 43607487.4510\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 45420242.0784 - val_loss: 43608722.7059\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 45418028.0392 - val_loss: 43608522.5098\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 45417923.8039 - val_loss: 43608717.6863\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 45417971.5686 - val_loss: 43608569.2157\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 45417885.4118 - val_loss: 43608751.6078\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 45417795.4902 - val_loss: 43608504.3529\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 803us/step - loss: 408406877.1765 - val_loss: 330579964.8627\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 176625718.9804 - val_loss: 105918374.9020\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 87703652.7843 - val_loss: 80171990.6667\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 74441336.7843 - val_loss: 67866599.2157\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 68895494.1961 - val_loss: 66686903.6078\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 69493453.4902 - val_loss: 65580878.4314\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 67612451.2941 - val_loss: 64551671.0980\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 66215928.8627 - val_loss: 64067747.9608\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 65863897.0980 - val_loss: 62847888.7843\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 64941014.8235 - val_loss: 62901889.0196\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 62259569.5686 - val_loss: 62362962.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 61644691.4118 - val_loss: 60412629.3333\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 63018241.6863 - val_loss: 61937565.3333\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 62836202.5882 - val_loss: 60910957.2549\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 61655469.2157 - val_loss: 60147343.0588\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 60550575.4118 - val_loss: 59509267.9216\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 59775681.3333 - val_loss: 59039732.7059\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 59422375.7647 - val_loss: 59177702.1961\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 220us/step - loss: 58315011.2157 - val_loss: 58325984.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 57920038.6667 - val_loss: 59410970.9412\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 57375573.7255 - val_loss: 57918774.5098\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 57017707.4118 - val_loss: 57676520.1961\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 56603039.0980 - val_loss: 57554893.6471\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 56276392.1176 - val_loss: 57497150.1569\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 56236604.2353 - val_loss: 58086852.3529\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 56029626.0784 - val_loss: 57715478.1569\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 55814287.1373 - val_loss: 57904713.6078\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 55598159.3725 - val_loss: 57493231.4118\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 55559351.5686 - val_loss: 57479570.7059\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 55448974.5490 - val_loss: 57396423.6078\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 55281905.6471 - val_loss: 57525621.2157\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 55343694.1176 - val_loss: 57451067.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 55153882.1176 - val_loss: 57448736.4706\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 55137096.9412 - val_loss: 57892781.8039\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 55201001.7255 - val_loss: 57426546.9020\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 225us/step - loss: 54912784.9412 - val_loss: 57379670.1569\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 54900455.6078 - val_loss: 57417727.7647\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 54833186.6667 - val_loss: 57401098.5882\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 54855909.2941 - val_loss: 57386017.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 54813975.0980 - val_loss: 57400250.8627\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 54806532.4706 - val_loss: 57402077.6078\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 54783870.7059 - val_loss: 57406543.1373\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 54783288.7059 - val_loss: 57402976.5882\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 54782661.4510 - val_loss: 57403206.8627\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 54784368.8235 - val_loss: 57401684.0000\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 54780646.3922 - val_loss: 57403912.4706\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 54778183.3333 - val_loss: 57403573.0588\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54778081.5294 - val_loss: 57403470.0000\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 54778231.4902 - val_loss: 57403675.0196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 54777995.0196 - val_loss: 57403462.5490\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 54778074.7843 - val_loss: 57403705.8039\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 54777707.8039 - val_loss: 57403644.9020\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 54777705.0980 - val_loss: 57403666.5098\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 54777672.3137 - val_loss: 57403636.7059\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 1s 236us/step - loss: 54777659.2941 - val_loss: 57403640.9020\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 1s 237us/step - loss: 54777728.7843 - val_loss: 57403655.0588\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 861us/step - loss: 139083540.7059 - val_loss: 101066495.6078\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 90960170.0392 - val_loss: 84202164.7059\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 94417585.6471 - val_loss: 97304843.6078\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 94362411.8431 - val_loss: 84264233.2549\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 87711525.0980 - val_loss: 87513833.4118\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 101887308.3922 - val_loss: 100340000.0784\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 93326950.9804 - val_loss: 156646870.7451\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 109186101.8039 - val_loss: 81537220.3137\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 83788696.9412 - val_loss: 83120527.9216\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 83609336.0000 - val_loss: 81975041.5686\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 82880653.3333 - val_loss: 82160670.1176\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 83500915.0588 - val_loss: 82762237.8039\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 83345812.0784 - val_loss: 81593199.0588\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 82652438.5882 - val_loss: 81698905.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 82565064.6275 - val_loss: 81204793.3333\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 82409156.4706 - val_loss: 80537052.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 82071935.2157 - val_loss: 80486923.5294\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 177us/step - loss: 81896907.8431 - val_loss: 80437850.7451\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 81783184.3137 - val_loss: 80364113.3333\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 81700099.6078 - val_loss: 80345113.4118\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 81623148.0000 - val_loss: 80126009.1765\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 81588619.1373 - val_loss: 80278232.6275\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 81554580.6275 - val_loss: 80064336.6275\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 81413678.1176 - val_loss: 80082529.0980\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 81436060.1569 - val_loss: 79846524.5490\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 81257651.3725 - val_loss: 79833409.4118\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 81201571.2941 - val_loss: 79782882.3529\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 81077331.2941 - val_loss: 79726791.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 81061062.0392 - val_loss: 79548710.1176\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 80982312.0784 - val_loss: 79549138.5098\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 80879493.9608 - val_loss: 79387090.8235\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 81223565.2549 - val_loss: 79819085.7255\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 81217032.6275 - val_loss: 79497402.3529\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 170us/step - loss: 81215977.6471 - val_loss: 79407343.2941\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 81114026.5490 - val_loss: 79314685.6471\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 81072677.1765 - val_loss: 79246695.8431\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 80879628.6275 - val_loss: 79127285.6471\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 80749249.8824 - val_loss: 78874171.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 80501394.1176 - val_loss: 78103921.2549\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 79650495.7647 - val_loss: 78322909.7255\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 79368311.6078 - val_loss: 78054956.7843\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 79404527.2941 - val_loss: 77696916.0000\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 79418160.7059 - val_loss: 77629908.3137\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 79071427.8431 - val_loss: 77411155.6863\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 79108691.0588 - val_loss: 77312531.9216\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 78990389.1373 - val_loss: 77161626.9804\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 78937880.3137 - val_loss: 77132547.6078\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 78516160.0784 - val_loss: 77044186.1961\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 78373688.5490 - val_loss: 76873655.2941\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 78175521.2549 - val_loss: 76612458.5882\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 78021499.3725 - val_loss: 76174802.4314\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 78920252.0784 - val_loss: 76257873.1765\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 78616337.8824 - val_loss: 75892014.7451\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 78449772.6275 - val_loss: 75834417.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 78415107.6078 - val_loss: 75851621.4902\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 78670341.0980 - val_loss: 75585469.8039\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 78658154.8235 - val_loss: 75348335.4510\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 78455056.6275 - val_loss: 75185437.6471\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 78219979.6863 - val_loss: 74704822.6667\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 77721228.2353 - val_loss: 74599596.3137\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 77355327.7255 - val_loss: 74196411.9216\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 77047023.5294 - val_loss: 74293419.0588\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76827133.0196 - val_loss: 73585150.9020\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 76629590.2745 - val_loss: 73501242.6667\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 171us/step - loss: 76322841.4902 - val_loss: 73725030.9020\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76218806.6667 - val_loss: 73422946.5882\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76253538.7451 - val_loss: 73256221.5686\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 76021944.6275 - val_loss: 72590311.8431\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 75144247.1373 - val_loss: 72472967.7647\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 172us/step - loss: 76108455.5294 - val_loss: 72934218.9804\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 175us/step - loss: 76531383.5294 - val_loss: 73211042.9804\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 75715773.4118 - val_loss: 73497461.7255\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 76059376.9412 - val_loss: 72864324.2353\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 76154595.7647 - val_loss: 74325104.3137\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 77092613.6471 - val_loss: 74186009.8824\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 77157161.9608 - val_loss: 74162556.1569\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 179us/step - loss: 77193709.2941 - val_loss: 74150208.4706\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 76612367.6863 - val_loss: 73928845.9608\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 76760293.0980 - val_loss: 74125651.9216\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 76640489.7255 - val_loss: 74073074.4314\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76586955.0980 - val_loss: 73934083.3725\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 76495984.8627 - val_loss: 73703491.8431\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 76474943.0980 - val_loss: 73714956.7843\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 76471710.8627 - val_loss: 73728434.9804\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 85/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76468051.1765 - val_loss: 73728492.6275\n",
      "Epoch 86/500\n",
      "2550/2550 [==============================] - 0s 173us/step - loss: 76467867.4510 - val_loss: 73728570.5098\n",
      "Epoch 87/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 76467692.7843 - val_loss: 73728675.2157\n",
      "Epoch 88/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 76467500.7843 - val_loss: 73728705.9608\n",
      "Epoch 89/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 76467226.5882 - val_loss: 73727949.4118\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 888us/step - loss: 407818732.1569 - val_loss: 84287360.5490\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 86359596.4706 - val_loss: 86022396.5490\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 81100528.9804 - val_loss: 89264327.2157\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 79311016.4706 - val_loss: 81830569.8039\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 72932488.2353 - val_loss: 109929820.1569\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 85539305.4118 - val_loss: 72436177.0196\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 73990634.5882 - val_loss: 78021975.7647\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 85590584.9412 - val_loss: 96539016.8627\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 82962549.4118 - val_loss: 76263789.7255\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 75217991.9216 - val_loss: 92012661.4118\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 83230637.7255 - val_loss: 84491628.3137\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 74339625.9608 - val_loss: 77104870.9804\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 70236457.3333 - val_loss: 73461957.9608\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 69543429.1765 - val_loss: 71768301.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 69589895.0588 - val_loss: 72369781.2549\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 66925986.5882 - val_loss: 72702940.3922\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 67391236.7843 - val_loss: 69574926.0392\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 69448319.4902 - val_loss: 74048475.3725\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 233us/step - loss: 68703133.5294 - val_loss: 72856931.7647\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 69761713.9608 - val_loss: 71565868.0000\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 67662095.6078 - val_loss: 70565393.0980\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 65849523.2157 - val_loss: 67847813.4118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 214us/step - loss: 66199910.1961 - val_loss: 68475224.0000\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 66019234.3529 - val_loss: 76649917.0980\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 1s 229us/step - loss: 68203649.6078 - val_loss: 73847783.9216\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67620943.7647 - val_loss: 73553070.0392\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 66418981.2157 - val_loss: 73672645.2549\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 66461178.5490 - val_loss: 73571439.3725\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 66473608.7451 - val_loss: 73604530.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 66288687.2549 - val_loss: 73560907.2157\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 66360777.9608 - val_loss: 73568689.0980\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 67132641.8039 - val_loss: 75339938.5882\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 67383825.0588 - val_loss: 75308078.8235\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 67344773.1765 - val_loss: 75291917.4118\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 67296491.7647 - val_loss: 75285267.7647\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 67297557.0980 - val_loss: 75281898.5882\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 67279775.3333 - val_loss: 75406158.5882\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67310883.7255 - val_loss: 75409461.4118\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 67300471.2157 - val_loss: 75417112.0784\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 67264503.3725 - val_loss: 75412434.0392\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 67307875.0980 - val_loss: 75394145.8039\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 67312908.5098 - val_loss: 75393130.9020\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "%%%%%%%%%%%%%%%%%%%% start experiments with lead time 10 %%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/statsmodels/base/model.py:488: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6192.628560298074, 10312.562536357069, 0.5224473594907217)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 645us/step - loss: 129550009.4118 - val_loss: 99495640.6275\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 98134998.3529 - val_loss: 92968206.3529\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 90256816.4706 - val_loss: 91541754.7451\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 40us/step - loss: 88982446.8235 - val_loss: 90995416.2353\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 88114691.7647 - val_loss: 91354018.2745\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 91738552.5490 - val_loss: 88420681.7255\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 87561482.1176 - val_loss: 88855851.1373\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 84303811.6863 - val_loss: 89767539.9216\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 83410492.7843 - val_loss: 90779699.7647\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 89833889.2549 - val_loss: 90143073.4902\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 86347796.6275 - val_loss: 90395379.3725\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 81959799.2941 - val_loss: 87299119.8431\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 81179050.9020 - val_loss: 87005365.0980\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 81455814.0392 - val_loss: 87220224.7059\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 81448616.1569 - val_loss: 87253125.9608\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 81398455.5294 - val_loss: 88250805.0196\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 81175592.7843 - val_loss: 87574135.7647\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80760185.4902 - val_loss: 87954282.5098\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80495236.7059 - val_loss: 87530472.4706\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80450902.6667 - val_loss: 87347827.7647\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80515136.9412 - val_loss: 87427861.8824\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80468603.7647 - val_loss: 87293897.2549\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80445585.3333 - val_loss: 87366490.1176\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 80374535.0196 - val_loss: 87373447.3725\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80370263.7647 - val_loss: 87363750.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 80373227.0196 - val_loss: 87361199.1373\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 80368588.4706 - val_loss: 87353626.1961\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 80369237.4902 - val_loss: 87357696.0784\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 38us/step - loss: 80364662.5098 - val_loss: 87357854.5882\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 39us/step - loss: 80364345.1765 - val_loss: 87357424.8627\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 80364502.3137 - val_loss: 87357686.1176\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 80364893.9608 - val_loss: 87357798.5882\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80363974.4314 - val_loss: 87356959.1373\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_14 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 607us/step - loss: 138067072.0784 - val_loss: 93219646.7451\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 94102016.9412 - val_loss: 88702702.4314\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 90449829.8824 - val_loss: 85796794.9020\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 89571938.0000 - val_loss: 92714074.1176\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88725180.0000 - val_loss: 83604334.1961\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 86602280.0000 - val_loss: 84363908.0000\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 86946841.6471 - val_loss: 82788836.4706\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88931565.1765 - val_loss: 83116589.4902\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 86226602.2745 - val_loss: 80699460.5490\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 86448478.5098 - val_loss: 84814490.9804\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 34us/step - loss: 85236301.9608 - val_loss: 88702562.8235\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 36us/step - loss: 88294816.2353 - val_loss: 80184527.6078\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 88243253.0196 - val_loss: 80077387.7647\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 91310277.7255 - val_loss: 80443004.3922\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 87483000.7843 - val_loss: 81141403.8431\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 84265031.6078 - val_loss: 80700778.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 42us/step - loss: 85096788.0000 - val_loss: 80202441.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 82929891.1765 - val_loss: 79283896.0784\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 88177396.8627 - val_loss: 86456261.8824\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 83735149.0980 - val_loss: 80654156.1569\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 84341760.4314 - val_loss: 85687634.7451\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 85556914.9020 - val_loss: 79100888.0784\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 85977282.1961 - val_loss: 78771377.4902\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 83122513.0196 - val_loss: 79812066.3529\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 82884661.3333 - val_loss: 79764611.5294\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 81758795.0588 - val_loss: 79976357.4510\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 81750564.0784 - val_loss: 81274667.7647\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 83614731.6078 - val_loss: 78385813.1765\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 86102224.8627 - val_loss: 82884894.8235\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 83560610.9804 - val_loss: 80457412.0784\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 82437798.5098 - val_loss: 87879963.5294\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 83004511.0588 - val_loss: 84534624.2353\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 90313474.5882 - val_loss: 97483716.6275\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 83584611.6863 - val_loss: 78747939.2157\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80147246.1961 - val_loss: 78279414.2745\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80156686.7451 - val_loss: 78537445.7255\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 79897701.4118 - val_loss: 78216195.0588\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80370300.5490 - val_loss: 78259778.2745\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80009145.2549 - val_loss: 80427195.1373\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 80116912.8627 - val_loss: 79600952.4706\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79805487.7647 - val_loss: 78157663.2941\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80280861.8431 - val_loss: 78853164.3137\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79790864.0000 - val_loss: 78492382.2745\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79504939.1373 - val_loss: 77941066.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80103775.0588 - val_loss: 79048778.9020\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79768708.7843 - val_loss: 77909061.9608\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 79703422.9020 - val_loss: 77817250.6667\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79533502.4314 - val_loss: 77929229.1765\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79951410.3529 - val_loss: 77776489.6471\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79972625.0980 - val_loss: 77818996.7843\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80470221.0980 - val_loss: 78866188.3137\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 79772701.0196 - val_loss: 77751528.7843\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79574327.8431 - val_loss: 78074043.3725\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79427181.6471 - val_loss: 78075289.2549\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79807029.8824 - val_loss: 78058161.3333\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 64us/step - loss: 79447639.4510 - val_loss: 81016375.1373\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 79792455.1373 - val_loss: 78185400.3137\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79116074.8235 - val_loss: 78122110.0392\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79100494.0392 - val_loss: 78033385.8824\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79083099.2941 - val_loss: 78100056.7059\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 79076192.3137 - val_loss: 78165730.2745\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 79079130.3529 - val_loss: 78290868.7059\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79046107.6863 - val_loss: 78266299.5294\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79042179.0588 - val_loss: 78249405.3333\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79046965.8039 - val_loss: 78235384.3922\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79043170.1961 - val_loss: 78212835.4510\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79039597.8039 - val_loss: 78216179.9216\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 79034680.7843 - val_loss: 78214424.7843\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79034330.0392 - val_loss: 78212561.0196\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79034354.3137 - val_loss: 78212347.8431\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79034057.2549 - val_loss: 78212134.5882\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79033937.0980 - val_loss: 78211475.6078\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_15 (Flatten)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               1600      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 676us/step - loss: 177828179.1373 - val_loss: 108278679.8431\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 96620132.1569 - val_loss: 98444283.8431\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 93657493.1765 - val_loss: 94437622.3529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 91575059.0588 - val_loss: 90086436.1569\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 88035441.8824 - val_loss: 87349420.3922\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 90900295.5294 - val_loss: 88765748.3137\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 88301652.0000 - val_loss: 91687782.9804\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 87814998.0392 - val_loss: 84875037.4118\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 85626908.2353 - val_loss: 86908075.1373\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 85481545.6471 - val_loss: 83729763.1373\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 85738219.0588 - val_loss: 84232057.0196\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 45us/step - loss: 84545669.2549 - val_loss: 82471869.4902\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 41us/step - loss: 87467663.6863 - val_loss: 88991914.2745\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 84955505.0196 - val_loss: 82688775.7647\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 85225960.7059 - val_loss: 82012998.5882\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 85851717.4118 - val_loss: 84017847.9216\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 83185571.4510 - val_loss: 81579376.1569\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 85274928.2353 - val_loss: 83694367.2157\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 84757241.0980 - val_loss: 81623691.8431\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 85231193.4902 - val_loss: 86030260.6275\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 83415727.5686 - val_loss: 87197192.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 83692735.6863 - val_loss: 83846058.8235\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 48us/step - loss: 82670835.7647 - val_loss: 80950463.9216\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80841676.0784 - val_loss: 81678050.3529\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 81031191.2549 - val_loss: 80613401.4902\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 81250499.1373 - val_loss: 80769238.1176\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 81175370.1176 - val_loss: 80462780.8627\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80710519.0588 - val_loss: 80599252.9412\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 80806574.9804 - val_loss: 80827862.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80465333.8039 - val_loss: 80283076.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 80777164.4706 - val_loss: 80191216.0784\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80747791.0588 - val_loss: 80853895.5294\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80532755.0980 - val_loss: 80290084.5490\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 80624005.6471 - val_loss: 80006581.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 80144530.7451 - val_loss: 81015853.8824\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 62us/step - loss: 80328092.3922 - val_loss: 80064687.1373\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80408314.1961 - val_loss: 79890460.5490\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 80522155.4510 - val_loss: 82750018.8235\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 57us/step - loss: 80764850.4706 - val_loss: 81657626.1961\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80316342.5882 - val_loss: 79803168.0784\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 81326376.6275 - val_loss: 79766894.1176\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 80295816.8627 - val_loss: 79734844.4706\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79686095.1373 - val_loss: 82948073.2549\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 80814731.8431 - val_loss: 80143386.9020\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 58us/step - loss: 79999822.3529 - val_loss: 80231850.2745\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 80322903.4510 - val_loss: 79655573.0980\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80586450.9020 - val_loss: 79599043.7647\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79973390.6667 - val_loss: 79735950.0392\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 80026362.8235 - val_loss: 79659672.7843\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 47us/step - loss: 80128099.2941 - val_loss: 80433641.2549\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 53us/step - loss: 80034679.1373 - val_loss: 79640093.5686\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 80148740.8627 - val_loss: 79537526.0392\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 61us/step - loss: 79938273.8431 - val_loss: 79657356.4706\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79698484.5882 - val_loss: 80141143.0588\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79998218.4314 - val_loss: 79958338.4314\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79685359.9216 - val_loss: 79856695.2941\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79841710.1961 - val_loss: 79435526.4314\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 46us/step - loss: 79721904.8627 - val_loss: 79610653.1765\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 44us/step - loss: 79395358.5098 - val_loss: 79489905.9608\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 55us/step - loss: 79907797.1765 - val_loss: 80220077.3333\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79545641.0196 - val_loss: 79737851.9216\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79979598.2353 - val_loss: 79694516.1569\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79223600.2353 - val_loss: 79551381.4118\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79219850.7059 - val_loss: 79415923.2941\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 56us/step - loss: 79225521.7255 - val_loss: 79492626.9804\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79204986.1961 - val_loss: 79496670.4314\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 43us/step - loss: 79167170.2745 - val_loss: 79439071.2941\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79200623.8431 - val_loss: 79487139.6863\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79166931.5294 - val_loss: 79512208.3137\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79122979.4118 - val_loss: 79502056.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 54us/step - loss: 79126159.2157 - val_loss: 79503902.5882\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79122773.0980 - val_loss: 79481922.5098\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79122201.0980 - val_loss: 79478582.7451\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79121976.5490 - val_loss: 79478598.8235\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 63us/step - loss: 79116648.0784 - val_loss: 79479264.5490\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 60us/step - loss: 79116033.0980 - val_loss: 79478138.9020\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 59us/step - loss: 79116763.9216 - val_loss: 79475677.0196\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 49us/step - loss: 79116260.0784 - val_loss: 79477647.5294\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79116081.0196 - val_loss: 79476814.1961\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 80/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79115453.3333 - val_loss: 79476804.0000\n",
      "Epoch 81/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79115475.9216 - val_loss: 79476734.0392\n",
      "Epoch 82/500\n",
      "2550/2550 [==============================] - 0s 52us/step - loss: 79115464.0000 - val_loss: 79476730.9804\n",
      "Epoch 83/500\n",
      "2550/2550 [==============================] - 0s 51us/step - loss: 79115456.0000 - val_loss: 79476670.1176\n",
      "Epoch 84/500\n",
      "2550/2550 [==============================] - 0s 50us/step - loss: 79115454.8235 - val_loss: 79476650.5882\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 770us/step - loss: 147814872.9412 - val_loss: 104910030.0392\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 101801252.3137 - val_loss: 95499301.1765\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 98084067.6078 - val_loss: 101566383.7647\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 91951195.2157 - val_loss: 91306630.5098\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 93254251.9216 - val_loss: 98008458.9804\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 91454585.4118 - val_loss: 92867548.1569\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 91041861.8824 - val_loss: 89055946.5098\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 86029199.8431 - val_loss: 89154607.8431\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 84510642.7451 - val_loss: 87908942.6667\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 85076673.2549 - val_loss: 91911061.3333\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87322138.8627 - val_loss: 89199871.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 85779492.0000 - val_loss: 87332140.0000\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 82583806.5882 - val_loss: 99397982.9804\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 83597587.2157 - val_loss: 91053220.5490\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 81447796.3137 - val_loss: 88992593.6471\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 82076597.6471 - val_loss: 85470812.9412\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 81056806.6667 - val_loss: 84720660.0784\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 81831108.6275 - val_loss: 87456941.2549\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 80749167.9216 - val_loss: 85335524.7059\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 80207553.7255 - val_loss: 90414273.4902\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 80967308.2353 - val_loss: 83770296.9412\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 79748518.6667 - val_loss: 84464642.5882\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 80179803.2941 - val_loss: 84768857.7255\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79405210.0392 - val_loss: 83784521.9608\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81089462.4706 - val_loss: 90873095.4510\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79203590.9020 - val_loss: 95962429.7255\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 81615975.6863 - val_loss: 84184809.8824\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 77913689.3333 - val_loss: 84480177.3333\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77649108.2353 - val_loss: 83596183.5294\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 78209102.3922 - val_loss: 83471180.2353\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77873463.6078 - val_loss: 83305678.9804\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77534016.7059 - val_loss: 83253344.0000\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77450999.0980 - val_loss: 84445648.6275\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 77819812.8627 - val_loss: 84256879.8431\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77550548.0784 - val_loss: 83512638.7451\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 77372438.0784 - val_loss: 83311713.6471\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 77321942.9020 - val_loss: 83956902.5882\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 77213559.0588 - val_loss: 83554334.5098\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 77117887.6078 - val_loss: 83459966.1961\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 77131872.3922 - val_loss: 83409116.0784\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77130818.8235 - val_loss: 83403614.7451\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 77102888.0000 - val_loss: 83420005.4118\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77088213.1765 - val_loss: 83412329.8824\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 77087506.4314 - val_loss: 83420091.2157\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 77092682.5882 - val_loss: 83414766.5882\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 77085386.5882 - val_loss: 83412335.6863\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 77085087.2157 - val_loss: 83410927.9216\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77082843.2941 - val_loss: 83410113.3333\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 77082937.5686 - val_loss: 83410424.3922\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 77083222.9020 - val_loss: 83410696.9412\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 77083136.0784 - val_loss: 83409355.2941\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77082953.1765 - val_loss: 83408465.7255\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 829us/step - loss: 166266639.2157 - val_loss: 109397458.7451\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 105532747.1373 - val_loss: 98062659.1373\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 98468983.8431 - val_loss: 88971025.8039\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 97485288.1569 - val_loss: 89355689.2549\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 93525733.0980 - val_loss: 86592620.8627\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 91788554.8235 - val_loss: 86850713.2549\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 91090018.6667 - val_loss: 84181775.6863\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 91275203.7647 - val_loss: 83142132.7843\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 86936087.1373 - val_loss: 82178061.2549\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 91321144.9412 - val_loss: 82035040.7843\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 91274281.3333 - val_loss: 81522157.2549\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 87835145.1765 - val_loss: 81227651.2941\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 90336644.4706 - val_loss: 85876499.0588\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 87607158.2745 - val_loss: 80491073.1765\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 85016595.2157 - val_loss: 80586192.2745\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 84736041.8824 - val_loss: 79943913.0980\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 83618715.0196 - val_loss: 81479631.2941\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 85221384.8627 - val_loss: 92297020.1569\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 89824539.0588 - val_loss: 79431118.5098\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 83374945.7255 - val_loss: 79316267.1765\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 85614398.6667 - val_loss: 78911228.6275\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 83446390.5098 - val_loss: 79186047.0588\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 89826365.6471 - val_loss: 79279774.9020\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 84740324.4706 - val_loss: 79774417.1373\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 82004445.1765 - val_loss: 81238474.5098\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 82245379.3725 - val_loss: 78592416.7843\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 83954387.9216 - val_loss: 78862664.3922\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 84968783.5294 - val_loss: 86687194.1961\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 83450491.3725 - val_loss: 83841149.5686\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 81884731.2941 - val_loss: 79832473.3333\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 82596822.1176 - val_loss: 86505195.5294\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 80959724.1569 - val_loss: 78243393.4118\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 80260545.3333 - val_loss: 78125051.4510\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 80327407.1373 - val_loss: 78200883.1373\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 80066628.4706 - val_loss: 78837704.7843\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 80163800.3137 - val_loss: 79003818.7451\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 80043575.2549 - val_loss: 78080674.0392\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 79979266.4314 - val_loss: 78577519.6078\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 80177521.8824 - val_loss: 78664618.2745\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 79878454.8235 - val_loss: 78014463.1373\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 79528810.9804 - val_loss: 80788943.6078\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 80042968.3137 - val_loss: 78319934.5098\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 79919219.6078 - val_loss: 79095888.7059\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79726550.1176 - val_loss: 77875009.4902\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79841688.0000 - val_loss: 78048596.2353\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79780013.3725 - val_loss: 77871013.7255\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 80072658.7451 - val_loss: 78590140.0784\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79850678.2745 - val_loss: 77943690.3529\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 88us/step - loss: 79804601.6471 - val_loss: 78181030.3529\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79699153.8824 - val_loss: 79634230.9804\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 80066804.2353 - val_loss: 78456549.1765\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79519289.1765 - val_loss: 78234946.6667\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79509021.5686 - val_loss: 78270630.1961\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79482878.9804 - val_loss: 78250895.1373\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79514159.2157 - val_loss: 78202150.0392\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 79488227.3725 - val_loss: 78261656.7059\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79467265.8039 - val_loss: 78263601.2549\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79465927.4510 - val_loss: 78260747.1373\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 79468886.6667 - val_loss: 78258175.8431\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 79468364.0000 - val_loss: 78256866.4314\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79465775.9216 - val_loss: 78257685.9608\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79461715.9216 - val_loss: 78258447.7647\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79462120.5490 - val_loss: 78256316.4706\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 79462196.5490 - val_loss: 78256555.0588\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79461963.6078 - val_loss: 78257937.9608\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79461502.5098 - val_loss: 78257258.1961\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 14, 64)            192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 100)               44900     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,193\n",
      "Trainable params: 45,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 816us/step - loss: 160503691.0588 - val_loss: 122343913.4118\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 105302894.1961 - val_loss: 105713483.6863\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 99177097.4118 - val_loss: 99711250.1961\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 91948745.4118 - val_loss: 109306292.7059\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 93529054.1961 - val_loss: 94066598.7451\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 89374360.6275 - val_loss: 91625064.5490\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 89122697.6471 - val_loss: 87727334.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 88537429.6471 - val_loss: 91436759.3725\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 86811622.9804 - val_loss: 94247732.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87770977.4118 - val_loss: 86584418.3529\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 87869163.0588 - val_loss: 84250660.1569\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 86815263.9216 - val_loss: 85205120.0000\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 85589223.6078 - val_loss: 83660945.4902\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 83495368.0784 - val_loss: 83192168.2353\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 84414561.1765 - val_loss: 84816139.9216\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 82966033.4118 - val_loss: 82767666.8235\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 84784321.6471 - val_loss: 82534641.4118\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 83405026.7451 - val_loss: 80200147.6078\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 67us/step - loss: 83392628.3922 - val_loss: 84804020.2353\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 66us/step - loss: 84221148.7843 - val_loss: 80236158.3529\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 82681327.0588 - val_loss: 80035496.8627\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82992495.7647 - val_loss: 81482424.8627\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 82701400.2353 - val_loss: 79757893.5686\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 82711273.0196 - val_loss: 80295913.7255\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 87900038.2745 - val_loss: 82827730.1961\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 81321005.0980 - val_loss: 79846107.0588\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 82164297.1373 - val_loss: 79165952.0784\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82754631.2941 - val_loss: 81456684.3922\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 82500038.7451 - val_loss: 79352610.9020\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 80709553.0980 - val_loss: 81288881.0980\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81587346.7451 - val_loss: 81790712.9412\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 82905307.3725 - val_loss: 87726462.5098\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 81360762.1176 - val_loss: 78909183.9216\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79460841.7255 - val_loss: 78282589.7255\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 79461501.4902 - val_loss: 79139069.1765\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79438160.4706 - val_loss: 78618985.0980\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 79571246.9020 - val_loss: 78486696.6275\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 79491330.5882 - val_loss: 78872636.7059\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79446718.5098 - val_loss: 78522182.5882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79157264.5882 - val_loss: 78353759.3725\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79103514.2745 - val_loss: 78300483.6078\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79139144.3922 - val_loss: 78320985.3333\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79108116.1569 - val_loss: 78253493.1765\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 79108530.1176 - val_loss: 78280774.8235\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79115517.3333 - val_loss: 78279882.4314\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79102577.0196 - val_loss: 78327639.7647\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79089594.1961 - val_loss: 78307222.9020\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79097627.0588 - val_loss: 78291430.5098\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79069144.8627 - val_loss: 78289830.6667\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79069570.0392 - val_loss: 78287871.6863\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 79068381.8824 - val_loss: 78289668.8627\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79067360.0392 - val_loss: 78288112.8627\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 109us/step - loss: 79069153.8824 - val_loss: 78292456.8627\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79064670.8235 - val_loss: 78292840.6275\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 79064043.4510 - val_loss: 78292321.6471\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 79063926.3529 - val_loss: 78291532.0000\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79064134.9804 - val_loss: 78291856.6275\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 79063940.2353 - val_loss: 78291605.2549\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79063592.0000 - val_loss: 78291588.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79063595.2157 - val_loss: 78291581.4118\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79063586.1961 - val_loss: 78291585.7255\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 79063616.7059 - val_loss: 78291601.7255\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 79063616.0000 - val_loss: 78291604.5490\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_7 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 818us/step - loss: 213724590.7451 - val_loss: 102536294.3529\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 93730012.0000 - val_loss: 92588372.4706\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 90818775.3725 - val_loss: 92366884.3922\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 90470424.7059 - val_loss: 90519173.5686\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 87889739.3725 - val_loss: 92055105.9608\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 87507276.1569 - val_loss: 89205338.9804\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 85955443.6863 - val_loss: 89138011.3725\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 85516181.6471 - val_loss: 91170161.5686\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 84660237.9608 - val_loss: 92285380.2353\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 81899192.7843 - val_loss: 87520634.7451\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 83390548.4706 - val_loss: 86604120.3137\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 82710737.2549 - val_loss: 85491737.2549\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 81317283.4510 - val_loss: 86820100.7843\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 82670225.9608 - val_loss: 85162621.8824\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 80437289.1765 - val_loss: 85508076.4706\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 80650200.6275 - val_loss: 87964408.1569\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 80765637.2549 - val_loss: 84382730.7451\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79884262.5490 - val_loss: 86058466.5882\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79110828.4706 - val_loss: 90318013.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 80491793.0980 - val_loss: 85734150.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 79597318.3529 - val_loss: 84470656.1569\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 75us/step - loss: 79514265.4118 - val_loss: 85179759.2941\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 77630460.7059 - val_loss: 83596701.2549\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 77653647.2157 - val_loss: 83489694.4314\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 77562428.9412 - val_loss: 84416645.9608\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 77512907.1373 - val_loss: 84126611.8431\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 77349627.0588 - val_loss: 83381745.4118\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 76us/step - loss: 77425695.4118 - val_loss: 83373492.7843\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 71us/step - loss: 77366676.3137 - val_loss: 83696957.0980\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 72us/step - loss: 77259090.7451 - val_loss: 83558350.8235\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 77237799.2157 - val_loss: 83110812.7059\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 77048212.3922 - val_loss: 83942245.4902\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 77277056.4314 - val_loss: 83321940.7843\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 77264559.7647 - val_loss: 83825721.5686\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 77330369.1765 - val_loss: 83542519.8431\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 103us/step - loss: 77167179.6471 - val_loss: 82906878.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 77189598.3529 - val_loss: 83213867.1373\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 76874058.5098 - val_loss: 82766675.4510\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 76857480.7059 - val_loss: 83269273.9608\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76813557.0980 - val_loss: 82888745.0980\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76849270.4314 - val_loss: 83024335.9216\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 76848790.9412 - val_loss: 83091113.4902\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76781543.3333 - val_loss: 82867458.9804\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 76422119.6078 - val_loss: 82684909.3333\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 76395428.3922 - val_loss: 82730273.4118\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 76378342.6667 - val_loss: 82635419.4510\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76372492.4706 - val_loss: 82621477.7255\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 76377096.3529 - val_loss: 82594562.1176\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76358605.6471 - val_loss: 82650999.6078\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 76352971.6863 - val_loss: 82574297.6471\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 76346919.9216 - val_loss: 82605108.5490\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 76355128.0784 - val_loss: 82613355.7647\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 76335437.2549 - val_loss: 82632850.6667\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 76346201.1765 - val_loss: 82562084.6275\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76364542.4314 - val_loss: 82626263.3725\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76387587.3725 - val_loss: 82554152.0784\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76367237.3333 - val_loss: 82575028.5490\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 76323863.7647 - val_loss: 82568488.2353\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76322590.0392 - val_loss: 82535795.4510\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76334361.5686 - val_loss: 82546516.3137\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 76331735.0588 - val_loss: 82578930.2745\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 76332302.0392 - val_loss: 82536935.7647\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 76352376.6275 - val_loss: 82542758.0392\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 81us/step - loss: 76312828.7843 - val_loss: 82571850.1961\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 76279287.3725 - val_loss: 82564443.0588\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 76273433.0980 - val_loss: 82564860.7843\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 76272336.5490 - val_loss: 82562544.1569\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 76270650.1176 - val_loss: 82565517.0196\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76272648.7843 - val_loss: 82567795.3725\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 113us/step - loss: 76268198.2353 - val_loss: 82567382.1176\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 76268049.0588 - val_loss: 82566685.1765\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 76267932.0000 - val_loss: 82566916.7059\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 76267812.1569 - val_loss: 82566744.5490\n",
      "Epoch 74/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 76268025.4902 - val_loss: 82566837.8039\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 75/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 76267483.2157 - val_loss: 82566833.3333\n",
      "Epoch 76/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 76267479.6863 - val_loss: 82566796.3137\n",
      "Epoch 77/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 76267475.0588 - val_loss: 82566788.3137\n",
      "Epoch 78/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 76267464.5490 - val_loss: 82566771.2157\n",
      "Epoch 79/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 76267498.9804 - val_loss: 82566816.0000\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_8 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 843us/step - loss: 208236590.9804 - val_loss: 87371934.5882\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 91506375.5294 - val_loss: 85930965.8824\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 90424060.0000 - val_loss: 89200661.8039\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 89430441.8039 - val_loss: 85039004.6275\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 88374083.7647 - val_loss: 83070494.5882\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 86882559.1373 - val_loss: 82523499.1373\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 88398969.8235 - val_loss: 86964850.1176\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 85914029.8824 - val_loss: 82392996.7059\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 85512976.5490 - val_loss: 81861073.0196\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 84733351.3725 - val_loss: 81762732.4706\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 83981954.9020 - val_loss: 82067242.7451\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 83988631.7647 - val_loss: 81313656.5490\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 83496227.9216 - val_loss: 84081457.5686\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 84731810.8235 - val_loss: 80555935.5294\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550/2550 [==============================] - 0s 86us/step - loss: 82744796.7059 - val_loss: 80147506.3529\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 82224448.2353 - val_loss: 81763602.6667\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 82394619.2941 - val_loss: 86845100.3137\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 84173189.4902 - val_loss: 82467210.0392\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 83784776.1569 - val_loss: 81478129.5686\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 83406215.6863 - val_loss: 80309756.2353\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 82471134.4314 - val_loss: 80029707.8431\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 81394933.7647 - val_loss: 79891215.5294\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 81522802.2745 - val_loss: 80862500.0000\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 81262523.9216 - val_loss: 80523218.7451\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 81133781.3333 - val_loss: 79972317.8824\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 74us/step - loss: 80968541.6078 - val_loss: 79739128.5490\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 81409950.5882 - val_loss: 80435173.4902\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 81003451.6863 - val_loss: 79743451.8431\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 81009814.2353 - val_loss: 79708206.2745\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 80739536.1569 - val_loss: 80927213.9608\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 80777202.1961 - val_loss: 80830206.3529\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 80581018.7451 - val_loss: 79801162.1176\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 80517099.5686 - val_loss: 79787854.5882\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 70us/step - loss: 80427350.9412 - val_loss: 79644839.4510\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 68us/step - loss: 80336957.8039 - val_loss: 79699356.7843\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 77us/step - loss: 80291791.8431 - val_loss: 80888262.9020\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 80274745.3333 - val_loss: 79722092.9412\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 80094977.8039 - val_loss: 80000285.9608\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79986840.0000 - val_loss: 79524470.6667\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 80011467.4510 - val_loss: 79553559.4510\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79833704.3137 - val_loss: 80018089.8039\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79848038.0392 - val_loss: 79687260.1569\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79870603.7647 - val_loss: 79549168.2353\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 82us/step - loss: 79737410.6667 - val_loss: 80351360.8627\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 79754568.7059 - val_loss: 80028687.2157\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 79649756.8627 - val_loss: 79878176.5490\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79624934.9804 - val_loss: 79798195.9216\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 79609159.2157 - val_loss: 79813828.2353\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 79599115.5294 - val_loss: 79712698.5098\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 79575356.3137 - val_loss: 79717107.7647\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 79576270.2745 - val_loss: 79719862.3529\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79574315.2157 - val_loss: 79716592.7843\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79574667.2941 - val_loss: 79716606.5098\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 79574378.1176 - val_loss: 79718702.2745\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 79571493.7255 - val_loss: 79717964.1569\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 79571340.2353 - val_loss: 79717723.3725\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 79571209.8824 - val_loss: 79718263.2157\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 79571333.3333 - val_loss: 79718565.8824\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 87us/step - loss: 79571139.9216 - val_loss: 79718379.8431\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_9 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,801\n",
      "Trainable params: 7,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 2s 915us/step - loss: 135953817.8824 - val_loss: 96976651.0588\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 91109233.0196 - val_loss: 92439895.5294\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 90664792.3922 - val_loss: 90336155.4510\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 88443198.5098 - val_loss: 88514929.6471\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 93us/step - loss: 87339977.8039 - val_loss: 88246529.7255\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 87688131.7647 - val_loss: 85554693.4902\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 86645661.6471 - val_loss: 88414197.4902\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 85689850.6667 - val_loss: 83625509.8039\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 84680011.1373 - val_loss: 84641883.2941\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 109us/step - loss: 84840807.7647 - val_loss: 89138936.3137\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 0s 100us/step - loss: 85192539.7647 - val_loss: 83478465.3333\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 84391319.5294 - val_loss: 81239040.7843\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 111us/step - loss: 84567717.4118 - val_loss: 81352105.0196\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 84425283.7647 - val_loss: 81415212.0784\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 83762512.0784 - val_loss: 83104634.9020\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 85512250.4314 - val_loss: 82707360.3922\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 83234975.7647 - val_loss: 81162506.7451\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 82868038.4314 - val_loss: 81087706.4314\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 84538447.0588 - val_loss: 79790745.4118\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 81745658.8235 - val_loss: 79581256.7059\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 82464051.9216 - val_loss: 90310402.6667\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 91us/step - loss: 82043512.5490 - val_loss: 79351926.1176\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 81445286.2745 - val_loss: 79939240.1569\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 80850200.8627 - val_loss: 80404616.4706\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 73us/step - loss: 83546872.7843 - val_loss: 79830581.7255\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 86us/step - loss: 81899190.0000 - val_loss: 80243149.0980\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 0s 85us/step - loss: 81788605.9216 - val_loss: 80206535.5294\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 80189010.4314 - val_loss: 78212420.0000\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 79544185.1765 - val_loss: 78102109.8824\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 79507264.1569 - val_loss: 78292548.1569\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 99us/step - loss: 79540191.2157 - val_loss: 78212863.4510\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 79278480.3137 - val_loss: 78050878.8235\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 0s 110us/step - loss: 79345772.4706 - val_loss: 77993163.8431\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 0s 115us/step - loss: 79350273.6471 - val_loss: 78298563.6078\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 79416296.4706 - val_loss: 77998973.2549\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 0s 94us/step - loss: 79156768.2353 - val_loss: 77959572.3922\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78942064.2353 - val_loss: 78069934.1961\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79202137.6863 - val_loss: 79313670.1176\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 79219417.8824 - val_loss: 78021311.4510\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 0s 98us/step - loss: 78909705.8431 - val_loss: 78115246.7451\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78893241.0196 - val_loss: 77945120.0000\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78810431.6863 - val_loss: 77829901.3333\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 84us/step - loss: 78964372.5490 - val_loss: 77928069.3333\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 0s 97us/step - loss: 78681836.6275 - val_loss: 77836806.0392\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 0s 102us/step - loss: 78755785.3333 - val_loss: 77853288.0784\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 0s 112us/step - loss: 78655470.2745 - val_loss: 77830080.3922\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 108us/step - loss: 78629887.2941 - val_loss: 78401561.5686\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 0s 104us/step - loss: 78748600.4706 - val_loss: 77927258.8235\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78487919.6863 - val_loss: 77859261.8824\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78418417.5686 - val_loss: 77819054.5098\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78413110.9804 - val_loss: 77793733.2549\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 0s 107us/step - loss: 78441716.5490 - val_loss: 77810767.5294\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 0s 117us/step - loss: 78431426.5882 - val_loss: 77769536.0784\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 0s 118us/step - loss: 78408497.6078 - val_loss: 77793385.9608\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78416566.1176 - val_loss: 77783128.4706\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 80us/step - loss: 78406778.4314 - val_loss: 77806482.9804\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 78us/step - loss: 78398578.5490 - val_loss: 77779556.8627\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 0s 88us/step - loss: 78392490.8235 - val_loss: 77784606.7451\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78365999.3725 - val_loss: 77787908.7843\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 0s 90us/step - loss: 78368747.2157 - val_loss: 77786245.3333\n",
      "Epoch 61/500\n",
      "2550/2550 [==============================] - 0s 95us/step - loss: 78364798.5490 - val_loss: 77786615.5294\n",
      "Epoch 62/500\n",
      "2550/2550 [==============================] - 0s 101us/step - loss: 78365829.8039 - val_loss: 77785705.0196\n",
      "Epoch 63/500\n",
      "2550/2550 [==============================] - 0s 114us/step - loss: 78367917.3333 - val_loss: 77782983.4510\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 64/500\n",
      "2550/2550 [==============================] - 0s 105us/step - loss: 78360979.4510 - val_loss: 77783001.1765\n",
      "Epoch 65/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 78361302.7451 - val_loss: 77783267.4510\n",
      "Epoch 66/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78362261.6471 - val_loss: 77782605.7255\n",
      "Epoch 67/500\n",
      "2550/2550 [==============================] - 0s 89us/step - loss: 78360837.8824 - val_loss: 77782965.1765\n",
      "Epoch 68/500\n",
      "2550/2550 [==============================] - 0s 106us/step - loss: 78360927.1765 - val_loss: 77783015.4510\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 69/500\n",
      "2550/2550 [==============================] - 0s 127us/step - loss: 78360619.3725 - val_loss: 77783020.0784\n",
      "Epoch 70/500\n",
      "2550/2550 [==============================] - 0s 83us/step - loss: 78360616.4706 - val_loss: 77783018.4314\n",
      "Epoch 71/500\n",
      "2550/2550 [==============================] - 0s 79us/step - loss: 78360614.5882 - val_loss: 77783017.5686\n",
      "Epoch 72/500\n",
      "2550/2550 [==============================] - 0s 92us/step - loss: 78360616.6667 - val_loss: 77783023.6863\n",
      "Epoch 73/500\n",
      "2550/2550 [==============================] - 0s 96us/step - loss: 78360616.4706 - val_loss: 77783018.6667\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 494285776.9412 - val_loss: 156438493.8039\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 138931980.0784 - val_loss: 112372941.8824\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 113740850.3529 - val_loss: 106236763.9216\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 110909238.7451 - val_loss: 101310583.6863\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 104883198.2745 - val_loss: 97303520.3137\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 100271682.9804 - val_loss: 99478035.7647\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 98861416.5490 - val_loss: 99059819.4510\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 111264492.7059 - val_loss: 108328921.1765\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 107584831.5294 - val_loss: 102184961.4902\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 103874750.4314 - val_loss: 99256614.6667\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 98852231.2941 - val_loss: 99208967.2941\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 97726495.0588 - val_loss: 99522613.9608\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 99934937.7255 - val_loss: 99638750.2745\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 98816310.3529 - val_loss: 98281887.4510\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 199us/step - loss: 98130376.6275 - val_loss: 98072254.7451\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 97329132.3922 - val_loss: 97896851.8431\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 97090086.1961 - val_loss: 98065913.1765\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 97112633.0980 - val_loss: 97634243.7647\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 200us/step - loss: 97131849.4902 - val_loss: 96975057.4902\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 96981260.8627 - val_loss: 96891074.3529\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 96963664.1569 - val_loss: 96594383.2941\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 96993045.1765 - val_loss: 96561632.5490\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 230us/step - loss: 96855605.3333 - val_loss: 96862034.9804\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 97312870.7451 - val_loss: 96681665.3333\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 176us/step - loss: 96918835.2157 - val_loss: 96771576.7059\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 97001043.0588 - val_loss: 96791917.0980\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96901162.4314 - val_loss: 96806971.4510\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 96930874.7451 - val_loss: 96728174.5098\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 96785188.4706 - val_loss: 97063736.0000\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 96689699.2941 - val_loss: 96701973.1765\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 96702271.2941 - val_loss: 96554643.3725\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 96615998.7451 - val_loss: 96815439.3725\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 96591394.8235 - val_loss: 96748623.2941\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 96575704.1569 - val_loss: 96781917.8824\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 96611180.0784 - val_loss: 96913454.6667\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 96598452.6275 - val_loss: 96907907.0588\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 0s 195us/step - loss: 96573417.4902 - val_loss: 96906224.9412\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96573571.1373 - val_loss: 96899922.7451\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96572572.8627 - val_loss: 96900379.9216\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 96573222.1176 - val_loss: 96901911.7647\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 96571494.4314 - val_loss: 96902376.3137\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 96570400.9412 - val_loss: 96902410.8235\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 96570358.5882 - val_loss: 96902382.6667\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 96570342.3529 - val_loss: 96902408.0784\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 96570314.9020 - val_loss: 96902400.4706\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 209us/step - loss: 96570280.4706 - val_loss: 96902394.3529\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 96570252.7059 - val_loss: 96902395.9216\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 96570253.1765 - val_loss: 96902394.9020\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 96570250.5882 - val_loss: 96902394.2745\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 96570252.2353 - val_loss: 96902394.6667\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 96570251.4510 - val_loss: 96902394.5098\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.00000008274e-10.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 426244818.5098 - val_loss: 163206097.2549\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 130738648.9412 - val_loss: 119624087.7647\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 119020760.1569 - val_loss: 138237734.9020\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 188us/step - loss: 132354556.8627 - val_loss: 102501016.3137\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 108892387.5294 - val_loss: 113778157.2549\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 110865417.7255 - val_loss: 114159373.6471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 0s 180us/step - loss: 108939485.6471 - val_loss: 111233822.4314\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 110819786.4314 - val_loss: 132673629.7255\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 142691937.2549 - val_loss: 110280754.1176\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 225us/step - loss: 112905996.9412 - val_loss: 109285466.0392\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 109892279.6078 - val_loss: 109242301.7255\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 109425825.0980 - val_loss: 107965519.6863\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 105687623.7647 - val_loss: 103451909.9608\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 224us/step - loss: 105766994.1961 - val_loss: 104392716.7843\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 106569866.0392 - val_loss: 103613029.5686\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 106056493.4902 - val_loss: 103899222.1176\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 105717336.7843 - val_loss: 103475693.4118\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 105804453.0196 - val_loss: 103730097.8039\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 0s 184us/step - loss: 105618924.7059 - val_loss: 103449592.1569\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 105570179.2157 - val_loss: 103387827.6078\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 0s 174us/step - loss: 106277553.0196 - val_loss: 103563784.4706\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 0s 178us/step - loss: 106091227.7647 - val_loss: 104717031.6863\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 106090904.9412 - val_loss: 104775477.7255\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 105700108.2353 - val_loss: 104765620.0784\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,601\n",
      "Trainable params: 15,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2550 samples, validate on 1275 samples\n",
      "Epoch 1/500\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 313867770.3529 - val_loss: 144243500.7059\n",
      "Epoch 2/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 128542064.4706 - val_loss: 128268490.9804\n",
      "Epoch 3/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 125539354.7451 - val_loss: 128898265.5686\n",
      "Epoch 4/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 111493032.6275 - val_loss: 127047080.4706\n",
      "Epoch 5/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 114454669.8039 - val_loss: 116225869.4902\n",
      "Epoch 6/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 105794367.2157 - val_loss: 112260505.0980\n",
      "Epoch 7/500\n",
      "2550/2550 [==============================] - 1s 207us/step - loss: 114526382.0392 - val_loss: 127449116.2353\n",
      "Epoch 8/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 117062977.8039 - val_loss: 123178639.8431\n",
      "Epoch 9/500\n",
      "2550/2550 [==============================] - 1s 213us/step - loss: 112955348.0784 - val_loss: 114367032.4706\n",
      "Epoch 10/500\n",
      "2550/2550 [==============================] - 1s 229us/step - loss: 104334621.5686 - val_loss: 107965076.1569\n",
      "Epoch 11/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 99550998.4314 - val_loss: 103347578.8235\n",
      "Epoch 12/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 97516005.8824 - val_loss: 101466654.1176\n",
      "Epoch 13/500\n",
      "2550/2550 [==============================] - 0s 189us/step - loss: 94673263.3725 - val_loss: 98102318.9020\n",
      "Epoch 14/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 93500883.2157 - val_loss: 97079084.6275\n",
      "Epoch 15/500\n",
      "2550/2550 [==============================] - 1s 196us/step - loss: 91806661.2549 - val_loss: 92776525.0196\n",
      "Epoch 16/500\n",
      "2550/2550 [==============================] - 0s 190us/step - loss: 90793722.4314 - val_loss: 93930332.0000\n",
      "Epoch 17/500\n",
      "2550/2550 [==============================] - 0s 196us/step - loss: 91483149.4118 - val_loss: 91375992.7843\n",
      "Epoch 18/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 89884833.9608 - val_loss: 91257579.1373\n",
      "Epoch 19/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 90364069.8039 - val_loss: 96524776.0784\n",
      "Epoch 20/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 89296985.4118 - val_loss: 94508978.4314\n",
      "Epoch 21/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 88982799.2941 - val_loss: 89551091.5294\n",
      "Epoch 22/500\n",
      "2550/2550 [==============================] - 1s 220us/step - loss: 89056907.4510 - val_loss: 89014724.9412\n",
      "Epoch 23/500\n",
      "2550/2550 [==============================] - 1s 228us/step - loss: 89388667.6078 - val_loss: 89446232.4706\n",
      "Epoch 24/500\n",
      "2550/2550 [==============================] - 0s 194us/step - loss: 89246160.0784 - val_loss: 88947594.5882\n",
      "Epoch 25/500\n",
      "2550/2550 [==============================] - 0s 181us/step - loss: 89493457.8039 - val_loss: 88417104.0784\n",
      "Epoch 26/500\n",
      "2550/2550 [==============================] - 1s 202us/step - loss: 88691375.3725 - val_loss: 88308164.5490\n",
      "Epoch 27/500\n",
      "2550/2550 [==============================] - 1s 223us/step - loss: 87539072.8627 - val_loss: 92103535.0588\n",
      "Epoch 28/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 88667190.7451 - val_loss: 89230947.9216\n",
      "Epoch 29/500\n",
      "2550/2550 [==============================] - 0s 192us/step - loss: 88801607.2941 - val_loss: 88920344.7059\n",
      "Epoch 30/500\n",
      "2550/2550 [==============================] - 0s 186us/step - loss: 88251366.1961 - val_loss: 87769068.7059\n",
      "Epoch 31/500\n",
      "2550/2550 [==============================] - 1s 211us/step - loss: 87850215.4510 - val_loss: 87880941.3333\n",
      "Epoch 32/500\n",
      "2550/2550 [==============================] - 1s 216us/step - loss: 87813968.3137 - val_loss: 88271850.1176\n",
      "Epoch 33/500\n",
      "2550/2550 [==============================] - 1s 217us/step - loss: 88244458.8235 - val_loss: 87605515.5294\n",
      "Epoch 34/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 87198071.2941 - val_loss: 87374079.7647\n",
      "Epoch 35/500\n",
      "2550/2550 [==============================] - 1s 221us/step - loss: 87840486.5098 - val_loss: 90925083.1373\n",
      "Epoch 36/500\n",
      "2550/2550 [==============================] - 1s 226us/step - loss: 90418764.2353 - val_loss: 88175458.4314\n",
      "Epoch 37/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 87286130.1176 - val_loss: 89179778.7451\n",
      "Epoch 38/500\n",
      "2550/2550 [==============================] - 0s 183us/step - loss: 87832618.7451 - val_loss: 90008825.1765\n",
      "Epoch 39/500\n",
      "2550/2550 [==============================] - 0s 185us/step - loss: 87689834.8235 - val_loss: 87915756.4706\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 40/500\n",
      "2550/2550 [==============================] - 1s 245us/step - loss: 86808519.1373 - val_loss: 86737648.5490\n",
      "Epoch 41/500\n",
      "2550/2550 [==============================] - 1s 208us/step - loss: 86797131.7647 - val_loss: 86795015.8431\n",
      "Epoch 42/500\n",
      "2550/2550 [==============================] - 1s 224us/step - loss: 86683860.0784 - val_loss: 86926961.7255\n",
      "Epoch 43/500\n",
      "2550/2550 [==============================] - 1s 204us/step - loss: 86683033.8824 - val_loss: 87036563.7647\n",
      "Epoch 44/500\n",
      "2550/2550 [==============================] - 1s 210us/step - loss: 86704396.1569 - val_loss: 86863297.9608\n",
      "Epoch 45/500\n",
      "2550/2550 [==============================] - 1s 212us/step - loss: 86659135.2941 - val_loss: 86814094.8235\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 46/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 86585356.6275 - val_loss: 86821088.2353\n",
      "Epoch 47/500\n",
      "2550/2550 [==============================] - 1s 203us/step - loss: 86569319.3725 - val_loss: 86793669.8039\n",
      "Epoch 48/500\n",
      "2550/2550 [==============================] - 1s 222us/step - loss: 86579217.4118 - val_loss: 86810974.0392\n",
      "Epoch 49/500\n",
      "2550/2550 [==============================] - 1s 228us/step - loss: 86579597.3333 - val_loss: 86798735.1373\n",
      "Epoch 50/500\n",
      "2550/2550 [==============================] - 1s 197us/step - loss: 86568187.3725 - val_loss: 86771225.5686\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 51/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 86561491.8039 - val_loss: 86770697.0980\n",
      "Epoch 52/500\n",
      "2550/2550 [==============================] - 1s 219us/step - loss: 86561442.0392 - val_loss: 86770655.8431\n",
      "Epoch 53/500\n",
      "2550/2550 [==============================] - 1s 201us/step - loss: 86560382.1961 - val_loss: 86771439.2941\n",
      "Epoch 54/500\n",
      "2550/2550 [==============================] - 1s 205us/step - loss: 86561979.5294 - val_loss: 86773301.1765\n",
      "Epoch 55/500\n",
      "2550/2550 [==============================] - 0s 193us/step - loss: 86559798.7451 - val_loss: 86772931.4510\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
      "Epoch 56/500\n",
      "2550/2550 [==============================] - 0s 187us/step - loss: 86559403.5294 - val_loss: 86773091.7647\n",
      "Epoch 57/500\n",
      "2550/2550 [==============================] - 0s 182us/step - loss: 86559307.6078 - val_loss: 86772809.4902\n",
      "Epoch 58/500\n",
      "2550/2550 [==============================] - 1s 198us/step - loss: 86559260.7059 - val_loss: 86772881.6471\n",
      "Epoch 59/500\n",
      "2550/2550 [==============================] - 0s 191us/step - loss: 86559387.4118 - val_loss: 86772701.2549\n",
      "Epoch 60/500\n",
      "2550/2550 [==============================] - 1s 206us/step - loss: 86559384.5490 - val_loss: 86772905.4902\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "        ['ARIMA', ARIMA],\n",
    "        ['SARIMAX', sarimax.SARIMAX],\n",
    "        ['ANN', build_ann],\n",
    "        ['CNN', build_cnn],\n",
    "        ['RNN', build_rnn],\n",
    "        ['LSTM', build_lstm]]\n",
    "initialization()\n",
    "X = data_X.values[:,:,np.newaxis]\n",
    "performance_y_test = {}\n",
    "performance_y_test[5]={}\n",
    "performance_y_test[5][\"MORN\"]={}\n",
    "performance_y_test[5][\"MORN\"][\"MAE\"] = 2435\n",
    "performance_y_test[5][\"MORN\"][\"RMSE\"] = 3567\n",
    "performance_y_test[5][\"MORN\"][\"R2\"] = 0.94\n",
    "performance_y_test[7]={}\n",
    "performance_y_test[7][\"MORN\"]={}\n",
    "performance_y_test[7][\"MORN\"][\"MAE\"] = 3028\n",
    "performance_y_test[7][\"MORN\"][\"RMSE\"] = 4389\n",
    "performance_y_test[7][\"MORN\"][\"R2\"] = 0.91\n",
    "performance_y_test[10]={}\n",
    "performance_y_test[10][\"MORN\"]={}\n",
    "performance_y_test[10][\"MORN\"][\"MAE\"] = 3580\n",
    "performance_y_test[10][\"MORN\"][\"RMSE\"] = 5367\n",
    "performance_y_test[10][\"MORN\"][\"R2\"] = 0.871\n",
    "\n",
    "n_splits = 3\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=3)\n",
    "for lead in LEAD:\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%% start experiments with lead time \"+str(lead)+\" %%%%%%%%%%%%%%%%%%%%\")\n",
    "    y = data_y.loc[:,'Q_'+str(lead)].values \n",
    "    X_train, y_train, X_test, y_test = X[:TRAIN_TEST], y[:TRAIN_TEST], X[TRAIN_TEST:], y[TRAIN_TEST:]    \n",
    "    \n",
    "    for name, model in models:\n",
    "        mae=0\n",
    "        rmse=0\n",
    "        r2=0\n",
    "        performance_y_test[lead][name]={}\n",
    "        if name == \"ARIMA\":            \n",
    "            prediction = list()\n",
    "            for t in X_test:\n",
    "                clf = model(t, order=(1,1,0))\n",
    "                clf_fit = clf.fit()\n",
    "                yhat = clf_fit.forecast(steps=lead)[0][-1]\n",
    "                prediction.append(yhat)\n",
    "            m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "            performance_y_test[lead][name][\"MAE\"] = m_mae\n",
    "            performance_y_test[lead][name][\"RMSE\"] = m_rmse\n",
    "            performance_y_test[lead][name][\"R2\"] = m_r2\n",
    "            print (m_mae,m_rmse,m_r2)\n",
    "        elif name == 'SARIMAX':\n",
    "            prediction = list()\n",
    "            for index, t in enumerate(X_test):\n",
    "                total_index = TRAIN_TEST+index\n",
    "                history = X[(total_index-3*YEAR_DAYS):total_index,-1]                 \n",
    "                print (history.shape)\n",
    "                clf = model(history, order=(1,1,0), seasonal_order=(1, 1, 0, YEAR_DAYS))\n",
    "                clf_fit = clf.fit()\n",
    "                yhat = clf_fit.forecast(steps=lead)[-1]\n",
    "                prediction.append(yhat)\n",
    "            m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "            performance_y_test[lead][name][\"MAE\"] = m_mae\n",
    "            performance_y_test[lead][name][\"RMSE\"] = m_rmse\n",
    "            performance_y_test[lead][name][\"R2\"] = m_r2\n",
    "            print (m_mae,m_rmse,m_r2)\n",
    "        else:\n",
    "            for train, validation in cv.split(X_train, y_train):\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=20, mode='auto')\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4)    \n",
    "                callbacks = [early_stopping,reduce_lr]\n",
    "                clf = model()\n",
    "                history = clf.fit(X_train[train],y_train[train],\n",
    "                            epochs=NUM_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_train[validation],y_train[validation]),               \n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1)\n",
    "                prediction = clf.predict(X_test,verbose=0)\n",
    "                m_mae,m_rmse,m_r2 = get_metrics(np.array(y_test),prediction)\n",
    "                mae +=m_mae\n",
    "                rmse +=m_rmse\n",
    "                r2 +=m_r2\n",
    "            performance_y_test[lead][name][\"MAE\"] = mae/n_splits\n",
    "            performance_y_test[lead][name][\"RMSE\"] = rmse/n_splits\n",
    "            performance_y_test[lead][name][\"R2\"] = r2/n_splits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame.from_dict({(i,j): performance_y_test[i][j] \n",
    "                           for i in performance_y_test.keys() \n",
    "                           for j in performance_y_test[i].keys()},\n",
    "                       orient='index')\n",
    "df_result.index = df_result.index.set_names(['Lead','Model'])\n",
    "df_result.to_csv(PATH_RESULT+'/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lead</th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>ANN</td>\n",
       "      <td>2701.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4404.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>2907.66</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4892.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3150.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5124.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3569.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5843.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MORN</td>\n",
       "      <td>2435.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3567.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>RNN</td>\n",
       "      <td>2417.66</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4049.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>SARIMAX</td>\n",
       "      <td>16809.83</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>22621.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ANN</td>\n",
       "      <td>3721.45</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5931.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>4298.94</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7148.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>CNN</td>\n",
       "      <td>4175.11</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6615.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>4810.26</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7490.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>MORN</td>\n",
       "      <td>3028.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>4389.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>RNN</td>\n",
       "      <td>3523.19</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5670.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>SARIMAX</td>\n",
       "      <td>17058.24</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>22781.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>5140.83</td>\n",
       "      <td>0.72</td>\n",
       "      <td>7921.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>6192.63</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10312.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>CNN</td>\n",
       "      <td>5455.38</td>\n",
       "      <td>0.69</td>\n",
       "      <td>8374.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>6352.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9592.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>MORN</td>\n",
       "      <td>3580.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5367.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>RNN</td>\n",
       "      <td>4921.47</td>\n",
       "      <td>0.74</td>\n",
       "      <td>7637.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lead    Model      MAE    R2     RMSE\n",
       "0      5      ANN  2701.88  0.91  4404.55\n",
       "1      5    ARIMA  2907.66  0.89  4892.25\n",
       "2      5      CNN  3150.90  0.88  5124.11\n",
       "3      5     LSTM  3569.83  0.85  5843.15\n",
       "4      5     MORN  2435.00  0.94  3567.00\n",
       "5      5      RNN  2417.66  0.93  4049.81\n",
       "6      5  SARIMAX 16809.83 -1.25 22621.09\n",
       "7      7      ANN  3721.45  0.84  5931.83\n",
       "8      7    ARIMA  4298.94  0.77  7148.26\n",
       "9      7      CNN  4175.11  0.81  6615.65\n",
       "10     7     LSTM  4810.26  0.75  7490.66\n",
       "11     7     MORN  3028.00  0.91  4389.00\n",
       "12     7      RNN  3523.19  0.86  5670.35\n",
       "13     7  SARIMAX 17058.24 -1.30 22781.69\n",
       "14    10      ANN  5140.83  0.72  7921.03\n",
       "15    10    ARIMA  6192.63  0.52 10312.56\n",
       "16    10      CNN  5455.38  0.69  8374.51\n",
       "17    10     LSTM  6352.20  0.59  9592.29\n",
       "18    10     MORN  3580.00  0.87  5367.00\n",
       "19    10      RNN  4921.47  0.74  7637.93"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = pd.read_csv(PATH_RESULT+'/results1.csv')\n",
    "display(df_result.head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/seaborn/axisgrid.py:715: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n",
      "/usr/local/lib/python2.7/dist-packages/seaborn/axisgrid.py:715: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n",
      "/usr/local/lib/python2.7/dist-packages/seaborn/axisgrid.py:715: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fdd93ba2cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = 6\n",
    "g = sns.FacetGrid(df_result, col=\"Lead\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"MAE\", color=\"m\")\n",
    "g = sns.FacetGrid(df_result, col=\"Lead\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"RMSE\", color=\"m\")\n",
    "g = sns.FacetGrid(df_result, col=\"Lead\",margin_titles=True, height=height)\n",
    "g.map(sns.barplot,  \"Model\", \"R2\", color=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
